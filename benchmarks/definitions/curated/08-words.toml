analysis = '''
This benchmark measures how long it takes for a regex engine to find words in
a haystack. We compare one regex that finds all words, `\b\w+\b` and another
regex that only looks for longer words, `\b\w{12,}\b`. We also compare ASCII
regexes on English text with Unicode regexes on Russian text.

The split between finding all words and finding only long words tends to
highlight the overhead of matching in each regex engine. Regex engines that are
quicker to get in and out of its match routine do better at finding all words
than regex engines that have higher overhead. For example, `regress` is faster
than `rust/regex` on `all-english`, but substantially slower than `rust/regex`
on `long-english`. This is likely because `rust/regex` is doing more work per
search call than `regress`, which is in part rooted in the optimizations it
performs to gain higher throughput.

Otherwise, `pcre2/jit` does quite well here across the board, but especially on
the Unicode variants. When comparing it against `rust/regex` for example, it
is substantially faster. In the case of `rust/regex`, its faster DFA oriented
engines cannot handle the Unicode aware `\b` on non-ASCII haystacks, and this
causes `rust/regex` to use a slower internal engine. It's so slow in fact
that `python/re` and `python/regex` are both faster than `rust/regex` for the
Unicode benchmarks. For the ASCII `long-english` benchmark, `rust/regex` and
`re2` both do well because most of the time is spent in its lazy DFA, which has
pretty good throughput performance when compared to a pure backtracker.

Note that several regex engines can't be used in the Unicode variants because
either they don't support a Unicode aware `\w` or because they don't support a
Unicode aware `\b` (or both).
'''

[[bench]]
model = "count-spans"
name = "all-english"
regex = '\b[0-9A-Za-z_]+\b'
haystack = { path = "opensubtitles/en-sampled.txt", line-end = 2_500 }
count = [
  { engine = '*', count = 56_691 },
  { engine = 'dotnet/compiled', count = 56_601 },
  { engine = 'dotnet/nobacktrack', count = 56_601 },
]
engines = [
  'dotnet/compiled',
  'dotnet/nobacktrack',
  'go/regexp',
  'hyperscan',
  'pcre2',
  'pcre2/jit',
  'perl',
  'python/re',
  'python/regex',
  're2',
  'regress',
  'rust/regex/meta',
]
analysis = '''
We specifically write out `[0-9A-Za-z_]` instead of using `\w` because some
regex engines, such as the one found in .NET, make `\w` Unicode aware and there
doesn't appear to be any easy way of disabling it.

Also, the .NET engine makes `\b` Unicode-aware, which also appears impossible
to disable. To account for that, we permit a different count.
'''

[[bench]]
model = "count-spans"
name = "all-russian"
regex = '\b\w+\b'
unicode = true
haystack = { path = "opensubtitles/ru-sampled.txt", line-end = 2_500 }
count = [
  { engine = '*', count = 107_391 },
  { engine = 'dotnet/compiled', count = 53_960 },
  { engine = 'dotnet/nobacktrack', count = 53_960 },
  { engine = 'perl', count = 53_960 },
]
engines = [
  'dotnet/compiled',
  'dotnet/nobacktrack',
  'pcre2',
  'pcre2/jit',
  'perl',
  'python/re',
  'python/regex',
  'rust/regex/meta',
]
analysis = '''
`regress`, `re2` and `go/regexp` are excluded because `\w` is not Unicode
aware. `hyperscan` is exclude because it doesn't support a Unicode aware `\b`.

For `dotnet/compiled`, since the length of matching spans is in the number of
UTF-16 code units, its expected count is smaller.

For `perl`, it has the same count as `dotnet/compiled`, but only because it
counts total encoded codepoints. Since every match span in this benchmark
seemingly corresponds to codepoints in the basic multi-lingual plane, it
follows that the number of UTF-16 code units is equivalent to the number of
codepoints.
'''

[[bench]]
model = "count-spans"
name = "long-english"
regex = '\b[0-9A-Za-z_]{12,}\b'
haystack = { path = "opensubtitles/en-sampled.txt", line-end = 2_500 }
count = 839
engines = [
  'dotnet/compiled',
  'dotnet/nobacktrack',
  'go/regexp',
  'hyperscan',
  'pcre2',
  'pcre2/jit',
  'perl',
  'python/re',
  'python/regex',
  're2',
  'regress',
  'rust/regex/meta',
]
analysis = '''
We specifically write out `[0-9A-Za-z_]` instead of using `\w` because some
regex engines, such as the one found in .NET, make `\w` Unicode aware and there
doesn't appear to be any easy way of disabling it.

Also, the fact that `\b` is Unicode-aware in .NET does not seem to impact the
match counts in this benchmark.
'''

[[bench]]
model = "count-spans"
name = "long-russian"
regex = '\b\w{12,}\b'
unicode = true
haystack = { path = "opensubtitles/ru-sampled.txt", line-end = 2_500 }
count = [
  { engine = '*', count = 5481 },
  { engine = 'dotnet/compiled', count = 2747 },
  { engine = 'dotnet/nobacktrack', count = 2747 },
  { engine = 'perl', count = 2747 },
]
engines = [
  'dotnet/compiled',
  'dotnet/nobacktrack',
  'pcre2',
  'pcre2/jit',
  'perl',
  'python/re',
  'python/regex',
  'rust/regex/meta',
]
analysis = '''
`regress`, `re2` and `go/regexp` are excluded because `\w` is not Unicode
aware. `hyperscan` is exclude because it doesn't support a Unicode aware `\b`.

For `dotnet/compiled`, since the length of matching spans is in the number of
UTF-16 code units, its expected count is smaller.

For `perl`, it has the same count as `dotnet/compiled`, but only because it
counts total encoded codepoints. Since every match span in this benchmark
seemingly corresponds to codepoints in the basic multi-lingual plane, it
follows that the number of UTF-16 code units is equivalent to the number of
codepoints.
'''
