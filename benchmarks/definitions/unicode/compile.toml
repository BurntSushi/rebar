analysis = '''
These benchmarks demonstrate the impact that Unicode has on the time it takes
to compile regexes. Generally speaking, Unicode impacts the compile times of
automata engines the most, since (most, but not all) automata engines compile
regexes down to machines whose transitions are defined over bytes. This takes
quite a bit of work to do when faced with large Unicode character classes.
'''

[[bench]]
model = "compile"
name = "one-letter"
regex = '\pL'
haystack = "ͱͳͷΐάέήίΰαβγδεζηθικλμνξοπρςστυφχψωϊϋόύώϙϛϝϟϡϸϻͱͳͷΐάέή"
unicode = true
count = 53
engines = [
  'go/regexp',
  'hyperscan',
  'pcre2/jit',
  'python/regex',
  're2',
  'rust/regex',
  'rust/regex/dense',
  'rust/regex/nfa',
  'rust/regexold',
]
analysis = '''
Measures how long it takes to compile one particularly large Unicode class.
'''

[[bench]]
model = "compile"
name = "fifty-letters"
regex = '\pL{50}'
haystack = "ͱͳͷΐάέήίΰαβγδεζηθικλμνξοπρςστυφχψωϊϋόύώϙϛϝϟϡϸϻͱͳͷΐάέή"
unicode = true
count = 1
engines = [
  'go/regexp',
  'pcre2/jit',
  'python/regex',
  're2',
  'rust/regex',
  'rust/regex/dense',
  'rust/regex/nfa',
  'rust/regexold',
]
analysis = '''
Like `one-letter`, but repeats that class 50 times. Backtrackers in
particular do well here and generally aren't any slower than the `one-letter`
case, where as byte-oriented automata engines do quite a bit worse. This is
because they compile classes like `\pL` down into machines with transitions
defined over bytes and *not* codepoints. So there is a lot of work that is
done here.

I don't think any of the automata engines "cache" the work done to compile
`\pL` once and then reuse it for subsequent `\pL`s.

Note that `go/regexp` does quite well here despite using automata internally.
This is likely because it defines its transitions over codepoints, and so never
compiles things like `\pL` down to bytes at all. (`go/regexp` also lacks a DFA,
which lessens the need for byte oriented transitions.)
'''

[[bench]]
model = "compile"
name = "fifty-letters-ascii"
regex = '[a-zA-Z]{50}'
haystack = "abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz"
unicode = false
count = [
  { engine = 'hyperscan', count = 3},
  { engine = '.*', count = 1 },
]
engines = [
  'rust/regex',
  'rust/regex/dense',
  'rust/regex/nfa',
  'rust/regexold',
  're2',
  'go/regexp',
  'pcre2/jit',
  'python/regex',
  'hyperscan',
]
analysis = '''
This just provides a baseline comparison point with `\pL{50}` to demonstate
that Unicode is indeed the culprit here.
'''

[[bench]]
model = "compile"
name = "match-every-line"
regex = '(?m)^.+$'
haystack = { path = "lines-with-invalid-utf8.txt" }
unicode = true
count = [
  # Go handles invalid UTF-8 differently, by assuming U+FFFD. Since '.' matches
  # any codepoint, that includes U+FFFD.
  { engine = 'go/regexp', count = 4 },
  { engine = '.*', count = 3 },
]
engines = [
  'go/regexp',
  'pcre2/jit',
  're2',
  'rust/regex',
  'rust/regex/nfa',
  'rust/regexold',
]
analysis = '''
This compiles a simple regex that matches non-empty lines that are valid UTF-8.
The haystack we use contains 4 lines, but one of them contains invalid UTF-8.
'''

[[bench]]
model = "compile"
name = "match-every-line-ascii"
regex = '(?m)^.+$'
haystack = { path = "lines-with-invalid-utf8.txt" }
count = 4
engines = [
  'rust/regex',
  'rust/regex/nfa',
  'rust/regexold',
  're2',
  'go/regexp',
  'pcre2/jit',
  'python/regex',
  'hyperscan',
]
analysis = '''
This is like `match-every-line`, but disables Unicode mode and permits `.`
to match any byte (except for `\n`). So this includes the line that contains
invalid UTF-8 in the count.
'''

[[bench]]
model = "compile"
name = "negated-class-matches-codepoint"
regex = '[^a]'
unicode = true
haystack = '☃'
count = 1
engines = [
  'go/regexp',
  'hyperscan',
  'pcre2',
  'pcre2/jit',
  'python/re',
  'python/regex',
  're2',
  'regress',
  'rust/regex',
  'rust/regex/nfa',
  'rust/regexold',
]
analysis = '''
This tests that `[^a]` in Unicode mode matches an entire codepoint and not
the individual bytes.
'''
