analysis = '''
These benchmarks are meant to demonstrate just how difficult Unicode can be,
and how easy it is for Unicode to make your regexes jump off of a perf cliff.
These benchmarks demonstrate why you should disable Unicode in the regex crate
unless you need it. (One wonders whether it should have been disabled by
default, but that ship has sailed and I don't think it's ever coming back.)
'''

[[bench]]
model = "count"
name = "any-one"
regex = '(?s:.)'
haystack = { path = "unicode/allcodepoints.txt" }
unicode = true
count = 1_112_064
engines = [
  'go/regexp',
  'hyperscan',
  'vectorscan',
  'pcre2/jit',
  'python/regex',
  're2',
  'rust/regex',
  'rust/regexold',
  'rust/regex/lite',
]
analysis = '''
A simple benchmark that matches each individual codepoint.
'''

[[bench]]
model = "count-spans"
name = "any-all"
regex = '(?s:.)+'
haystack = { path = "unicode/allcodepoints.txt" }
unicode = true
count = 4_382_592
engines = [
  'go/regexp',
  'pcre2/jit',
  'python/regex',
  're2',
  'rust/regex',
  'rust/regexold',
  'rust/regex/lite',
]
analysis = '''
Same as 'any-one', but matches all the codepoints in one go.
'''

[[bench]]
model = "count"
name = "letters-one"
regex = '\p{L}{100}'
haystack = { path = "unicode/allcodepoints.txt" }
unicode = true
count = [
  # Very likely due to update to Unicode 15.1 (a month old at time of writing).
  { engine = '^re2$', count = 1250 },
  # Very likely due to update to Unicode 16 (a month old at time of writing).
  { engine = '^(python/regex|rust/regex)$', count = 1289 },
  # Maybe a different version of Unicode?
  { engine = 'pcre2/jit', count = 1203 },
  { engine = '.*', count = 1244 },
]
engines = [
  'go/regexp',
  'pcre2/jit',
  'python/regex',
  're2',
  'rust/regex',
  'rust/regexold',
]
analysis = '''
This benchmarks finding 100 contiguous letters in a sequence of all codepoints.
This basically takes a huge character class and then repeats it 100 times over.
The regex for this---at least at the automata level---is absolutely huge.
This wreaks all kind of havoc. It prevents a lazy DFA from being used because
it requires a cache that is beyond the default capacity. It's way too big
for a one-pass DFA and also the backtracker. So the PikeVM is stuck with the
unenviable task of dealing with it.

The bounded repeat really slows the PikeVM way down. Because at each position
in the haystack, the PikeVM has to keep track of how many letters have matched.
And huge Unicode classes are very branchy, which just overall leads to a lot of
state shuffling.

`python/regex` does *really* well on this benchmark. It likely has a specific
optimization in place for bounded repeats.
'''

[[bench]]
model = "count"
name = "letters-alt"
regex = '''(?x)(?:
  \p{Lowercase}
  |\p{Uppercase}
  |\p{Titlecase_Letter}
  |\p{Modifier_Letter}
  |\p{Other_Letter}
){100}'''
haystack = { path = "unicode/allcodepoints.txt" }
unicode = true
count = [
  # Very likely due to update to Unicode 15.1 (a month old at time of writing).
  { engine = '^re2$', count = 1250 },
  # Very likely due to update to Unicode 16 (a month old at time of writing).
  { engine = '^(python/regex|rust/regex)$', count = 1289 },
  # Maybe a different version of Unicode?
  { engine = 'pcre2/jit', count = 1203 },
  { engine = '.*', count = 1244 },
]
engines = [
  'rust/regex',
  'rust/regexold',
  # 're2', # does not support 'x' flag
  # 'go/regexp', # does not support 'x' flag
  # 'pcre2/jit', # lacks necessary property support
  'python/regex',
]
analysis = '''
This benchmark looks for exactly the same thing as `letters-one`, except it
uses the constituent Unicode properties that make up `\p{Letter}`.

The regex crate will actually see the alternation of classes at the syntax
level and join them into a single class.

`python/regex` is quite interesting here because it does about an order of
magnitude worse on this benchmark than in `letters-one`. (Although still
quite a bit better than the regex crate.) It seems likely that it has some
kind of optimization for bounded repeats that kicks in, but the alternation
slows it down because it is, after, a backtracker.
'''

[[bench]]
model = "count"
name = "letters-lower-or-upper"
regex = '(?:\p{Lowercase}|\p{Uppercase}){100}'
haystack = { path = "unicode/allcodepoints.txt" }
unicode = true
count = 13
engines = [
  'rust/regex',
  'rust/regexold',
  # 're2', # appears to crap out and reports 0 matches
  # 'go/regexp', # insufficient Unicode property support
  'pcre2/jit',
  'python/regex',
]
analysis = '''
This is yet another variant on the "letters" idea, but we only select a subset
here: uppercase and lowercase. This does substantially reduce the match
count, but that doesn't matter so much for perf. (Change `\pL{100}` above to
`\pL{100}x` and perf generally remains the same for the regex crate.)

But! This does result in the regex becoming substantially smaller. Small
enough for the lazy DFA's default cache size to handle it. This makes it nice
and fast. You can get a similar speed on `letters-alt` if you increase the lazy
DFA's cache capacity.
'''

[[bench]]
model = "count"
name = "contiguous-greek"
regex = '\p{Greek}+'
haystack = { path = "unicode/allcodepoints.txt" }
unicode = true
count = [
  # Perhaps a different version of Unicode?
  { engine = 'pcre2', count = 38 },
  { engine = 'pcre2/jit', count = 38 },
  { engine = '.*', count = 36 },
]
engines = [
  'go/regexp',
  'pcre2',
  'pcre2/jit',
  'python/regex',
  're2',
  'rust/regex',
  'rust/regexold',
]
