<!-- Auto-generated by rebar, do not edit manually! -->
<!-- Generated with command: -->
<!-- rebar report --statistic median --units throughput record/all/2023-04-11/dotnet-compiled.csv record/all/2023-04-11/dotnet.csv record/all/2023-04-11/dotnet-nobacktrack.csv record/all/2023-04-11/go-regexp.csv record/all/2023-04-11/hyperscan.csv record/all/2023-04-11/icu.csv record/all/2023-04-11/java-hotspot.csv record/all/2023-04-11/javascript-v8.csv record/all/2023-04-11/pcre2.csv record/all/2023-04-11/pcre2-jit.csv record/all/2023-04-11/perl.csv record/all/2023-04-11/python-re.csv record/all/2023-04-11/python-regex.csv record/all/2023-04-11/re2.csv record/all/2023-04-11/regress.csv record/all/2023-04-11/rust-aho-corasick-dfa.csv record/all/2023-04-11/rust-aho-corasick-nfa.csv record/all/2023-04-11/rust-memchr-memmem.csv record/all/2023-04-11/rust-regex-ast.csv record/all/2023-04-11/rust-regex-backtrack.csv record/all/2023-04-11/rust-regex.csv record/all/2023-04-11/rust-regex-dense.csv record/all/2023-04-11/rust-regex-hir.csv record/all/2023-04-11/rust-regex-hybrid.csv record/all/2023-04-11/rust-regex-meta.csv record/all/2023-04-11/rust-regex-nfa.csv record/all/2023-04-11/rust-regexold.csv record/all/2023-04-11/rust-regex-onepass.csv record/all/2023-04-11/rust-regex-pikevm.csv record/all/2023-04-11/rust-regex-sparse.csv -e ^(?:dotnet|dotnet/compiled|dotnet/nobacktrack|go/regexp|hyperscan|icu|java/hotspot|javascript/v8|pcre2|pcre2/jit|perl|python/re|python/regex|re2|regress|rust/regex|rust/regex/meta)$ -->
### Summary

Below are two tables summarizing the results of regex engines benchmarked.
Each regex engine includes its version at the time measurements were captured,
a summary score that ranks it relative to other regex engines across all
benchmarks and the total number of measurements collected.

The first table ranks regex engines based on search time. The second table
ranks regex engines based on compile time.

The summary statistic used is the [geometric mean] of the speed ratios for
each regex engine across all benchmarks that include it. The ratios within
each benchmark are computed from the median of all timing samples taken, and
dividing it by the best median of the regex engines that participated in the
benchmark. For example, given two regex engines `A` and `B` with results `35
ns` and `25 ns` on a single benchmark, `A` has a speed ratio of `1.4` and
`B` has a speed ratio of `1.0`. The geometric mean reported here is then the
"average" speed ratio for that regex engine across all benchmarks.

Each regex engine is linked to the directory containing the runner program
responsible for compiling a regex, using it in a search and reporting timing
results. Each directory contains a `README` file briefly describing any engine
specific details for the runner program.

Each regex engine is also defined in
[benchmarks/engines.toml](benchmarks/engines.toml), using the same name listed
in the table below. Each definition includes instructions for how to run,
build, clean and obtain the version of each regex engine.

**Caution**: Using a single number to describe the overall performance of a
regex engine is a fraught endeavor, and it is debatable whether it should be
included here at all. It is included primarily because the number of benchmarks
is quite large and overwhelming. It can be quite difficult to get a general
sense of things without a summary statistic. In particular, a summary statistic
is also useful to observe how the _overall picture_ itself changes as changes
are made to the barometer. (Whether it be by adding new regex engines or
adding/removing/changing existing benchmarks.) One particular word of caution
is that while geometric mean is more robust with respect to outliers than
arithmetic mean, it is not unaffected by them. Therefore, it is still critical
to examine individual benchmarks if one wants to better understanding the
performance profile of any specific regex engine or workload.

[geometric mean]: https://dl.acm.org/doi/pdf/10.1145/5666.5673

#### Summary of search-time benchmarks

| Engine | Version | Geometric mean of speed ratios | Benchmark count |
| ------ | ------- | ------------------------------ | --------------- |
| [rust/regex/meta](engines/rust/regex-automata) | 0.2.0 | 1.66 | 242 |
| [hyperscan](engines/hyperscan) | 5.4.1 2023-02-22 | 1.76 | 76 |
| [rust/regex](engines/rust/regex) | 1.7.2 | 3.67 | 69 |
| [pcre2/jit](engines/pcre2) | 10.42 2022-12-11 | 4.07 | 200 |
| [re2](engines/re2) | 2023-03-01 | 6.97 | 196 |
| [dotnet/compiled](engines/dotnet) | 7.0.3 | 7.53 | 57 |
| [dotnet/nobacktrack](engines/dotnet) | 7.0.3 | 9.02 | 52 |
| [javascript/v8](engines/javascript) | 19.7.0 | 16.78 | 53 |
| [dotnet](engines/dotnet) | 7.0.3 | 17.91 | 1 |
| [regress](engines/regress) | 0.5.0 | 29.33 | 145 |
| [python/regex](engines/python) | 2023.3.23 | 32.89 | 179 |
| [perl](engines/perl) | 5.36.0 | 37.35 | 56 |
| [pcre2](engines/pcre2) | 10.42 2022-12-11 | 40.25 | 144 |
| [python/re](engines/python) | 3.10.9 | 62.25 | 153 |
| [icu](engines/icu) | 72.1.0 | 63.49 | 55 |
| [java/hotspot](engines/java) | 20+36-2344 | 68.43 | 57 |
| [go/regexp](engines/go) | 1.20.1 | 70.03 | 188 |

#### Summary of compile-time benchmarks

| Engine | Version | Geometric mean of speed ratios | Benchmark count |
| ------ | ------- | ------------------------------ | --------------- |
| [pcre2](engines/pcre2) | 10.42 2022-12-11 | 1.31 | 12 |
| [regress](engines/regress) | 0.5.0 | 2.61 | 11 |
| [icu](engines/icu) | 72.1.0 | 2.92 | 11 |
| [pcre2/jit](engines/pcre2) | 10.42 2022-12-11 | 3.60 | 20 |
| [go/regexp](engines/go) | 1.20.1 | 6.66 | 21 |
| [re2](engines/re2) | 2023-03-01 | 10.75 | 21 |
| [rust/regex/meta](engines/rust/regex-automata) | 0.2.0 | 12.44 | 27 |
| [rust/regex](engines/rust/regex) | 1.7.2 | 17.71 | 11 |
| [dotnet/compiled](engines/dotnet) | 7.0.3 | 19.52 | 10 |
| [python/re](engines/python) | 3.10.9 | 37.31 | 14 |
| [python/regex](engines/python) | 2023.3.23 | 61.70 | 20 |
| [dotnet/nobacktrack](engines/dotnet) | 7.0.3 | 130.38 | 6 |
| [hyperscan](engines/hyperscan) | 5.4.1 2023-02-22 | 570.66 | 13 |

### Benchmark Groups

Below is a list of links to each benchmark group in this particular barometer.
Each benchmark group contains 1 or more related benchmarks. The idea of each
group is to tell some kind of story about related workloads, and to give
a sense of how performance changes based on the variations between each
benchmark.

* [captures](#captures)
* [curated](#curated)
  * [literal](#literal)
  * [literal-alternate](#literal-alternate)
  * [date](#date)
  * [ruff-noqa](#ruff-noqa)
  * [lexer-veryl](#lexer-veryl)
  * [cloud-flare-redos](#cloud-flare-redos)
  * [unicode-character-data](#unicode-character-data)
  * [words](#words)
  * [aws-keys](#aws-keys)
  * [bounded-repeat](#bounded-repeat)
  * [unstructured-to-json](#unstructured-to-json)
  * [dictionary](#dictionary)
  * [noseyparker](#noseyparker)
* [dictionary](#dictionary)
  * [compile](#compile)
  * [search](#search)
* [folly](#folly)
* [grep](#grep)
* [hyperscan](#hyperscan)
* [imported](#imported)
  * [leipzig](#leipzig)
  * [lh3lh3-reb](#lh3lh3-reb)
  * [mariomka](#mariomka)
  * [regex-redux](#regex-redux)
  * [rsc](#rsc)
  * [sherlock](#sherlock)
* [opt](#opt)
  * [accelerate](#accelerate)
  * [backtrack](#backtrack)
  * [fixed-length](#fixed-length)
  * [literal-alt](#literal-alt)
  * [onepass](#onepass)
  * [prefilter](#prefilter)
  * [reverse-anchored](#reverse-anchored)
  * [reverse-inner](#reverse-inner)
  * [reverse-suffix](#reverse-suffix)
* [reported](#reported)
  * [i787-keywords](#i787-keywords)
* [slow](#slow)
* [unicode](#unicode)
  * [codepoints](#codepoints)
  * [compile](#compile)
  * [overlapping-words](#overlapping-words)
  * [word](#word)
* [wild](#wild)
  * [bibleref](#bibleref)
  * [caddy](#caddy)
  * [dot-star-capture](#dot-star-capture)
  * [grapheme](#grapheme)
  * [parol-veryl](#parol-veryl)
  * [ruff](#ruff)
  * [rustsec-cargo-audit](#rustsec-cargo-audit)
  * [url](#url)

### captures

These benchmarks specifically try to measure the performance of tasks that
require resolving capturing groups. Backtrackers tend to have the advantage
here, since the core algorithm they use also handles captures. But automata
oriented engines like the regex crate usually rely on faster DFAs as a work
horse, but those DFAs generally can't resolve capture groups. So dealing with
capture groups usually requires additionally running a slower engine.

These benchmarks tend to have high match counts because otherwise we would
mostly just be measuring the throughput of DFAs in the automata oriented
engines. We already know they tend to do well, and their characteristics are
well represented in other benchmarks. So here we try to focus measurements
more on tasks that spend the majority of their time resolving capture groups.

Sadly, this also entangles latency measurement with resolving capture groups.
Namely, if a regex engine has generally higher latency (and the regex crate
kind of does at time of writing), then it's going to get penalized on these
benchmarks. But that's just the way the cookie crumbles.

| Engine | contiguous-letters |
| - | - |
| go/regexp | 1081.6 KB/s |
| pcre2/jit | **21.0 MB/s** |
| python/re | 1851.7 KB/s |
| re2 | 5.7 MB/s |
| regress | 14.6 MB/s |
| rust/regex/meta | 10.4 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**contiguous-letters**

| Parameter | Value |
| --------- | ----- |
| full name | `captures/contiguous-letters` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex | `````(?:(a+)\|(b+)\|(c+)\|(d+)\|(e+)\|(f+)\|(g+)\|(h+)\|(i+)\|(j+)\|(k+)\|(l+)\|(m+)\|(n+)\|(o+)\|(p+)\|(q+)\|(r+)\|(s+)\|(t+)\|(u+)\|(v+)\|(w+)\|(x+)\|(y+)\|(z+))````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-medium.txt`](benchmarks/haystacks/opensubtitles/en-medium.txt) |
| count(`.*`) | 81494 |

This tests a case where there are lots of capture groups and also lots of
matches. PCRE2's JIT reigns supreme here, but pretty much all of the regex
engines benchmarked here slow down quite a bit. Down to the 1-20 MB/s area on
my machine.

</details>

### curated

#### literal

This group of benchmarks measures regex patterns that are simple literals. When
possible, we also measure case insensitive versions of the same pattern. We do
this across three languages: English, Russian and Chinese. For English, Unicode
mode is disabled while it is enabled for Russian and Chinese. (Which mostly
only matters for the case insensitive benchmarks.)

This group is mainly meant to demonstrate two things. Firstly is whether the
regex engine does some of the most basic forms of optimization by recognizing
that a pattern is just a literal, and that a full blown regex engine is
probably not needed. Indeed, naively using a regex engine for this case is
likely to produce measurements much worse than most regex engines. Secondly is
how the performance of simple literal searches changes with respect to both
case insensitivity and Unicode. Namely, substring search algorithms that work
well on ASCII text don't necessarily also work well on UTF-8 that contains many
non-ASCII codepoints. This is especially true for case insensitive searches.

Notice, for example, how RE2 seems to be faster in the `sherlock-casei-ru`
benchmark than in the `sherlock-ru` benchmark, even though the latter is "just"
a simple substring search where as the former is a multiple substring search.
In the case of `sherlock-ru`, RE2 actually attempts a literal optimization that
likely gets caught up in dealing with a high false positive rate of candidates.
Where as in the case of `sherlock-casei-ru`, no literal optimization is
attempted and instead its lazy DFA is used. The high false positive rate in the
simpler literal case winds up making it overall slower than it likely would be
if it would just use the DFA.

This is not in any way to pick on RE2. Every regex engine that does literal
optimizations (and most do) will suffer from this kind of setback in one way
or another.

| Engine | sherlock-en | sherlock-casei-en | sherlock-ru | sherlock-casei-ru | sherlock-zh |
| - | - | - | - | - | - |
| dotnet/compiled | 12.6 GB/s | 6.1 GB/s | 23.1 GB/s | 5.1 GB/s | 38.9 GB/s |
| dotnet/nobacktrack | 7.8 GB/s | 4.1 GB/s | 7.1 GB/s | 2.4 GB/s | 27.1 GB/s |
| go/regexp | 3.9 GB/s | 42.9 MB/s | 2.1 GB/s | 32.9 MB/s | 2035.7 MB/s |
| hyperscan | **34.8 GB/s** | **29.5 GB/s** | 4.4 GB/s | 7.3 GB/s | **48.7 GB/s** |
| icu | 1590.6 MB/s | 451.4 MB/s | 3.0 GB/s | 281.0 MB/s | 4.2 GB/s |
| java/hotspot | 2.4 GB/s | 280.3 MB/s | 3.6 GB/s | 222.9 MB/s | 5.2 GB/s |
| javascript/v8 | 6.3 GB/s | 2.9 GB/s | **41.4 GB/s** | 3.3 GB/s | 10.4 GB/s |
| pcre2 | 7.3 GB/s | 988.7 MB/s | 2.0 MB/s | 1990.7 KB/s | 57.3 MB/s |
| pcre2/jit | 26.4 GB/s | 16.9 GB/s | 31.6 GB/s | **17.7 GB/s** | 36.8 GB/s |
| perl | 2.6 GB/s | 556.9 MB/s | 3.3 GB/s | 98.7 MB/s | 7.8 GB/s |
| python/re | 3.7 GB/s | 295.7 MB/s | 6.6 GB/s | 441.8 MB/s | 11.2 GB/s |
| python/regex | 3.6 GB/s | 3.1 GB/s | 4.7 GB/s | 4.0 GB/s | 5.6 GB/s |
| re2 | 11.9 GB/s | 2.5 GB/s | 745.2 MB/s | 942.0 MB/s | 2.8 GB/s |
| regress | 3.6 GB/s | 1151.5 MB/s | 3.6 GB/s | 319.4 MB/s | 3.6 GB/s |
| rust/regex | 31.2 GB/s | 8.9 GB/s | 33.3 GB/s | 8.2 GB/s | 39.4 GB/s |
| rust/regex/meta | 31.8 GB/s | 10.7 GB/s | 31.5 GB/s | 8.9 GB/s | 39.1 GB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**sherlock-en**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/01-literal/sherlock-en` |
| model | [`count`](MODELS.md#count) |
| regex | `````Sherlock Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-sampled.txt`](benchmarks/haystacks/opensubtitles/en-sampled.txt) |
| count(`.*`) | 513 |


**sherlock-casei-en**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/01-literal/sherlock-casei-en` |
| model | [`count`](MODELS.md#count) |
| regex | `````Sherlock Holmes````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-sampled.txt`](benchmarks/haystacks/opensubtitles/en-sampled.txt) |
| count(`.*`) | 522 |


**sherlock-ru**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/01-literal/sherlock-ru` |
| model | [`count`](MODELS.md#count) |
| regex | `````Шерлок Холмс````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-sampled.txt`](benchmarks/haystacks/opensubtitles/ru-sampled.txt) |
| count(`.*`) | 724 |


**sherlock-casei-ru**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/01-literal/sherlock-casei-ru` |
| model | [`count`](MODELS.md#count) |
| regex | `````Шерлок Холмс````` |
| case-insensitive | `true` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-sampled.txt`](benchmarks/haystacks/opensubtitles/ru-sampled.txt) |
| count(`.*`) | 746 |


**sherlock-zh**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/01-literal/sherlock-zh` |
| model | [`count`](MODELS.md#count) |
| regex | `````夏洛克·福尔摩斯````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/zh-sampled.txt`](benchmarks/haystacks/opensubtitles/zh-sampled.txt) |
| count(`.*`) | 30 |


</details>

#### literal-alternate

This group is like `literal`, but expands the complexity from a simple literal
to a small alternation of simple literals, including case insensitive variants
where applicable. Once again, we do this across three languages: English,
Russian and Chinese. We disable Unicode mode for English but enable it for
Russian and Chinese. Enabling Unicode here generally only means that case
insensitivity takes Unicode case folding rules into account.

This benchmark ups the ante when it comes to literal optimizations. Namely,
for a regex engine to optimize this case, it generally needs to be capable of
reasoning about literal optimizations that require one or more literals from
a set to match. Many regex engines don't deal with this case well, or at all.
For example, after a quick scan at comparing the `sherlock-en` benchmark here
and in the previous `literal` group, one thing that should stand out is the
proportion of regex engines that now measure throughput in MB/s instead of
GB/s.

One of the difficulties in optimizing for this case is that multiple substring
search is difficult to do in a way that is fast. In particular, this benchmark
carefully selected each alternation literal to start with a different character
than the other alternation literals. This, for example, inhibits clever regex
engines from noticing that all literals begin with the same byte (or small
number of bytes). Consider an alternation like `foo|far|fight`. It is not hard
to see that a regex engine _could_ just scan for the letter `f` as a prefilter
optimization. Here, we pick our regex such that this sort of shortcut isn't
available. For the regex engine to optimize this case, it really needs to deal
with the problem of multiple substring search.

Multiple substring search _can_ be implemented via a DFA, and perhaps in some
cases, quite quickly via a [shift DFA]. Beyond that though, multiple substring
search can be implemented by other various algorithms such as Aho-Corasick or
Rabin-Karp. (The standard Aho-Corasick formulation is an NFA, but it can also
be converted to a DFA by pre-computing all failure transitions. This winds up
with a similar result as using Thompson's construction to produce an NFA and
then powerset construction to get a DFA, but the Aho-Corasick construction
algorithm is usually quite a bit faster because it doesn't need to deal with a
full NFA.)

The problem here is that DFA speeds may or may not help you. For example, in
the case of RE2 and Rust's regex engine, it will already get DFA speeds by
virtue of their lazy DFAs. Indeed, in this group, RE2 performs roughly the same
across all benchmarks. So even if you, say build an Aho-Corasick DFA, it's not
going to help much if at all. So it makes sense to avoid it.

But Rust's regex crate has quite a bit higher throughputs than RE2 on most of
the benchmarks in this group. So how is it done? Currently, this is done via
the [Teddy] algorithm, which was ported out of [Hyperscan]. It is an algorithm
that makes use of SIMD to accelerate searching for a somewhat small set of
literals. Most regex engines don't have this sort of optimization, and indeed,
it seems like Teddy is not particularly well known. Alas, regex engines that
want to move past typical DFA speeds for multiple substring search likely need
some kind of vectorized algorithm to do so. (Teddy is also used by Rust's
regex crate in the previous `literal` group of benchmarks for accelerating
case insensitive searches. Namely, it enumerates some finite set of prefixes
like `she`, `SHE`, `ShE` and so on, and then looks for matches of those as a
prefilter.)

[shift DFA]: https://gist.github.com/pervognsen/218ea17743e1442e59bb60d29b1aa725
[Teddy]: https://github.com/BurntSushi/aho-corasick/tree/4e7fa3b85dd3a3ce882896f1d4ee22b1f271f0b4/src/packed/teddy
[Hyperscan]: https://github.com/intel/hyperscan

| Engine | sherlock-en | sherlock-casei-en | sherlock-ru | sherlock-casei-ru | sherlock-zh |
| - | - | - | - | - | - |
| dotnet/compiled | 3.7 GB/s | 446.7 MB/s | 2.2 GB/s | 780.1 MB/s | **16.8 GB/s** |
| dotnet/nobacktrack | 2.6 GB/s | 382.8 MB/s | 1025.9 MB/s | 295.4 MB/s | 12.1 GB/s |
| go/regexp | 24.8 MB/s | 15.4 MB/s | 32.3 MB/s | 9.0 MB/s | 45.5 MB/s |
| hyperscan | **17.2 GB/s** | **13.2 GB/s** | 4.6 GB/s | **3.9 GB/s** | 16.5 GB/s |
| icu | 635.2 MB/s | 113.3 MB/s | 168.3 MB/s | 106.9 MB/s | 335.8 MB/s |
| java/hotspot | 69.8 MB/s | 65.0 MB/s | 119.4 MB/s | 56.2 MB/s | 176.3 MB/s |
| javascript/v8 | 686.1 MB/s | 675.3 MB/s | 942.0 MB/s | 599.1 MB/s | 6.7 GB/s |
| pcre2 | 907.2 MB/s | 176.8 MB/s | 1697.6 KB/s | 1581.2 KB/s | 8.4 MB/s |
| pcre2/jit | 1557.2 MB/s | 649.7 MB/s | 1188.7 MB/s | 297.2 MB/s | 2.5 GB/s |
| perl | 1087.7 MB/s | 113.9 MB/s | 129.7 MB/s | 73.2 MB/s | 208.0 MB/s |
| python/re | 437.5 MB/s | 36.7 MB/s | 411.5 MB/s | 55.5 MB/s | 1030.8 MB/s |
| python/regex | 297.8 MB/s | 68.6 MB/s | 301.4 MB/s | 85.2 MB/s | 871.5 MB/s |
| re2 | 923.7 MB/s | 921.9 MB/s | 930.3 MB/s | 930.3 MB/s | 947.1 MB/s |
| regress | 1519.7 MB/s | 274.9 MB/s | 239.6 MB/s | 108.9 MB/s | 245.5 MB/s |
| rust/regex | 13.1 GB/s | 2.7 GB/s | 6.1 GB/s | 1635.9 MB/s | 15.1 GB/s |
| rust/regex/meta | 11.4 GB/s | 2.9 GB/s | **6.6 GB/s** | 1678.7 MB/s | 12.6 GB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**sherlock-en**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/02-literal-alternate/sherlock-en` |
| model | [`count`](MODELS.md#count) |
| regex | `````Sherlock Holmes\|John Watson\|Irene Adler\|Inspector Lestrade\|Professor Moriarty````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-sampled.txt`](benchmarks/haystacks/opensubtitles/en-sampled.txt) |
| count(`.*`) | 714 |


**sherlock-casei-en**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/02-literal-alternate/sherlock-casei-en` |
| model | [`count`](MODELS.md#count) |
| regex | `````Sherlock Holmes\|John Watson\|Irene Adler\|Inspector Lestrade\|Professor Moriarty````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-sampled.txt`](benchmarks/haystacks/opensubtitles/en-sampled.txt) |
| count(`.*`) | 725 |


**sherlock-ru**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/02-literal-alternate/sherlock-ru` |
| model | [`count`](MODELS.md#count) |
| regex | `````Шерлок Холмс\|Джон Уотсон\|Ирен Адлер\|инспектор Лестрейд\|профессор Мориарти````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-sampled.txt`](benchmarks/haystacks/opensubtitles/ru-sampled.txt) |
| count(`.*`) | 899 |


**sherlock-casei-ru**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/02-literal-alternate/sherlock-casei-ru` |
| model | [`count`](MODELS.md#count) |
| regex | `````Шерлок Холмс\|Джон Уотсон\|Ирен Адлер\|инспектор Лестрейд\|профессор Мориарти````` |
| case-insensitive | `true` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-sampled.txt`](benchmarks/haystacks/opensubtitles/ru-sampled.txt) |
| count(`.*`) | 971 |


**sherlock-zh**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/02-literal-alternate/sherlock-zh` |
| model | [`count`](MODELS.md#count) |
| regex | `````夏洛克·福尔摩斯\|约翰华生\|阿德勒\|雷斯垂德\|莫里亚蒂教授````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/zh-sampled.txt`](benchmarks/haystacks/opensubtitles/zh-sampled.txt) |
| count(`.*`) | 207 |


</details>

#### date

This is a monster regex for extracting dates from unstructured text from
the [datefinder] project written in Python. The regex itself was taken from
[printing the `DATES_PATTERN`][datefinder-regex] variable in the `datefinder`
project. I then removed all names from the capture groups, unnecessary escapes
and collapsed it to a single line (because not all regex engines support
verbose mode).

The regex is more akin to a tokenizer, and the `datefinder` library attempts to
combine these tokens into timestamps.

We measure an ASCII only version of it and a Unicode-aware version of it.
Unicode is relevant here because of case insensitivity, and because the regex
makes use of the character classes `\s` and `\d`, which are bigger when they're
Unicode aware. We also measure the compilation time of each.

The results here can be a little tricky to interpret. Namely, it looks like
backtrackers tend to do worse than automata oriented regex engines, but
`go/regexp` uses automata and is itself quite slow here. Notice, though, that
`hyperscan`, `re2` and `rust/regex` do well here. While I'm less familiar with
`hyperscan`, the explanation for `re2` and `rust/regex` is obvious once you
look at a profile: it's the lazy DFA. Both have implementations of a regex
engine that build a DFA during search time, with at most one new transition
(and one new state) being create per byte of haystack. In practice, most
transitions get reused, which means that it tends to act like a real DFA most
of the time for most regexes on most haystacks.

Compilation time of this monster regex is also all over the place. PCRE2 does
the best, and Hyperscan winds up being quite slow. Once you enable Unicode
mode, compilation time generally gets worse, and especially so for `re2` and
`rust/regex`. In particular, both compile _byte oriented_ automata, which means
the transitions are defined over bytes and not codepoints. That means large
Unicode classes like `\d` tend to balloon in size, because they get converted
into UTF-8 automata.

[datefinder]: https://github.com/akoumjian/datefinder/tree/master
[datefinder-regex]: https://github.com/akoumjian/datefinder/blob/5376ece0a522c44762b1ab656fc80737b427ed16/datefinder/constants.py#L112-L124

| Engine | ascii | unicode | compile-ascii | compile-unicode |
| - | - | - | - | - |
| dotnet/compiled | 1080.9 KB/s | 1163.3 KB/s | - | 1.65ms |
| go/regexp | 252.4 KB/s | - | 4.10ms | - |
| hyperscan | 105.1 MB/s | - | 647.53ms | - |
| icu | 320.3 KB/s | 321.0 KB/s | 431.44us | 430.62us |
| java/hotspot | 2.1 MB/s | 1624.6 KB/s | - | - |
| javascript/v8 | 31.2 MB/s | 28.3 MB/s | - | - |
| pcre2 | 1257.2 KB/s | 179.8 KB/s | **116.09us** | **132.42us** |
| pcre2/jit | 21.2 MB/s | 13.0 MB/s | 702.94us | 0.97ms |
| perl | 2.8 MB/s | - | - | - |
| python/re | 1077.7 KB/s | 796.8 KB/s | 4.77ms | 5.02ms |
| python/regex | 1118.2 KB/s | 988.2 KB/s | 15.44ms | 36.44ms |
| re2 | 75.6 MB/s | - | 1.17ms | - |
| regress | 1926.6 KB/s | 1934.0 KB/s | 1.21ms | 1.21ms |
| rust/regex | **140.1 MB/s** | **140.1 MB/s** | 1.93ms | 6.26ms |
| rust/regex/meta | 122.7 MB/s | 122.1 MB/s | 1.49ms | 5.20ms |

<details>
<summary>Show individual benchmark parameters.</summary>

**ascii**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/03-date/ascii` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex-path | [`wild/date.txt`](benchmarks/regexes/wild/date.txt) |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`dotnet.*`) | 111825 |
| count(`hyperscan`) | 547662 |
| count(`icu`) | 111825 |
| count(`javascript/v8`) | 111825 |
| count(`regress`) | 111841 |
| count(`.*`) | 111817 |

As with many other benchmarks, Hyperscan reports all matches, even ones that
are overlapping. This particular regex is too big to analyze closely, but it
seems plausible one could still use it (possibly with a slightly tweaked regex)
for this task.

**unicode**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/03-date/unicode` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex-path | [`wild/date.txt`](benchmarks/regexes/wild/date.txt) |
| case-insensitive | `true` |
| unicode | `true` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`dotnet/compiled|icu|java/hotspot|javascript/v8`) | 111825 |
| count(`.*`) | 111841 |

`regress` is included here despite its `\d` not being Unicode-aware (as
required by ECMAScript). Notably, its `\s` _is_ Unicode aware. (`\w` is too,
but it's not used in this regex.) In this particular haystack, `\d` being
ASCII-only doesn't impact the match count.

However, neither `re2` nor `go/regexp` are included here because neither `\d`
nor `\s` are Unicode-aware, and the `\s` being ASCII-only does impact the match
count.

`hyperscan` is excluded here because the pattern results in a "too large"
compilation error. As far as I know, Hyperscan doesn't expose any knobs for
increasing this limit.

`dotnet/compiled` gets a different count here, but it's not clear why.

`perl` is left out of this benchmark because it times out.

**compile-ascii**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/03-date/compile-ascii` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`wild/date.txt`](benchmarks/regexes/wild/date.txt) |
| case-insensitive | `true` |
| unicode | `false` |
| haystack | `2010-03-14` |
| count(`hyperscan`) | 10 |
| count(`.*`) | 5 |

Notice that `regress` is now include in the ASCII benchmark, because in
`compile-unicode` we specifically test that the `\d` used in this regex is
Unicode-aware. `regress` does not make `\d` Unicode-aware, so it gets thrown
into the ASCII group. But do note that it does appear to have some Unicode
awareness.

**compile-unicode**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/03-date/compile-unicode` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`wild/date.txt`](benchmarks/regexes/wild/date.txt) |
| case-insensitive | `true` |
| unicode | `true` |
| haystack | `۲۰۱۰-۰۳-۱۴` |
| count(`javascript/v8|regress`) | 2 |
| count(`.*`) | 5 |

We use "extended arabic-indic digits" to represent the same date, `2010-03-14`,
that we use for verification in `compile-ascii`. These digits are part of `\d`
when it is Unicode aware.

</details>

#### ruff-noqa

The regex benchmarked here comes from the [Ruff project][ruff], which is a
Python linter written in Rust. The project uses many regexes, but [we pluck
one out in particular][noqa] that is likely to be run more frequently than the
others:

```
(\s*)((?i:# noqa)(?::\s?(([A-Z]+[0-9]+(?:[,\s]+)?)+))?)
```

This is a regex that looks for `# noqa` annotations on each line. The `noqa`
annotation generally causes the linter to ignore those lines with respect to
warnings it emits. The regex also tries to extract annotations following the
`noqa` that permit ignoring only specific rules in the linter.

This benchmark has a few interesting characteristics worth pointing out:

* It is line oriented, which means the haystacks it searches are likely to be
small. This in turn means that the overhead of the regex engine is likely to
matter more than in throughput oriented benchmarks.
* On this particular haystack (the CPython source code), the number of matches
is quite small. Therefore, it is quite beneficial here to be able to have a
fast path to say "there is no match" without doing any extra work. While the
number of matches here is perhaps uncharacteristically small for a Python
project, you would generally expect _most_ lines to not have `# noqa` in them,
and so the presumption of a fast rejection is probably a decent assumption for
this particular regex.
* Ruff uses capturing groups to pick out parts of the match, so when a match
is found, the regex engine needs to report additional information beyond just
the overall match spans. The spans of each matching capture group also need
to be reported.
* There are no prefix (or suffix) literals in the regex to enable any
straight-forward prefilter optimizations.

With respect to the point about no prefix or suffix literals, we also include
a tweaked version of the regex that removes the leading `(\s*)`:

```
(?i:# noqa)(?::\s?(([A-Z]+[0-9]+(?:[,\s]+)?)+))?
```

In this case, the regex now starts with a literal, albeit one that is asked
to match case insensitively. We can actually see pretty clearly the impact
the tweaked version has on the speed for each regex engine. `pcre2/jit`, for
example, improves its throughput from around 500 MB/s to 1.5 GB/s. `go/regexp`
has an even more dramatic (relatively speaking) improvement.

`rust/regex` is a little different in that it's quite fast in both cases.
The key optimization that applies for `rust/regex` is the "reverse inner"
optimization. Even in the original regex, `rust/regex` will pluck out the `#
noqa` literal and search for it case insensitively. When a candidate is found,
it then searches for `(\s*)` in reverse to find the start position, and then
finally does a standard forward search from that point to find the reverse
position.

[ruff]: https://github.com/charliermarsh/ruff
[noqa]: https://github.com/charliermarsh/ruff/blob/5c987874c48e6ed5d0ef7f9a09c4cb1940bd4018/crates/ruff/src/noqa.rs#L22

| Engine | real | tweaked | compile-real |
| - | - | - | - |
| dotnet/compiled | 155.6 MB/s | 502.1 MB/s | 47.10us |
| dotnet/nobacktrack | 240.7 MB/s | 451.4 MB/s | 413.02us |
| go/regexp | 33.0 MB/s | 739.4 MB/s | 8.95us |
| icu | 20.5 MB/s | 320.7 MB/s | 5.46us |
| java/hotspot | 37.4 MB/s | 199.5 MB/s | - |
| pcre2 | 131.4 MB/s | 1405.0 MB/s | **1.11us** |
| pcre2/jit | 571.7 MB/s | 1492.2 MB/s | 6.85us |
| perl | 102.8 MB/s | 130.9 MB/s | - |
| python/re | 28.7 MB/s | 109.9 MB/s | 74.07us |
| python/regex | 97.0 MB/s | 97.1 MB/s | 184.69us |
| re2 | 537.7 MB/s | 713.7 MB/s | 7.78us |
| rust/regex | 731.3 MB/s | 1319.5 MB/s | 31.20us |
| rust/regex/meta | **1652.0 MB/s** | **1562.1 MB/s** | 55.87us |

<details>
<summary>Show individual benchmark parameters.</summary>

**real**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/04-ruff-noqa/real` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````(\s*)((?i:# noqa)(?::\s?(([A-Z]+[0-9]+(?:[,\s]+)?)+))?)````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/cpython-226484e4.py`](benchmarks/haystacks/wild/cpython-226484e4.py) |
| count(`.*`) | 84 |


**tweaked**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/04-ruff-noqa/tweaked` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````(?i:# noqa)(?::\s?(([A-Z]+[0-9]+(?:[,\s]+)?)+))?````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/cpython-226484e4.py`](benchmarks/haystacks/wild/cpython-226484e4.py) |
| count(`.*`) | 44 |


**compile-real**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/04-ruff-noqa/compile-real` |
| model | [`compile`](MODELS.md#compile) |
| regex | `````(\s*)((?i:# noqa)(?::\s?(([A-Z]+[0-9]+(?:[,\s]+)?)+))?)````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `# noqa` |
| count(`.*`) | 1 |


</details>

#### lexer-veryl

This group benchmarks a "lexer" where it combines a whole bunch of different
patterns that identify tokens in a language into a single regex. It then uses
capture groups to determine which branch of the alternation actually matched,
and thus, which token matched. We also benchmark a variant of this that asks
the regex engine to search for each pattern individually (most regex engines
don't support this mode).

This is used by the [Veryl] project by way of the [Parol] parser generator. The
regex was [extracted by the Parol maintainers upon my request][parol-issue].

We use this regex to represent the "lexing" use case, where sometimes folks
will build a pretty big regex with a bunch of small regexes for identifying
tokens. Usually the idea is that the lexer matches literally everything in the
haystack (indeed, the last branch in this regex is a `.` and the first is any
newline), and thus these sorts of regexes tend to be quite latency sensitive.
Namely, it really matters just how much overhead is involved in reporting
matches. This is likely one of the reasons why most regex engines are overall
pretty slow here.

The other aspect of this that's quite difficult is the sheer number of
capturing groups. There's several dozen of them, which means regex engines have
to keep track of a fair bit of state to handle it.

You might think this would be bad for backtrackers and good for automata
engines, since automata engines are *supposed* to be able to handle large
alternations better than backtrackers. But that's not the case here. Even for
example Python's regex engine (backtracker) beats RE2 (automata). My hypothesis
for why this is, is latency. Automata engines tend to have multiple engines
internally and therefore tend to have higher latency, and sometimes multiple
engines run to service one search. Backtrackers tend to have one engine that
handles everything. But still, shouldn't the huge alternation be disastrous for
the backtracker? Perhaps, unless many of the matches occur in an early branch,
which is likely the case here. Namely, the second alternation matches a ` `
(single ASCII space), which is probably the most frequently occurring byte in
the haystack. An automata engine that doesn't use a DFA (which might be the
case here, because the regex is so big), will wind up spending a lot of time
keeping track of all branches of the alternation, even if it doesn't need to
explore all of them. In contrast, a backtracker will try one after the other,
and if most cases match an early branch, the backtracker is likely to take less
overall time.

Most regex engines are stuck in the 1 MB/s (or less) range. The regex crate and
PCRE2's JIT get up to about 10 MB/s, with PCRE2 edging out the regex crate.

Note that the regex was lightly modified from the original to increase
portability across different regex engines. For example, the `[\s--\r\n]` class
was changed to `[\t\v\f ]`.

As for the second benchmark, `multiple`, it uses the same patterns from each
alternation in the `single` benchmark, but treats each one as a distinct
pattern. Doing this requires explicit support for searching multiple regex
patterns. (RE2's and Rust's regex crate "regex set" functionality is not enough
for this, as it only reports which patterns match a haystack, and not where
they match. That's why the lower level `regex/automata/meta` engine is used,
which exposes a more powerful but more complex API.)

In the `multiple` case, the `rust/regex/meta` does very well and the key reason
is the abdication of capture groups as a necessary tool to determine which
token matched. Namely, now we can simply use a pattern ID from the match to
determine which "branch" in the original regex was taken. We no longer need to
ask for or inspect capture groups. This gives a critical benefit to automata
engines that support searching for multiple patterns, because it no longer
requires them to use slower engines for resolving capturing groups.

[Veryl]: https://github.com/dalance/veryl
[Parol]: https://github.com/jsinger67/parol
[parol-issue]: https://github.com/jsinger67/parol/issues/56

| Engine | single | compile-single | multi |
| - | - | - | - |
| dotnet/compiled | 197.1 KB/s | 278.06us | - |
| go/regexp | 321.0 KB/s | 208.31us | - |
| hyperscan | - | - | 17.6 MB/s |
| icu | 945.2 KB/s | 57.57us | - |
| java/hotspot | 6.3 MB/s | - | - |
| javascript/v8 | 6.6 MB/s | - | - |
| pcre2 | 2.8 MB/s | **24.46us** | - |
| pcre2/jit | **12.5 MB/s** | 131.83us | - |
| perl | 1167.7 KB/s | - | - |
| python/re | 1575.0 KB/s | 1.14ms | - |
| python/regex | 1427.0 KB/s | 3.77ms | - |
| re2 | 1230.0 KB/s | 146.21us | - |
| regress | 8.3 MB/s | - | - |
| rust/regex | 230.8 KB/s | 289.04us | - |
| rust/regex/meta | 9.6 MB/s | 275.38us | **64.1 MB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**single**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/05-lexer-veryl/single` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex-path | [`wild/parol-veryl.txt`](benchmarks/regexes/wild/parol-veryl.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/parol-veryl.vl`](benchmarks/haystacks/wild/parol-veryl.vl) |
| count(`.*`) | 124800 |

Note that we don't include Hyperscan here because it doesn't support the
`count-captures` benchmark model. It is included in the `multiple` benchmark
below, which doesn't require capture groups.

Also, I tried to use `dotnet/nobacktrack` here, but it failed because it was
too big and it wasn't obvious to me how to raise the limit.

**compile-single**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/05-lexer-veryl/compile-single` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`wild/parol-veryl.txt`](benchmarks/regexes/wild/parol-veryl.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `abcdefg_foobar` |
| count(`.*`) | 1 |

This measures how long it takes to a compile a moderately large lexer.

**multi**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/05-lexer-veryl/multi` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex-path | [`wild/parol-veryl.txt`](benchmarks/regexes/wild/parol-veryl.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/parol-veryl.vl`](benchmarks/haystacks/wild/parol-veryl.vl) |
| count(`hyperscan`) | 669500 |
| count(`.*`) | 150600 |

Hyperscan reports everything that matches, including overlapping matches,
and that's why its count is higher. It is likely still serviceable for
this use case, but might in practice require changing the regex to suit
Hyperscan's match semantics. Still, it's a decent barometer to include it here,
particularly because of its multi-regex support.

Most regex engines do not support searching for multiple patterns and finding
the corresponding match offsets, which is why this benchmark has very few
entries.

</details>

#### cloud-flare-redos

This benchmark uses a regex that helped cause an [outage at
Cloudflare][cloudflare-blog]. This class of vulnerability is typically called a
"regular expression denial of service," or "ReDoS" for short. It doesn't always
require a malicious actor to trigger. Since it can be difficult to reason about
the worst case performance of a regex when using an unbounded backtracking
implementation, it might happen entirely accidentally on valid inputs.

The particular regex that contributed to the outage was:

```
(?:(?:"|'|\]|\}|\\|\d|(?:nan|infinity|true|false|null|undefined|symbol|math)|`|\-|\+)+[)]*;?((?:\s|-|~|!|\{\}|\|\||\+)*.*(?:.*=.*)))
```

As discussed in Cloudflare's post mortem, the specific problematic portion of
the regex is:

```
.*(?:.*=.*)
```

Or more simply:

```
.*.*=.*;
```

We benchmark the original regex along with the simplified variant. We also
split the simplified variant into one with a short haystack (about 100 bytes)
and one with a long haystack (about 10,000 bytes). The benchmark results for
the original and simplified short variant should be roughly similar, but the
difference between the short and long variant is where things get interesting.
The automata based engines generally maintain a similar throughput for both the
short and long benchmarks, but the backtrackers slow way down. This is because
the backtracking algorithm for this specific regex and haystack doesn't scale
linearly with increases in the size of the haystack.

The purpose of this benchmark is to show a real world scenario where the use of
a backtracking engine can bite you in production if you aren't careful.

We include Hyperscan in this benchmark, although it is questionable to do so.
Hyperscan reports many overlapping matches from the regex used by Cloudflare
because of the trailing `.*`, so it is probably not a great comparison.
In particular, this regex was originally used in a firewall, so it seems
likely that it would be used in a "is a match" or "not a match" scenario. But
our benchmark here reproduces the analysis in the appendix of Cloudflare's
port mortem. But the real utility in including Hyperscan here is that it
demonstrates that it is not a backtracking engine. While its throughput is not
as high as some other engines, it remains roughly invariant with respect to
haystack length, just like other automata oriented engines.

Note that `rust/regex` has very high throughput here because the regex is
small enough to get compiled into a full DFA. The compilation process also
"accelerates" some states, particularly the final `.*`. This acceleration works
by noticing that almost all of the state's transitions loop back on itself, and
only a small number transition to another state. The final `.*` for example
only leaves its state if it sees the end of the haystack or a `\n`. So the DFA
will actually run `memchr` on `\n` and skip right to the end of the haystack.

[cloudflare-blog]: https://blog.cloudflare.com/details-of-the-cloudflare-outage-on-july-2-2019/

| Engine | original | simplified-short | simplified-long |
| - | - | - | - |
| dotnet/compiled | 131.2 MB/s | 876.3 MB/s | 13.4 GB/s |
| dotnet/nobacktrack | 12.9 MB/s | 188.2 MB/s | 289.1 MB/s |
| go/regexp | 41.3 MB/s | 44.0 MB/s | 48.3 MB/s |
| hyperscan | 85.8 MB/s | 82.4 MB/s | 85.1 MB/s |
| icu | 3.4 MB/s | 3.5 MB/s | 42.7 KB/s |
| java/hotspot | 9.1 MB/s | 6.3 MB/s | 99.8 KB/s |
| javascript/v8 | 19.4 MB/s | 18.9 MB/s | 335.4 KB/s |
| pcre2 | 2.9 MB/s | 2.8 MB/s | 30.5 KB/s |
| pcre2/jit | 49.5 MB/s | 42.5 MB/s | 670.8 KB/s |
| perl | 10.4 MB/s | 10.0 MB/s | 176.6 KB/s |
| python/re | 22.2 MB/s | 21.5 MB/s | 337.7 KB/s |
| python/regex | 6.3 MB/s | 6.2 MB/s | 91.9 KB/s |
| re2 | 348.3 MB/s | 333.1 MB/s | 493.4 MB/s |
| regress | 8.3 MB/s | 8.0 MB/s | 106.2 KB/s |
| rust/regex | 441.7 MB/s | 496.3 MB/s | 593.5 MB/s |
| rust/regex/meta | **560.7 MB/s** | **1801.4 MB/s** | **82.4 GB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**original**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/06-cloud-flare-redos/original` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````(?:(?:"\|'\|\]\|\}\|\\\|\d\|(?:nan\|infinity\|true\|false\|null\|undefined\|symbol\|math)\|`\|-\|\+)+[)]*;?((?:\s\|-\|~\|!\|\{\}\|\\|\\|\|\+)*.*(?:.*=.*)))````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `math x=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx [.. snip ..]` |
| count(`hyperscan`) | 5757 |
| count(`.*`) | 107 |


**simplified-short**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/06-cloud-flare-redos/simplified-short` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````.*.*=.*````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `x=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx [.. snip ..]` |
| count(`hyperscan`) | 5252 |
| count(`.*`) | 102 |


**simplified-long**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/06-cloud-flare-redos/simplified-long` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````.*.*=.*````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`cloud-flare-redos.txt`](benchmarks/haystacks/cloud-flare-redos.txt) |
| count(`hyperscan`) | 50004999 |
| count(`.*`) | 10000 |


</details>

#### unicode-character-data

This regex parses data from `UnicodeData.txt`, which is part of the [Unicode
Character Database][ucd]. This regex was [extracted from the `ucd-parse`
crate][ucd-parse-regex], which is part of the [ucd-generate] project.

This benchmark works by iterating over every line in the haystack and then
running the regex on each line. Every line matches the regex, so regex engines
that attempt to do some extra work to reject non-matches quickly will get
penalized. For example, `rust/regex` looks for a semi-colon first via its
"reverse inner" optimization, since a semi-colon is a required part of the
regex. But this optimization is just extra work here. Indeed, disabling it will
improve the thoughput of `rust/regex` on this benchmark.

`pcre2/jit` does remarkably well here, and these types of regexes are one of
the many things that `pcre2/jit` does quickly compared to most other regex
engines.

We also include compilation time for this regex, where PCRE2 again does quite
well.

[ucd]: https://unicode.org/ucd/
[ucd-generate]: https://github.com/BurntSushi/ucd-generate
[ucd-parse-regex]: https://github.com/BurntSushi/ucd-generate/blob/47ae5cbe739d46d3d2eed75e1326d9814d940c3f/ucd-parse/src/unicode_data.rs#L103-L124

| Engine | parse-line | compile |
| - | - | - |
| dotnet/compiled | 89.0 MB/s | 41.31us |
| dotnet/nobacktrack | 17.2 MB/s | 153.12us |
| go/regexp | 61.7 MB/s | 39.42us |
| icu | 132.7 MB/s | 13.88us |
| java/hotspot | 204.4 MB/s | - |
| javascript/v8 | 218.3 MB/s | - |
| pcre2 | 214.2 MB/s | **2.13us** |
| pcre2/jit | **696.6 MB/s** | 12.48us |
| perl | 23.2 MB/s | - |
| python/re | 45.5 MB/s | 127.82us |
| python/regex | 34.5 MB/s | 396.78us |
| re2 | 108.6 MB/s | 15.15us |
| regress | 209.5 MB/s | 5.95us |
| rust/regex | 97.5 MB/s | 22.53us |
| rust/regex/meta | 354.4 MB/s | 27.90us |

<details>
<summary>Show individual benchmark parameters.</summary>

**parse-line**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/07-unicode-character-data/parse-line` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex-path | [`wild/ucd-parse.txt`](benchmarks/regexes/wild/ucd-parse.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/UnicodeData-15.0.0.txt`](benchmarks/haystacks/wild/UnicodeData-15.0.0.txt) |
| count(`.*`) | 558784 |


**compile**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/07-unicode-character-data/compile` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`wild/ucd-parse.txt`](benchmarks/regexes/wild/ucd-parse.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `249D;PARENTHESIZED LATIN SMALL LETTER B;So;0;L;<compat> 0028 [.. snip ..]` |
| count(`.*`) | 1 |


</details>

#### words

This benchmark measures how long it takes for a regex engine to find words in
a haystack. We compare one regex that finds all words, `\b\w+\b` and another
regex that only looks for longer words, `\b\w{12,}\b`. We also compare ASCII
regexes on English text with Unicode regexes on Russian text.

The split between finding all words and finding only long words tends to
highlight the overhead of matching in each regex engine. Regex engines that are
quicker to get in and out of its match routine do better at finding all words
than regex engines that have higher overhead. For example, `regress` is faster
than `rust/regex` on `all-english`, but substantially slower than `rust/regex`
on `long-english`. This is likely because `rust/regex` is doing more work per
search call than `regress`, which is in part rooted in the optimizations it
performs to gain higher throughput.

Otherwise, `pcre2/jit` does quite well here across the board, but especially on
the Unicode variants. When comparing it against `rust/regex` for example, it
is substantially faster. In the case of `rust/regex`, its faster DFA oriented
engines cannot handle the Unicode aware `\b` on non-ASCII haystacks, and this
causes `rust/regex` to use a slower internal engine. It's so slow in fact
that `python/re` and `python/regex` are both faster than `rust/regex` for the
Unicode benchmarks. For the ASCII `long-english` benchmark, `rust/regex` and
`re2` both do well because most of the time is spent in its lazy DFA, which has
pretty good throughput performance when compared to a pure backtracker.

Note that several regex engines can't be used in the Unicode variants because
either they don't support a Unicode aware `\w` or because they don't support a
Unicode aware `\b` (or both).

| Engine | all-english | all-russian | long-english | long-russian |
| - | - | - | - | - |
| dotnet/compiled | 58.8 MB/s | 98.5 MB/s | 178.5 MB/s | 113.7 MB/s |
| dotnet/nobacktrack | 29.6 MB/s | 40.1 MB/s | 146.1 MB/s | 124.4 MB/s |
| go/regexp | 12.0 MB/s | - | 45.3 MB/s | - |
| hyperscan | 153.9 MB/s | - | 445.1 MB/s | - |
| icu | 78.2 MB/s | 105.5 MB/s | 41.6 MB/s | 55.5 MB/s |
| java/hotspot | 73.6 MB/s | 141.3 MB/s | 71.4 MB/s | 112.6 MB/s |
| javascript/v8 | 160.6 MB/s | - | 196.8 MB/s | - |
| pcre2 | 98.5 MB/s | 127.6 KB/s | 70.7 MB/s | 6.2 MB/s |
| pcre2/jit | **189.4 MB/s** | **228.0 MB/s** | 244.5 MB/s | **196.0 MB/s** |
| perl | 13.7 MB/s | 35.1 KB/s | 104.4 MB/s | 1869.6 KB/s |
| python/re | 33.7 MB/s | 43.9 MB/s | 109.6 MB/s | 114.8 MB/s |
| python/regex | 24.0 MB/s | 44.9 MB/s | 36.6 MB/s | 99.3 MB/s |
| re2 | 63.4 MB/s | - | **921.0 MB/s** | - |
| regress | 169.5 MB/s | - | 146.2 MB/s | - |
| rust/regex/meta | 107.6 MB/s | 19.5 MB/s | 800.0 MB/s | 35.2 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**all-english**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/08-words/all-english` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b[0-9A-Za-z_]+\b````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-sampled.txt`](benchmarks/haystacks/opensubtitles/en-sampled.txt) |
| count(`dotnet/compiled`) | 56601 |
| count(`dotnet/nobacktrack`) | 56601 |
| count(`icu`) | 56601 |
| count(`.*`) | 56691 |

We specifically write out `[0-9A-Za-z_]` instead of using `\w` because some
regex engines, such as the one found in .NET, make `\w` Unicode aware and there
doesn't appear to be any easy way of disabling it.

Also, the .NET engine makes `\b` Unicode-aware, which also appears impossible
to disable. To account for that, we permit a different count.

**all-russian**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/08-words/all-russian` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b\w+\b````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-sampled.txt`](benchmarks/haystacks/opensubtitles/ru-sampled.txt) |
| count(`dotnet.*`) | 53960 |
| count(`icu`) | 53960 |
| count(`java.*`) | 53960 |
| count(`perl`) | 53960 |
| count(`.*`) | 107391 |

`regress`, `re2` and `go/regexp` are excluded because `\w` is not Unicode
aware. `hyperscan` is exclude because it doesn't support a Unicode aware `\b`.

For `dotnet/compiled`, since the length of matching spans is in the number of
UTF-16 code units, its expected count is smaller.

For `perl`, it has the same count as `dotnet/compiled`, but only because it
counts total encoded codepoints. Since every match span in this benchmark
seemingly corresponds to codepoints in the basic multi-lingual plane, it
follows that the number of UTF-16 code units is equivalent to the number of
codepoints.

**long-english**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/08-words/long-english` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b[0-9A-Za-z_]{12,}\b````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-sampled.txt`](benchmarks/haystacks/opensubtitles/en-sampled.txt) |
| count(`.*`) | 839 |

We specifically write out `[0-9A-Za-z_]` instead of using `\w` because some
regex engines, such as the one found in .NET, make `\w` Unicode aware and there
doesn't appear to be any easy way of disabling it.

Also, the fact that `\b` is Unicode-aware in .NET does not seem to impact the
match counts in this benchmark.

**long-russian**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/08-words/long-russian` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b\w{12,}\b````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-sampled.txt`](benchmarks/haystacks/opensubtitles/ru-sampled.txt) |
| count(`dotnet.*`) | 2747 |
| count(`icu`) | 2747 |
| count(`java.*`) | 2747 |
| count(`perl`) | 2747 |
| count(`.*`) | 5481 |

`regress`, `re2` and `go/regexp` are excluded because `\w` is not Unicode
aware. `hyperscan` is exclude because it doesn't support a Unicode aware `\b`.

For `dotnet/compiled`, since the length of matching spans is in the number of
UTF-16 code units, its expected count is smaller.

For `perl`, it has the same count as `dotnet/compiled`, but only because it
counts total encoded codepoints. Since every match span in this benchmark
seemingly corresponds to codepoints in the basic multi-lingual plane, it
follows that the number of UTF-16 code units is equivalent to the number of
codepoints.

</details>

#### aws-keys

This [measures a regex][pypi-aws-secrets-regex] for [detecting AWS keys in
source code][pypi-aws-secrets-regex][aws-key-blog]. In particular, to reduce
false positives, it looks for both an access key and a secret key within a few
lines of one another.

We also measure a "quick" version of the regex that is used to find possible
candidates by searching for things that look like an AWS access key.

The measurements here demonstrate why the [pypi-aws-secrets] project splits
this task into two pieces. First it uses the "quick" version to identify
candidates, and then it uses the "full" version to lower the false positive
rate of the "quick" version. The "quick" version of the regex runs around
an order of magnitude faster than the "full" version across the board. To
understand why, let's look at the "quick" regex:

```
((?:ASIA|AKIA|AROA|AIDA)([A-Z0-7]{16}))
```

Given this regex, every match starts with one of `ASIA`, `AKIA`, `AROA` or
`AIDA`. This makes it quite amenable to prefilter optimizations where a regex
engine can look for matches of one of those 4 literals, and only then use the
regex engine to confirm whether there is a match at that position. Some regex
engines will also notice that every match starts with an `A` and use `memchr`
to look for occurrences of `A` as a fast prefilter.

We also include compilation times to give an idea of how long it takes
to compile a moderately complex regex, and how that might vary with the
compilation time of a much simpler version of the regex.

Note that in all of the measurements for this group, we search the CPython
source code (concatenated into one file). We also lossily convert it to UTF-8
so that regex engines like `regress` can participate in this benchmark. (The
CPython source code contains a very small amount of invalid UTF-8.)

[pypi-aws-secrets]: https://github.com/pypi-data/pypi-aws-secrets
[pypi-aws-secrets-regex]: https://github.com/pypi-data/pypi-aws-secrets/blob/903a7bd35bc8d9963dbbb7ca35e8ecb02e31bed4/src/scanners/mod.rs#L15-L23
[aws-key-blog]: https://tomforb.es/i-scanned-every-package-on-pypi-and-found-57-live-aws-keys/

| Engine | full | quick | compile-full | compile-quick |
| - | - | - | - | - |
| dotnet/compiled | 511.2 MB/s | 794.1 MB/s | 104.24us | 42.08us |
| dotnet/nobacktrack | - | 676.9 MB/s | - | 214.48us |
| go/regexp | 105.0 MB/s | 874.5 MB/s | 64.59us | 10.16us |
| hyperscan | - | 1373.9 MB/s | - | 6.96ms |
| icu | 186.1 MB/s | 339.5 MB/s | 11.62us | 2.98us |
| java/hotspot | 40.9 MB/s | 117.3 MB/s | - | - |
| javascript/v8 | 271.9 MB/s | 311.2 MB/s | - | - |
| pcre2 | 951.2 MB/s | 1471.7 MB/s | **3.61us** | **878.00ns** |
| pcre2/jit | 1216.0 MB/s | 1012.4 MB/s | 20.45us | 4.79us |
| perl | 103.8 MB/s | 139.2 MB/s | - | - |
| python/re | 97.1 MB/s | 164.3 MB/s | 206.73us | 47.95us |
| python/regex | 99.8 MB/s | 113.9 MB/s | 686.21us | 136.39us |
| re2 | 537.3 MB/s | 999.0 MB/s | 70.23us | 9.19us |
| regress | 263.4 MB/s | 709.9 MB/s | 8.47us | 2.07us |
| rust/regex | 679.1 MB/s | 1476.6 MB/s | 79.63us | 19.92us |
| rust/regex/meta | **1776.0 MB/s** | **1872.5 MB/s** | 87.34us | 14.64us |

<details>
<summary>Show individual benchmark parameters.</summary>

**full**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/09-aws-keys/full` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````(('\|")((?:ASIA\|AKIA\|AROA\|AIDA)([A-Z0-7]{16}))('\|").*?(\n^.*?){0,4}(('\|")[a-zA-Z0-9+/]{40}('\|"))+\|('\|")[a-zA-Z0-9+/]{40}('\|").*?(\n^.*?){0,3}('\|")((?:ASIA\|AKIA\|AROA\|AIDA)([A-Z0-7]{16}))('\|"))+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/cpython-226484e4.py`](benchmarks/haystacks/wild/cpython-226484e4.py) |
| count(`.*`) | 0 |


**quick**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/09-aws-keys/quick` |
| model | [`grep`](MODELS.md#grep) |
| regex | `````((?:ASIA\|AKIA\|AROA\|AIDA)([A-Z0-7]{16}))````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/cpython-226484e4.py`](benchmarks/haystacks/wild/cpython-226484e4.py) |
| count(`.*`) | 0 |


**compile-full**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/09-aws-keys/compile-full` |
| model | [`compile`](MODELS.md#compile) |
| regex | `````(('\|")((?:ASIA\|AKIA\|AROA\|AIDA)([A-Z0-7]{16}))('\|").*?(\n^.*?){0,4}(('\|")[a-zA-Z0-9+/]{40}('\|"))+\|('\|")[a-zA-Z0-9+/]{40}('\|").*?(\n^.*?){0,3}('\|")((?:ASIA\|AKIA\|AROA\|AIDA)([A-Z0-7]{16}))('\|"))+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `"AIDAABCDEFGHIJKLMNOP""aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa [.. snip ..]` |
| count(`.*`) | 1 |


**compile-quick**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/09-aws-keys/compile-quick` |
| model | [`compile`](MODELS.md#compile) |
| regex | `````((?:ASIA\|AKIA\|AROA\|AIDA)([A-Z0-7]{16}))````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `AIDAABCDEFGHIJKLMNOP` |
| count(`.*`) | 1 |


</details>

#### bounded-repeat

This group of benchmarks measures how well regex engines do with bounded
repeats. Bounded repeats are sub-expressions that are permitted to match
up to some fixed number of times. For example, `a{3,5}` matches 3, 4 or 5
consecutive `a` characters. Unlike unbounded repetition operators, the regex
engine needs some way to track when the bound has reached its limit. For this
reason, many regex engines will translate `a{3,5}` to `aaaa?a?`. Given that
the bounds may be much higher than `5` and that the sub-expression may be much
more complicated than a single character, bounded repeats can quickly cause the
underlying matcher to balloon in size.

We measure three different types of bounded repeats:

* A search for a number of consecutive letters, both ASCII only and Unicode
aware.
* A search for certain types of words surrounding a `Result` type in Rust
source code.
* A search for consecutive words, all beginning with a capital letter.

We also include measurements for the compilation time of the last two.

Hyperscan does unusually well here, particularly for an automata oriented
engine. It's plausible that it has some specific optimizations in place for
bounded repeats.

`rust/regex` slows down quite a bit on the `context` regex. Namely, the
`context` regex is quite gnarly and its `(?s:.)` sub-expression coupled with
the bounded repeat causes a large portion of its transition table to get filled
out. This in turn results in more time than usual being spent actually building
the lazy DFA's transition table during a search. Typically, the lazy DFA's
transition table is built pretty quickly and then mostly reused on subsequent
searches. But in this case, the transition table exceeds the lazy DFA's cache
capacity and results in the cache getting cleared. However, the rate at which
new transitions are created is still low enough that the lazy DFA is used
instead of falling back to a slower engine.

| Engine | letters-en | letters-ru | context | capitals | compile-context | compile-capitals |
| - | - | - | - | - | - | - |
| dotnet/compiled | 264.1 MB/s | 191.4 MB/s | 333.1 MB/s | 870.5 MB/s | 33.33us | 28.82us |
| dotnet/nobacktrack | 143.1 MB/s | 110.4 MB/s | 53.9 MB/s | 556.7 MB/s | 205.09us | 48.50us |
| go/regexp | 29.4 MB/s | 27.6 MB/s | 28.8 MB/s | 54.1 MB/s | 75.06us | 54.76us |
| hyperscan | **736.2 MB/s** | 266.1 MB/s | **499.1 MB/s** | **2.7 GB/s** | 24.63ms | 665.71us |
| icu | 50.9 MB/s | 70.2 MB/s | 69.6 MB/s | 265.5 MB/s | **4.00us** | 2.80us |
| java/hotspot | 92.6 MB/s | 133.4 MB/s | 74.3 MB/s | 125.5 MB/s | - | - |
| javascript/v8 | 149.0 MB/s | 57.3 MB/s | 133.9 MB/s | 726.0 MB/s | - | - |
| pcre2 | 70.8 MB/s | 408.7 KB/s | 76.0 MB/s | 586.9 MB/s | 4.36us | 28.67us |
| pcre2/jit | 335.8 MB/s | 285.8 MB/s | 376.8 MB/s | 1547.8 MB/s | 13.11us | 36.75us |
| perl | 69.5 MB/s | 50.0 MB/s | 88.6 MB/s | 217.8 MB/s | - | - |
| python/re | 73.0 MB/s | - | 68.8 MB/s | 66.4 MB/s | 57.87us | 32.35us |
| python/regex | 34.7 MB/s | 75.6 MB/s | 33.5 MB/s | 270.3 MB/s | 149.76us | 75.33us |
| re2 | 494.2 MB/s | 5.5 MB/s | 94.8 MB/s | 982.2 MB/s | 93.16us | 123.48us |
| regress | 163.1 MB/s | 30.2 MB/s | 163.5 MB/s | 404.0 MB/s | - | **1.22us** |
| rust/regex | 610.4 MB/s | 536.9 MB/s | 20.5 MB/s | 825.6 MB/s | 40.85us | 60.28us |
| rust/regex/meta | 682.7 MB/s | **630.4 MB/s** | 101.1 MB/s | 825.6 MB/s | 62.38us | 61.54us |

<details>
<summary>Show individual benchmark parameters.</summary>

**letters-en**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/10-bounded-repeat/letters-en` |
| model | [`count`](MODELS.md#count) |
| regex | `````[A-Za-z]{8,13}````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-sampled.txt`](benchmarks/haystacks/opensubtitles/en-sampled.txt) |
| count(`hyperscan`) | 3724 |
| count(`.*`) | 1833 |


**letters-ru**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/10-bounded-repeat/letters-ru` |
| model | [`count`](MODELS.md#count) |
| regex | `````\p{L}{8,13}````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-sampled.txt`](benchmarks/haystacks/opensubtitles/ru-sampled.txt) |
| count(`hyperscan`) | 8570 |
| count(`.*`) | 3475 |


**context**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/10-bounded-repeat/context` |
| model | [`count`](MODELS.md#count) |
| regex | `````[A-Za-z]{10}\s+[\s\S]{0,100}Result[\s\S]{0,100}\s+[A-Za-z]{10}````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`hyperscan`) | 109 |
| count(`.*`) | 53 |


**capitals**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/10-bounded-repeat/capitals` |
| model | [`count`](MODELS.md#count) |
| regex | `````(?:[A-Z][a-z]+\s*){10,100}````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`hyperscan`) | 237 |
| count(`.*`) | 11 |


**compile-context**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/10-bounded-repeat/compile-context` |
| model | [`compile`](MODELS.md#compile) |
| regex | `````[A-Za-z]{10}\s+(?s:.){0,100}Result(?s:.){0,100}\s+[A-Za-z]{10}````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `abcdefghij blah blah blah Result blib blab klmnopqrst` |
| count(`.*`) | 1 |


**compile-capitals**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/10-bounded-repeat/compile-capitals` |
| model | [`compile`](MODELS.md#compile) |
| regex | `````(?:[A-Z][a-z]+\s*){10,100}````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `Crazy Janey Mission Man Wild Billy Greasy Lake Hazy Davy Kil [.. snip ..]` |
| count(`hyperscan`) | 12 |
| count(`.*`) | 1 |


</details>

#### unstructured-to-json

These benchmarks come from a [task that converts unstructured log data to
structured JSON data][OpatrilPeter description]. It works by iterating over
every line in the log file and parsing various parts of each line into
different sections using capture groups. The regex matches every line, so any
fast logic design to reject non-matches will generally penalize regex engines
here.

The original regex looks like this:

```
(?x)
^
(?P<timestamp>[^\ ]+\ [^\ ]+)

[\ ](?P<level>[DIWEF])[1234]:[\ ]

(?P<header>
    (?:
        (?:
            \[ [^\]]*? \] | \( [^\)]*? \)
        ):[\ ]
    )*
)

(?P<body>.*?)

[\ ]\{(?P<location>[^\}]*)\}
$
```

(The actual regex is flattened since not all engines support verbose mode. We
also remove the names from each capture group.)

`pcre2/jit` does _really_ well here. I'm not personally familiar with how
PCRE2's JIT works, but if I had to guess, I'd say there are some clever
optimizations with respect to the `[^ ]+` (and similar) sub-expressions in this
regex.

Otherwise, the backtracking engines generally outperform the automata engines
in this benchmark. Interestingly, all of `re2`, `go/regexp` and `rust/regex`
principally use their own bounded backtracking algorithms. But it looks like
"proper" backtrackers tend to be better optimized than the ones found in RE2
and its descendants. (Bounded backtracking does have to pay for checking that
no combination of haystack position and NFA state is visited more than once,
but even removing that check does not bring, e.g., `rust/regex` up to speeds
similar to other backtrackers.)

[OpatrilPeter description]: https://github.com/rust-lang/regex/discussions/960#discussioncomment-5106322

| Engine | extract | compile |
| - | - | - |
| dotnet/compiled | 540.4 MB/s | 44.81us |
| dotnet/nobacktrack | 17.2 MB/s | 505.21us |
| go/regexp | 82.5 MB/s | 21.14us |
| icu | 94.6 MB/s | 7.76us |
| java/hotspot | 218.0 MB/s | - |
| javascript/v8 | 979.1 MB/s | - |
| pcre2 | 208.1 MB/s | **1.37us** |
| pcre2/jit | **1571.0 MB/s** | 7.08us |
| perl | 146.7 MB/s | - |
| python/re | 119.9 MB/s | 88.31us |
| python/regex | 125.8 MB/s | 279.87us |
| re2 | 120.4 MB/s | 9.50us |
| regress | 281.8 MB/s | 4.03us |
| rust/regex | 93.0 MB/s | 17.53us |
| rust/regex/meta | 108.2 MB/s | 20.18us |

<details>
<summary>Show individual benchmark parameters.</summary>

**extract**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/11-unstructured-to-json/extract` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex-path | [`wild/unstructured-to-json.txt`](benchmarks/regexes/wild/unstructured-to-json.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/unstructured-to-json.log`](benchmarks/haystacks/wild/unstructured-to-json.log) |
| count(`.*`) | 600 |


**compile**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/11-unstructured-to-json/compile` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`wild/unstructured-to-json.txt`](benchmarks/regexes/wild/unstructured-to-json.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `2022/06/17 06:25:22 I4: [17936:140245395805952:(17998)]: (8f [.. snip ..]` |
| count(`.*`) | 1 |


</details>

#### dictionary

This benchmark highlights how well each regex engine does searching for a small
dictionary of words. The dictionary is made up of about 2,500 words, where
every word is at least 15 bytes in length. The number of words was chosen to
be small enough that _most_ regex engines can execute a search in reasonable
time. The bigger minimum length of each word was chosen in order to make this
a throughput benchmark. That is, there is only one match found here, so this
benchmark is measuring the raw speed with which an engine can handle a big
alternation of plain literals.

Most regex engines run quite slowly here. `perl`, `re2` and `rust/regex` lead
the pack with throughput measured in MB/s, while the rest are measured in
KB/s. One might think that this is a benchmark that would manifest as a bright
dividing line between finite automata engines and backtracking engines. Namely,
finite automata engines should handle all of the alternations in "parallel,"
where as backtrackers will essentially try to match each alternate at each
position in the haystack (owch). Indeed, this seems _mostly_ true, but `perl`
(a backtracker) does quite well while `go/regexp` (a finite automata engine)
does quite poorly. Moreover, what explains the differences between `perl`,
`re2` and `rust/regex`?

There are several knots to untangle here.

First, we'll tackle the reason why `go/regexp` has a poor showing here. The
answer lies in how the Thompson NFA construction works. A Thompson NFA can be
built in worst case linear time (in the size of the pattern), but in exchange,
it has _epsilson transitions_ in its state graph. Epsilon transitions are
transitions in a finite state machine that are followed without consuming
any input. In a case like `foo|bar|quux`, you can think of the corresponding
Thompson NFA (very loosely) as creating a single starting state with three
epsilon transitions to each of `foo`, `bar` and `quux`. In a Thompson NFA
simulation (i.e., a regex search using a Thompson NFA), all of these epsilon
transitions have to be continually followed at every position in the haystack.
With a large number of alternations, the amount of time spent shuffling through
these epsilon transitions can be quite enormous. While the search time remains
linear with respect to the size of the haystack, the "constant" factor here
(i.e., the size of the regex pattern) can become quite large. In other words,
a Thompson NFA scales poorly with respect to the size of the pattern. In this
particular case, a Thompson NFA just doesn't do any better than a backtracker.

The second knot to untangle here is why `perl` does so well despite being a
backtracker. While I'm not an expert on Perl internals, it appears to do well
here because of something called a _trie optimization_. That is, Perl's regex
engine will transform large alternations like this into an equivalent but
much more efficient structure by essentially building a trie and encoding it
into the regex itself. It turns out that `rust/regex` does the same thing,
because the exact same optimization helps a backtracker in the same way it
helps a Thompson NFA simulation. The optimization exploits the fact that the
branches in the alternation are not truly independent and actually share a lot
of overlap. Without the optimization, the branches are treated as completely
independent and one must brute force their way through each one.

So what does this trie optimization look like? Consider a regex like
`zapper|z|zap`. There is quite a bit of redundant structure. With some
care, and making sure to preserve leftmost-first match semantics, it can be
translated to the equivalent pattern `z(apper||ap)`. Notice how in the pattern
we started with, the alternation needs to be dealt with for every byte in the
haystack, because you never know which branch is going to match, if any. But in
the latter case, you now don't even need to consider the alternation until the
byte `z` matches, which is likely to be quite rare.

Indeed, the algorithm for constructing such a pattern effectively proceeds by
building a trie from the original alternation, and then converting the trie
back to whatever intermediate representation the regex engine uses.

The last knot to untangle is to explain the differences between `perl`, `re2`
and `rust/regex`. Perl still uses a backtracking strategy, but with the trie
optimization described above, it can try much fewer things for each position
in the haystack. But what's going on with `re2` and `rust/regex`? In this
case, `re2` uses the Thompson NFA simulation, but `re2` does not use the trie
optimization described above, so it gets stuck in a lot epsilon transition
shuffling. Finally, `rust/regex` does the trie optimization _and_ uses its lazy
DFA internally for this case. `re2` probably could too, but both libraries use
various heuristics for deciding which engine to use. In this case, the regex
might be too big for `re2` to use its lazy DFA.

OK, that wraps up discussion of the `single` benchmark. But what is the `multi`
benchmark? Where `single` represents combining all words in the dictionary into
a single pattern, `multi` represents a strategy where each word is treated as
its own distinct pattern. In the `single` case, Hyperscan actually rejects
the pattern for being too large, but is happy to deal with it if each word
is treated as its own pattern. The main semantic difference between these
strategies is that the `multi` approach permits not only identifying where a
match occurred, but *which* word in the dictionary matched. And this is done
without using capture groups.

Hyperscan does really well here. While its source code is difficult penetrate,
my understanding is that Hyperscan uses its "FDR" algorithm here, which is
essentially SIMD-ified variant of multi-substring Shift-Or. This benchmark
represents Hyperscan's bread and butter: multi-pattern search.

`rust/regex` actually does _worse_ in the `multi` case versus the `single`
case. `rust/regex`'s support for multi-pattern search is still young, and in
particular, the multi-pattern case currently inhibits the trie optimization
discussed above.

Finally, we also include compile-time benchmarks for each of the above cases in
order to give an idea of how long it takes to build a regex from a dictionary
like this. I don't have much to say here other than to call out the fact
that the trie optimization does have a meaningful impact on regex compile
times in the `rust/regex` case at least.

| Engine | single | multi | compile-single | compile-multi |
| - | - | - | - | - |
| dotnet/compiled | 1283.1 KB/s | - | 11.28ms | - |
| go/regexp | 571.3 KB/s | - | 14.26ms | - |
| hyperscan | - | **7.8 GB/s** | - | 20.20ms |
| icu | 146.2 KB/s | - | **1.46ms** | - |
| java/hotspot | 109.4 KB/s | - | - | - |
| javascript/v8 | 28.4 KB/s | - | - | - |
| perl | 131.7 MB/s | - | - | - |
| python/re | 153.5 KB/s | - | 34.21ms | - |
| python/regex | 141.0 KB/s | - | 115.23ms | - |
| re2 | 5.6 MB/s | - | 4.19ms | - |
| regress | 75.7 KB/s | - | 3.00ms | - |
| rust/regex | 176.2 MB/s | - | 8.79ms | - |
| rust/regex/meta | **713.2 MB/s** | 193.1 MB/s | 7.73ms | **16.64ms** |

<details>
<summary>Show individual benchmark parameters.</summary>

**single**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/12-dictionary/single` |
| model | [`count`](MODELS.md#count) |
| regex-path | [`dictionary/english/length-15.txt`](benchmarks/regexes/dictionary/english/length-15.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-medium.txt`](benchmarks/haystacks/opensubtitles/en-medium.txt) |
| count(`.*`) | 1 |

`dotnet/nobacktrack` is omitted because the regex is too large.

`hyperscan` is omitted because the regex is too large.

`pcre2/*` are omitted because the regex is too large.

**multi**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/12-dictionary/multi` |
| model | [`count`](MODELS.md#count) |
| regex-path | [`dictionary/english/length-15.txt`](benchmarks/regexes/dictionary/english/length-15.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-medium.txt`](benchmarks/haystacks/opensubtitles/en-medium.txt) |
| count(`.*`) | 1 |

Only `hyperscan` and `rust/regex/meta` are included because they are the only
regex engines to support multi-pattern regexes.

**compile-single**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/12-dictionary/compile-single` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`dictionary/english/length-15.txt`](benchmarks/regexes/dictionary/english/length-15.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `Zubeneschamali's` |
| count(`.*`) | 1 |

`dotnet/nobacktrack` is omitted because the regex is too large.

`hyperscan` is omitted because the regex is too large.

`java/hotspot` is omitted because we currently don't benchmark Perl regex
compilation.

`javascript/v8` is omitted because we currently don't benchmark Perl regex
compilation.

`pcre2/*` are omitted because the regex is too large.

`perl` is omitted because we currently don't benchmark Perl regex compilation.

**compile-multi**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/12-dictionary/compile-multi` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`dictionary/english/length-15.txt`](benchmarks/regexes/dictionary/english/length-15.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `Zubeneschamali's` |
| count(`.*`) | 1 |

Only `hyperscan` and `rust/regex/meta` are included because they are the only
regex engines to support multi-pattern regexes.

</details>

#### noseyparker

This benchmark measures how well regex engines do when asked to look for
matches for many different patterns. The patterns come from the [Nosey Parker]
project, which finds secrets and sensitive
information in textual data and source repositories. Nosey Parker operates
principally by defining a number of rules for detecting secrets (for example,
AWS API keys), and then looking for matches of those rules in various corpora.
The rules are, as you might have guessed, defined as regular expressions.

I went through each of its rules and extracted a total of 96 regular
expressions, as of [commit `be8c26e8`][be8c26e8]. These 96 regexes make up the
`single` and `multi` benchmarks below, with `single` corresponding to joining
all of patterns into one big alternation and `multi` corresponding to treating
each pattern as its own regex. In the latter case, only the `rust/regex/meta`
and `hyperscan` engines are measured, since they are the only ones to support
multi-regex matching.

This is a particularly brutal benchmark. Most regex engines can't deal with it
at all, and will either reject it at compilation time for being too big or
simply take longer than we're willing to wait. (rebar imposes a reasonable
timeout for all benchmarks, and if the timeout is exceeded, no measurements are
collected.)

Hyperscan is in its own class here. Hyperscan was purpose built to deal with
the multi-pattern use case, and it deals with it *very* well here. The specific
patterns also put this in its wheelhouse because they all have some kind of
literal string in them. Hyperscan uses a [literal searching and finite automata
decomposition strategy][hyperpub] to quickly identify candidate matches and
avoids doing redundant work. Although how it all fits together and avoids
pitfalls such as worst case quadratic search time doesn't appear to be written
down anywhere.

`rust/regex/meta` just barely does serviceably here. It uses its lazy DFA to
handle this regex, but with the default cache sizes, profiling suggests that it
is spending a lot of its time building the DFA. It's plausible that increasing
the cache size for such a big regex would let it execute searches faster.

`pcre2/jit` doesn't do as well here, but one might expect that because it is
a backtracking engine. With that said, no other backtracking engine could deal
with this regex at all, so `pcre2/jit` is doing quite well relative to other
backtracking engines.

Finally, we also include compile time benchmarks for each of the `single` and
`multi` cases to give a general sense of how long this monster regex takes to
build.

[Nosey Parker]: https://github.com/praetorian-inc/noseyparker
[be8c26e8]: https://github.com/praetorian-inc/noseyparker/tree/be8c26e8b2e8550f101ae62c3f374d0226808214
[hyperpub]: https://www.usenix.org/system/files/nsdi19-wang-xiang.pdf

| Engine | single | multi | compile-single | compile-multi |
| - | - | - | - | - |
| hyperscan | **4.3 GB/s** | **4.3 GB/s** | 215.41ms | 134.69ms |
| pcre2/jit | 12.9 MB/s | - | **605.16us** | - |
| rust/regex/meta | 131.5 MB/s | 102.5 MB/s | 2.48ms | **2.81ms** |

<details>
<summary>Show individual benchmark parameters.</summary>

**single**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/13-noseyparker/single` |
| model | [`count`](MODELS.md#count) |
| regex-path | [`wild/noseyparker.txt`](benchmarks/regexes/wild/noseyparker.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/cpython-226484e4.py`](benchmarks/haystacks/wild/cpython-226484e4.py) |
| count(`hyperscan`) | 241 |
| count(`.*`) | 55 |

* `dotnet/compiled` is omitted because it times out.
* `dotnet/nobacktrack` is omitted because the regex is too big.
* `go/regexp` is omitted because there are bounded repeats that exceed its
limit.
* `icu` is omitted because it times out.
* `java/hotspot` is omitted because it times out.
* `javascript/v8` is omitted because it doesn't support inline flags.
* `pcre2` is omitted because it times out.
* `perl` is omitted because it times out.
* `python/*` is omitted because it times out.
* `re2` is omitted because it seems to fail and reports a count of `0`.
* `regress` is omitted because it doesn't support inline flags.

**multi**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/13-noseyparker/multi` |
| model | [`count`](MODELS.md#count) |
| regex-path | [`wild/noseyparker.txt`](benchmarks/regexes/wild/noseyparker.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/cpython-226484e4.py`](benchmarks/haystacks/wild/cpython-226484e4.py) |
| count(`hyperscan`) | 241 |
| count(`.*`) | 55 |

Only `hyperscan` and `rust/regex/meta` are included because they are the only
regex engines to support multi-pattern regexes.

**compile-single**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/13-noseyparker/compile-single` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`wild/noseyparker.txt`](benchmarks/regexes/wild/noseyparker.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `TWITTER_API_KEY = 'UZYoBAfBzNace3mBwPOGYw'` |
| count(`.*`) | 1 |

We only include the engines that are measured in the `single` benchmark.

**compile-multi**

| Parameter | Value |
| --------- | ----- |
| full name | `curated/13-noseyparker/compile-multi` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`wild/noseyparker.txt`](benchmarks/regexes/wild/noseyparker.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `TWITTER_API_KEY = 'UZYoBAfBzNace3mBwPOGYw'` |
| count(`.*`) | 1 |

We only include the engines that are measured in the `multi` benchmark.

</details>

### dictionary

#### compile

These benchmarks test how long it takes to build a regex for a large
dictionary of words as a single alternation. Overall, the compile times here
are pretty brutal and are in the milliseconds on my machine.

If you really do know that you just have an alternation of literals you
really probably should use the `aho-corasick` Rust crate directly. But such
things might be unknown unknowns or you might not even know you're running
a huge alternation of literals, in which case, the regex crate will do its
best.

| Engine | english | english-10 | english-15 |
| - | - | - | - |
| go/regexp | - | **85.26ms** | 14.85ms |
| re2 | - | 144.41ms | 3.96ms |
| regress | - | - | **2.96ms** |
| rust/regex/meta | **201.91ms** | 94.85ms | 8.26ms |

<details>
<summary>Show individual benchmark parameters.</summary>

**english**

| Parameter | Value |
| --------- | ----- |
| full name | `dictionary/compile/english` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`dictionary/english/sorted-by-length.txt`](benchmarks/regexes/dictionary/english/sorted-by-length.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack | `zygotes` |
| count(`.*`) | 1 |


**english-10**

| Parameter | Value |
| --------- | ----- |
| full name | `dictionary/compile/english-10` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`dictionary/english/length-10.txt`](benchmarks/regexes/dictionary/english/length-10.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack | `Zubeneschamali's` |
| count(`.*`) | 1 |


**english-15**

| Parameter | Value |
| --------- | ----- |
| full name | `dictionary/compile/english-15` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`dictionary/english/length-15.txt`](benchmarks/regexes/dictionary/english/length-15.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack | `Zubeneschamali's` |
| count(`.*`) | 1 |


</details>

#### search

These benchmarks attempt to search for occurrences of words in varying
dictionary sizes. Engines like PCRE2 can't handle any of these. Some
backtrackers, like python/re and regress, can at least execute some of the
smaller dictionaries but they are quite slow. The regex crate generally does
pretty well as it has a specific optimization for large literal alternations:
it just diverts to Aho-Corasick.

| Engine | english | english-tiny | english-10 | english-15 |
| - | - | - | - | - |
| go/regexp | - | - | - | 569.2 KB/s |
| python/re | - | - | - | 174.6 KB/s |
| re2 | - | - | 137.0 KB/s | 5.6 MB/s |
| regress | - | - | - | 84.5 KB/s |
| rust/regex/meta | **106.4 MB/s** | **203.1 MB/s** | **159.6 MB/s** | **712.3 MB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**english**

| Parameter | Value |
| --------- | ----- |
| full name | `dictionary/search/english` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex-path | [`dictionary/english/sorted-by-length.txt`](benchmarks/regexes/dictionary/english/sorted-by-length.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/en-medium.txt`](benchmarks/haystacks/opensubtitles/en-medium.txt) |
| count(`.*`) | 45315 |


**english-tiny**

| Parameter | Value |
| --------- | ----- |
| full name | `dictionary/search/english-tiny` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex-path | [`dictionary/english/sorted-by-length.txt`](benchmarks/regexes/dictionary/english/sorted-by-length.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/en-tiny.txt`](benchmarks/haystacks/opensubtitles/en-tiny.txt) |
| count(`.*`) | 87 |


**english-10**

| Parameter | Value |
| --------- | ----- |
| full name | `dictionary/search/english-10` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex-path | [`dictionary/english/length-10.txt`](benchmarks/regexes/dictionary/english/length-10.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/en-medium.txt`](benchmarks/haystacks/opensubtitles/en-medium.txt) |
| count(`.*`) | 690 |


**english-15**

| Parameter | Value |
| --------- | ----- |
| full name | `dictionary/search/english-15` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex-path | [`dictionary/english/length-15.txt`](benchmarks/regexes/dictionary/english/length-15.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/en-medium.txt`](benchmarks/haystacks/opensubtitles/en-medium.txt) |
| count(`.*`) | 15 |


</details>

### folly

These benchmarks demonstrate the folly of literal optimizations. Basically,
literal optimizations are a classic case of selling out some cases in
exchange for making some other (typically more common) cases exceptionally
fast. But when you hit the cases where the literal optimizations break down,
perf can be slower than if you didn't attempt the literal optimizations at
all. Typically, the way to trash perf is to increase the false positive rate
of detecting candidates via literal matches. This in turn causes a ping-pong
effect: a candidate is found via a fast substring (or multi-substring)
search, but it fails to lead to an overall match, so then you go back to the
substring search and repeat this ping-ponging between the regex engine and
the literal search. The overhead of this can add up to quite a bit with a
very high false positive rate.

The main purpose of these benchmarks is to demonstrate these cases, shine
some light on them and hopefully work toward making them "not too bad."

| Engine | awyer-inn-busted | literal-never-match-rare | literal-never-match-frequent | literal-never-match-tricksy |
| - | - | - | - | - |
| go/regexp | 61.5 MB/s | 59.7 GB/s | 3.1 GB/s | 4.5 GB/s |
| hyperscan | **21.9 GB/s** | 46.5 GB/s | **59.0 GB/s** | 10.1 GB/s |
| pcre2/jit | 897.1 MB/s | 38.7 GB/s | 38.7 GB/s | 2.4 GB/s |
| re2 | 985.2 MB/s | **79.3 GB/s** | 3.8 GB/s | 1496.5 MB/s |
| rust/regex/meta | 163.7 MB/s | 49.1 GB/s | 41.8 GB/s | **11.9 GB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**awyer-inn-busted**

| Parameter | Value |
| --------- | ----- |
| full name | `folly/awyer-inn-busted` |
| model | [`count`](MODELS.md#count) |
| regex | `````([A-Z]awyer\|[A-Z]inn)[0-9\s]````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `awyerinnawyerinnawyerinnawyerinnawyerinnawyerinnawyerinnawye [.. snip ..]` |
| count(`.*`) | 1 |

This one causes the Rust regex engine to look for matches of `awyer|inn`,
then do a reverse search for '[A-Z]' which fails. Then it restarts the search
for 'awyer|inn' again only to find another candidate match immediately that
doesn't lead to an overall match. And so on.

Note that [A-Z] was chosen specifically here. If you use [A-Za-z], then the
backwards search will overlap with a previous candidate match, which in turn
causes it to detect potential quadratic behavior almost immediately. At that
point, it gives up the "reverse inner" strategy and falls back to a normal
DFA forward scan, which has respectable speed.

**literal-never-match-rare**

| Parameter | Value |
| --------- | ----- |
| full name | `folly/literal-never-match-rare` |
| model | [`count`](MODELS.md#count) |
| regex | `````ZQZQZQZQZQ````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`.*`) | 0 |

This benchmark is a good example of how even the most basic literal
optimizations can vary across regex engines. Here, the 're2' and 'go/regexp'
engines do quite well in part because their literal optimizations aren't
terribly sophisticated. I *believe* in this case they work by feeding the
first byte of the pattern to memchr to find candidate matches and then using
the regex engine to confirm or reject. Since 'Z' occurs quite rarely in the
haystack, most of the search stays in the very hot memchr loop.

Contrast this to the regex crate which uses a SIMD substring search instead
of memchr. It still performs admirably, but at around half the speed.

**literal-never-match-frequent**

| Parameter | Value |
| --------- | ----- |
| full name | `folly/literal-never-match-frequent` |
| model | [`count`](MODELS.md#count) |
| regex | `````aeaeaeaeae````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`.*`) | 0 |

You might read the description for 'literal-never-match-rare' above and
think, "okay so some regex engines aren't sophisticated but at least they
get predictable and consistent performance." Not so! In this benchmark, we
also search for a literal that never matches, but the bytes that make up
the literal are overall much more frequently occurring than in the above
benchmark.

In this case, `re2` and `go/regexp` still do just fine (hovering at just above
3 GB/s, versus 80 GB/s for 'literal-never-match-rare'), but the regex crate
maintains identical performance as above at around 49 GB/s. This is because its
SIMD substring search, while making use of frequency heuristics, is more robust
in the face of frequently occurring bytes.

**literal-never-match-tricksy**

| Parameter | Value |
| --------- | ----- |
| full name | `folly/literal-never-match-tricksy` |
| model | [`count`](MODELS.md#count) |
| regex | `````fooYbarZquux````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `fooYbarZquuffooYbarZquuffooYbarZquuffooYbarZquuffooYbarZquuf [.. snip ..]` |
| count(`.*`) | 1 |

This benchmark is an attempt to make the regex crate's uses of memmem slow
down. We have to carefully construct the needle to contain some rare bytes that
we know the SIMD algorithm will hone in on. In this case, that's Y and Z. The
inner SIMD loop will look for places where not only those two bytes match, but
match at a distance that is equal to their distance in the needle. Thus, we
may sure our haystack has a lot of those two bytes occurring at that distance.
*But*, we also make sure to change one *other* byte from the needle to ensure
that the overall match fails.

In effect, this causes the SIMD algorithm to constantly ping-pong in and
out of its inner loop where it finds a candidate and causes it to do its
confirmation step. (Which, for a needle this small, is just memcmp.) The
confirmation then fails and the whole process repeats itself.

If you're having trouble groking this, you might consider perusing the
memchr crate's memmem submodule, which has lots of comments for how it
works. The algorithm is also [described at a high level by Wojciech
Muła][0x80-simd-strfind], although that description doesn't account for the
frequency based heuristics that `memchr::memmem` uses, and are in particular
relevant to this benchmark.

This does cause the memmem search to slow down by about a factor of 4. Other
regex engines also seem to slow down. Do note that this depends on where we
put the mismatch! For example, if we repeated 'aooYbarZquux' in our haystack,
then RE2's search will be extremely fast because it will give 'f' (the first
byte in the needle) to memchr, and since 'f' doesn't occur at all until the
match at the end of the haystack, its false positive rate is zero.

[0x80-simd-strfind]: http://0x80.pl/articles/simd-strfind.html

</details>

### grep

These are grep-oriented benchmarks that test how fast a regex engine can
process each line in a haystack.

| Engine | every-line | long-words-ascii | long-words-unicode |
| - | - | - | - |
| go/regexp | 784.2 MB/s | 72.7 MB/s | - |
| hyperscan | - | 723.0 MB/s | - |
| pcre2/jit | 1626.4 MB/s | 483.7 MB/s | 262.4 MB/s |
| python/re | 116.5 MB/s | - | - |
| python/regex | 99.1 MB/s | 64.0 MB/s | 70.1 MB/s |
| re2 | 958.2 MB/s | 480.7 MB/s | - |
| regress | 825.6 MB/s | 143.5 MB/s | - |
| rust/regex/meta | **1878.0 MB/s** | **802.1 MB/s** | **773.9 MB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**every-line**

| Parameter | Value |
| --------- | ----- |
| full name | `grep/every-line` |
| model | [`grep`](MODELS.md#grep) |
| regex | `````````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`.*`) | 239963 |

This is a baseline benchmark that matches every line. It also serves to
confirm that line iteration is done correctly.

**long-words-ascii**

| Parameter | Value |
| --------- | ----- |
| full name | `grep/long-words-ascii` |
| model | [`grep`](MODELS.md#grep) |
| regex | `````\b\w{25,}\b````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`.*`) | 5073 |

Looks for long words, but only using an ASCII definition of a word character.

**long-words-unicode**

| Parameter | Value |
| --------- | ----- |
| full name | `grep/long-words-unicode` |
| model | [`grep`](MODELS.md#grep) |
| regex | `````\b\w{25,}\b````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`.*`) | 5075 |

Looks for long words, but with "word character" being defined by Unicode.

Even though the regex crate's lazy DFA can't handle Unicode word boundaries,
it does work quite well here anyway. Namely, if a haystack is purely ASCII,
then a Unicode word boundary is equivalent to an ASCII word boundary. So the
lazy DFA will heuristically support Unicode word boundaries for regexes like
this, but as soon as it sees a non-ASCII byte, it bails and the meta engine
defers to another (usually slower) engine. In this benchmark, the haystack
is mostly ASCII, so most searches just use the lazy DFA.

</details>

### hyperscan

These benchamrks are somewhat explicitly crafted for Hyperscan because it
diverges from most other regex engines in that it reports every match location.
We also in particular focus on the impact of asking Hyperscan for the start of
each match (SOM). Namely, the `count` model only requires the engine to count
all matches which doesn't require finding the SOM of any match at all, but the
`count-spans` model does require finding the SOM of a match.

| Engine | literal-english-nosom | literal-english-som | literal-casei-english-nosom | literal-casei-english-som | literal-russian-nosom | literal-russian-som | literal-casei-russian-nosom | literal-casei-russian-som | literal-suffix-nosom | literal-suffix-som | literal-inner-nosom | literal-inner-som | fixed-length-words-nosom | fixed-length-words-som | fixed-length-words-unicode-nosom |
| - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - |
| hyperscan | **54.2 GB/s** | **54.8 GB/s** | **52.6 GB/s** | **51.5 GB/s** | 5.5 GB/s | 5.5 GB/s | 8.9 GB/s | 10.7 GB/s | **28.0 GB/s** | 4.2 GB/s | 14.2 GB/s | 3.1 GB/s | **1365.9 MB/s** | **1366.3 MB/s** | 383.4 MB/s |
| rust/regex/meta | 41.5 GB/s | 41.1 GB/s | 16.9 GB/s | 15.4 GB/s | **40.2 GB/s** | **40.3 GB/s** | **13.4 GB/s** | **11.3 GB/s** | 18.0 GB/s | **18.0 GB/s** | **20.4 GB/s** | **19.4 GB/s** | 823.4 MB/s | 803.0 MB/s | **823.4 MB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**literal-english-nosom**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/literal-english-nosom` |
| model | [`count`](MODELS.md#count) |
| regex | `````Sherlock Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 1 |


**literal-english-som**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/literal-english-som` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sherlock Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 15 |


**literal-casei-english-nosom**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/literal-casei-english-nosom` |
| model | [`count`](MODELS.md#count) |
| regex | `````Sherlock Holmes````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 1 |


**literal-casei-english-som**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/literal-casei-english-som` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sherlock Holmes````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 15 |


**literal-russian-nosom**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/literal-russian-nosom` |
| model | [`count`](MODELS.md#count) |
| regex | `````Шерлок Холмс````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-huge.txt`](benchmarks/haystacks/opensubtitles/ru-huge.txt) |
| count(`.*`) | 1 |


**literal-russian-som**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/literal-russian-som` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Шерлок Холмс````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-huge.txt`](benchmarks/haystacks/opensubtitles/ru-huge.txt) |
| count(`.*`) | 23 |


**literal-casei-russian-nosom**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/literal-casei-russian-nosom` |
| model | [`count`](MODELS.md#count) |
| regex | `````Шерлок Холмс````` |
| case-insensitive | `true` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-huge.txt`](benchmarks/haystacks/opensubtitles/ru-huge.txt) |
| count(`.*`) | 1 |


**literal-casei-russian-som**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/literal-casei-russian-som` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Шерлок Холмс````` |
| case-insensitive | `true` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-huge.txt`](benchmarks/haystacks/opensubtitles/ru-huge.txt) |
| count(`.*`) | 23 |


**literal-suffix-nosom**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/literal-suffix-nosom` |
| model | [`count`](MODELS.md#count) |
| regex | `````\w+\s+Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 319 |


**literal-suffix-som**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/literal-suffix-som` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+\s+Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 4073 |


**literal-inner-nosom**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/literal-inner-nosom` |
| model | [`count`](MODELS.md#count) |
| regex | `````\b\w+\s+Holmes\s+\w+\b````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 137 |


**literal-inner-som**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/literal-inner-som` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b\w+\s+Holmes\s+\w+\b````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 2593 |


**fixed-length-words-nosom**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/fixed-length-words-nosom` |
| model | [`count`](MODELS.md#count) |
| regex | `````\w{5}\s\w{6}\s\w{7}````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 120 |


**fixed-length-words-som**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/fixed-length-words-som` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w{5}\s\w{6}\s\w{7}````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 2400 |


**fixed-length-words-unicode-nosom**

| Parameter | Value |
| --------- | ----- |
| full name | `hyperscan/fixed-length-words-unicode-nosom` |
| model | [`count`](MODELS.md#count) |
| regex | `````\w{5}\s\w{6}\s\w{7}````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 120 |


</details>

### imported

#### leipzig

This benchmark seems to have originated from John Maddock as part of measuring
the performance of [boost's regex engine][maddock-bench]. It was then [adapted
by Zoltan Herczeg][herczeg-bench] to benchmark [sljit], the JIT backend for
PCRE and PCRE2. The benchmark was then further adapted by the [Rust Leipzig
User Group][leipzig-bench]. The last iteration is the one we use here.

As alluded to in the benchmark methodology, and as demonstrated in the [Leipzig
runner program][leipzig-runner], the benchmark is of the "find all matches"
variety. That is, it is not line oriented. It loads the entire haystack into
memory and then finds all matches.

As appears to be a pattern in regex benchmarks, this one also does not compare
apples-to-apples. It goes out of its way, for example, to [disable Unicode
mode for RE2][re2-disable-unicode], but [does not do the same for the regex
crate][rustregex-enable-unicode]. PCRE2 also has Unicode mode disabled (which
is the default). Anyway, we disable Unicode for almost all benchmarks, since
this is a predominantly ASCII benchmark. There are a couple benchmarks that
make use of Unicode features, and for those, we enable Unicode for all regex
engines.

[maddock-bench]: https://www.boost.org/doc/libs/1_41_0/libs/regex/doc/gcc-performance.html
[herczeg-bench]: https://zherczeg.github.io/sljit/regex_perf.html
[sljit]: https://github.com/zherczeg/sljit/
[leipzig-bench]: https://github.com/rust-leipzig/regex-performance
[leipzig-runner]: https://github.com/rust-leipzig/regex-performance/blob/52cb0538eca86ad549f6895dbfa9a2f71bc82244/src/main.c
[re2-disable-unicode]: https://github.com/rust-leipzig/regex-performance/blob/52cb0538eca86ad549f6895dbfa9a2f71bc82244/src/re2.cpp#L13
[rustregex-enable-unicode]: https://github.com/rust-leipzig/regex-performance/blob/52cb0538eca86ad549f6895dbfa9a2f71bc82244/src/rust/src/lib.rs#L17-L20

| Engine | twain | twain-insensitive | shing | huck-saw | word-ending-nn | certain-long-strings-ending-x | tom-sawyer-huckle-finn | tom-sawyer-huckle-fin-insensitive | tom-sawyer-huckle-fin-prefix-short | tom-sawyer-huckle-fin-prefix-long | tom-river | ing | ing-whitespace | awyer-inn | quotes-bounded | non-ascii-alternate | math-symbols | bounded-strings-ending-z |
| - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - |
| dotnet/compiled | 11.2 GB/s | 3.5 GB/s | 11.0 GB/s | 11.0 GB/s | 98.4 MB/s | **14.3 GB/s** | 6.8 GB/s | 506.5 MB/s | 92.0 MB/s | 90.1 MB/s | 2.7 GB/s | 264.3 MB/s | 209.8 MB/s | 1218.8 MB/s | 4.0 GB/s | 16.8 GB/s | 2.1 GB/s | 50.0 MB/s |
| dotnet/nobacktrack | 11.0 GB/s | 3.0 GB/s | 10.7 GB/s | 10.5 GB/s | 166.5 MB/s | 9.7 GB/s | 5.4 GB/s | 252.6 MB/s | 482.4 MB/s | 483.9 MB/s | 2.2 GB/s | 177.1 MB/s | 361.2 MB/s | 1143.1 MB/s | 2.2 GB/s | 17.0 GB/s | 958.7 MB/s | 484.2 MB/s |
| go/regexp | 17.4 GB/s | 62.3 MB/s | 51.0 MB/s | 48.1 MB/s | 34.5 MB/s | 15.4 MB/s | 24.2 MB/s | 14.8 MB/s | 15.4 MB/s | 13.5 MB/s | 46.6 MB/s | 40.3 MB/s | 28.8 MB/s | 31.0 MB/s | 54.9 MB/s | 81.2 MB/s | 48.5 MB/s | 32.9 MB/s |
| hyperscan | **54.8 GB/s** | **44.4 GB/s** | 8.9 GB/s | 19.9 GB/s | **39.5 GB/s** | 593.1 MB/s | 13.1 GB/s | **12.5 GB/s** | 15.1 GB/s | 14.9 GB/s | **19.8 GB/s** | **5.1 GB/s** | 2.7 GB/s | 7.3 GB/s | 2.5 GB/s | 22.7 GB/s | **13.2 GB/s** | 30.9 GB/s |
| icu | 1422.0 MB/s | 344.1 MB/s | 82.6 MB/s | 1068.7 MB/s | 37.7 MB/s | 29.7 MB/s | 893.6 MB/s | 86.6 MB/s | 11.6 MB/s | 11.2 MB/s | 429.4 MB/s | 33.3 MB/s | 66.4 MB/s | 46.4 MB/s | 507.9 MB/s | 1225.7 MB/s | 1210.2 MB/s | - |
| java/hotspot | 946.2 MB/s | 280.5 MB/s | 184.7 MB/s | 155.3 MB/s | 81.0 MB/s | 48.3 MB/s | 83.9 MB/s | 69.5 MB/s | 21.4 MB/s | 19.0 MB/s | 138.1 MB/s | 64.1 MB/s | 88.1 MB/s | 62.6 MB/s | 226.7 MB/s | 208.1 MB/s | 286.4 MB/s | 4.7 MB/s |
| javascript/v8 | 15.7 GB/s | 1890.1 MB/s | 2.2 GB/s | 2.2 GB/s | 267.5 MB/s | 74.4 MB/s | 936.4 MB/s | 424.0 MB/s | 353.8 MB/s | 338.5 MB/s | 1530.3 MB/s | 185.9 MB/s | 223.2 MB/s | 1383.3 MB/s | 1345.6 MB/s | 2.4 GB/s | 137.5 MB/s | 26.6 MB/s |
| pcre2 | 12.5 GB/s | 600.8 MB/s | 79.7 MB/s | 2.1 GB/s | 54.3 MB/s | 63.7 MB/s | 1545.8 MB/s | 122.7 MB/s | 13.8 MB/s | 13.3 MB/s | 646.6 MB/s | 33.8 MB/s | 80.7 MB/s | 32.9 MB/s | 660.6 MB/s | 2.3 GB/s | 34.5 MB/s | 86.6 MB/s |
| pcre2/jit | 37.5 GB/s | 26.1 GB/s | 18.6 GB/s | **22.8 GB/s** | 366.4 MB/s | 211.2 MB/s | 1474.1 MB/s | 465.6 MB/s | 191.7 MB/s | 172.5 MB/s | 8.5 GB/s | 432.8 MB/s | 341.3 MB/s | 2.1 GB/s | 4.0 GB/s | 36.3 GB/s | 758.3 MB/s | 829.6 MB/s |
| perl | 2.1 GB/s | 910.7 MB/s | 4.2 GB/s | 2.3 GB/s | 67.7 MB/s | 9.6 GB/s | 1923.4 MB/s | 105.9 MB/s | 27.4 MB/s | 28.2 MB/s | 826.4 MB/s | 93.2 MB/s | 494.7 MB/s | 31.5 MB/s | 742.8 MB/s | 447.1 MB/s | 354.6 MB/s | **56.5 GB/s** |
| python/re | 4.3 GB/s | 276.1 MB/s | 141.6 MB/s | 476.8 MB/s | 93.3 MB/s | 50.1 MB/s | 498.3 MB/s | 51.6 MB/s | 36.9 MB/s | 37.6 MB/s | 345.8 MB/s | 66.6 MB/s | 103.9 MB/s | 81.3 MB/s | 325.0 MB/s | 485.1 MB/s | - | 4.1 MB/s |
| python/regex | 1725.7 MB/s | 1481.3 MB/s | 1347.9 MB/s | 425.2 MB/s | 44.3 MB/s | 2.2 GB/s | 360.5 MB/s | 61.0 MB/s | 10.0 MB/s | 9.8 MB/s | 255.9 MB/s | 24.2 MB/s | 465.8 MB/s | 34.1 MB/s | 273.0 MB/s | 448.4 MB/s | 483.0 MB/s | 4.1 MB/s |
| re2 | 23.8 GB/s | 2.9 GB/s | 979.0 MB/s | 982.1 MB/s | 986.6 MB/s | 298.1 MB/s | 975.2 MB/s | 969.0 MB/s | 975.9 MB/s | 979.0 MB/s | 983.4 MB/s | 703.1 MB/s | 760.9 MB/s | 983.4 MB/s | 936.9 MB/s | 966.6 MB/s | 980.9 MB/s | 985.3 MB/s |
| regress | 3.4 GB/s | 2.3 GB/s | 192.6 MB/s | 15.4 GB/s | 117.8 MB/s | 71.7 MB/s | 2.3 GB/s | 200.4 MB/s | 25.5 MB/s | 24.6 MB/s | 2.2 GB/s | 82.7 MB/s | 147.4 MB/s | 114.6 MB/s | 1247.7 MB/s | **85.0 GB/s** | 94.5 MB/s | 7.6 MB/s |
| rust/regex | 38.8 GB/s | 16.7 GB/s | **28.3 GB/s** | 18.6 GB/s | 825.5 MB/s | 440.0 MB/s | **20.6 GB/s** | 782.8 MB/s | 822.0 MB/s | 821.5 MB/s | 13.8 GB/s | 3.3 GB/s | 545.8 MB/s | 825.1 MB/s | 3.4 GB/s | 23.0 GB/s | 825.1 MB/s | 810.2 MB/s |
| rust/regex/meta | 35.0 GB/s | 16.3 GB/s | 27.2 GB/s | 22.0 GB/s | 32.0 GB/s | 5.7 GB/s | 20.3 GB/s | 1229.6 MB/s | **19.2 GB/s** | **18.4 GB/s** | 14.2 GB/s | 3.0 GB/s | **2.8 GB/s** | **15.8 GB/s** | **4.3 GB/s** | 23.0 GB/s | 826.0 MB/s | 49.4 GB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**twain**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/twain` |
| model | [`count`](MODELS.md#count) |
| regex | `````Twain````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`.*`) | 811 |


**twain-insensitive**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/twain-insensitive` |
| model | [`count`](MODELS.md#count) |
| regex | `````Twain````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`.*`) | 965 |


**shing**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/shing` |
| model | [`count`](MODELS.md#count) |
| regex | `````[a-z]shing````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`.*`) | 1540 |


**huck-saw**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/huck-saw` |
| model | [`count`](MODELS.md#count) |
| regex | `````Huck[a-zA-Z]+\|Saw[a-zA-Z]+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`hyperscan`) | 977 |
| count(`.*`) | 262 |


**word-ending-nn**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/word-ending-nn` |
| model | [`count`](MODELS.md#count) |
| regex | `````\b\w+nn\b````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`.*`) | 262 |


**certain-long-strings-ending-x**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/certain-long-strings-ending-x` |
| model | [`count`](MODELS.md#count) |
| regex | `````[a-q][^u-z]{13}x````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`.*`) | 4094 |


**tom-sawyer-huckle-finn**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/tom-sawyer-huckle-finn` |
| model | [`count`](MODELS.md#count) |
| regex | `````Tom\|Sawyer\|Huckleberry\|Finn````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`.*`) | 2598 |


**tom-sawyer-huckle-fin-insensitive**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/tom-sawyer-huckle-fin-insensitive` |
| model | [`count`](MODELS.md#count) |
| regex | `````Tom\|Sawyer\|Huckleberry\|Finn````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`.*`) | 4152 |


**tom-sawyer-huckle-fin-prefix-short**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/tom-sawyer-huckle-fin-prefix-short` |
| model | [`count`](MODELS.md#count) |
| regex | `````.{0,2}(Tom\|Sawyer\|Huckleberry\|Finn)````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`.*`) | 2598 |


**tom-sawyer-huckle-fin-prefix-long**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/tom-sawyer-huckle-fin-prefix-long` |
| model | [`count`](MODELS.md#count) |
| regex | `````.{2,4}(Tom\|Sawyer\|Huckleberry\|Finn)````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`hyperscan`) | 2128 |
| count(`.*`) | 1976 |


**tom-river**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/tom-river` |
| model | [`count`](MODELS.md#count) |
| regex | `````Tom.{10,25}river\|river.{10,25}Tom````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`.*`) | 2 |


**ing**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/ing` |
| model | [`count`](MODELS.md#count) |
| regex | `````[a-zA-Z]+ing````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`hyperscan`) | 78872 |
| count(`.*`) | 78424 |


**ing-whitespace**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/ing-whitespace` |
| model | [`count`](MODELS.md#count) |
| regex | `````\s[a-zA-Z]{0,12}ing\s````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`hyperscan`) | 55640 |
| count(`.*`) | 55248 |


**awyer-inn**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/awyer-inn` |
| model | [`count`](MODELS.md#count) |
| regex | `````([A-Za-z]awyer\|[A-Za-z]inn)\s````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`.*`) | 209 |


**quotes-bounded**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/quotes-bounded` |
| model | [`count`](MODELS.md#count) |
| regex | `````["'][^"']{0,30}[?!\.]["']````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`hyperscan`) | 8898 |
| count(`.*`) | 8886 |


**non-ascii-alternate**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/non-ascii-alternate` |
| model | [`count`](MODELS.md#count) |
| regex | `````∞\|✓````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`.*`) | 2 |


**math-symbols**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/math-symbols` |
| model | [`count`](MODELS.md#count) |
| regex | `````\p{Sm}````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`.*`) | 69 |

* `python/re` doesn't support `\p{Sm}`.

**bounded-strings-ending-z**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/leipzig/bounded-strings-ending-z` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````(.*?,){13}z````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/leipzig-3200.txt`](benchmarks/haystacks/imported/leipzig-3200.txt) |
| count(`.*`) | 0 |


</details>

#### lh3lh3-reb

According to the brief methodology description, [lh3lh3-reb] is a line oriented
benchmark. That is, it proceeds by reading the input line-by-line and printing
lines that match. So it doesn't measure the time for finding all matches, but
rather, all matching lines. This is why we use the 'grep' benchmark type here.

Note that the haystack contains invalid UTF-8. Many regex engines can handle
this, but many cannot.

It's also consistent with the [benchmark programs
themselves][lh3lh3-re2-runner].

[lh3lh3-reb]: http://lh3lh3.users.sourceforge.net/reb.shtml
[lh3lh3-re2-runner]: https://sourceforge.net/p/klib/code/HEAD/tree/trunk/reb/re2/test.cc

| Engine | uri | email | date | uri-or-email |
| - | - | - | - | - |
| dotnet/compiled | 120.9 MB/s | 117.1 MB/s | 573.9 MB/s | 70.9 MB/s |
| dotnet/nobacktrack | 179.0 MB/s | 187.0 MB/s | 381.7 MB/s | 227.4 MB/s |
| go/regexp | 44.8 MB/s | 48.7 MB/s | 80.2 MB/s | 24.2 MB/s |
| hyperscan | 1649.6 MB/s | 1719.7 MB/s | 1551.4 MB/s | 708.4 MB/s |
| icu | 22.4 MB/s | 16.5 MB/s | 268.7 MB/s | 9.3 MB/s |
| java/hotspot | 50.7 MB/s | 37.7 MB/s | 173.3 MB/s | 18.9 MB/s |
| javascript/v8 | 123.7 MB/s | 99.7 MB/s | 357.6 MB/s | 66.5 MB/s |
| pcre2 | 574.0 MB/s | 1133.1 MB/s | 925.3 MB/s | 30.9 MB/s |
| pcre2/jit | 1157.3 MB/s | 1635.4 MB/s | 1194.0 MB/s | 228.1 MB/s |
| perl | 138.2 MB/s | 141.3 MB/s | 141.4 MB/s | 14.4 MB/s |
| python/re | 35.9 MB/s | 20.8 MB/s | 101.7 MB/s | 13.3 MB/s |
| python/regex | 114.7 MB/s | 111.9 MB/s | 118.2 MB/s | 8.2 MB/s |
| re2 | 521.0 MB/s | 517.8 MB/s | 516.9 MB/s | 520.9 MB/s |
| regress | 59.4 MB/s | 30.7 MB/s | 489.6 MB/s | 22.6 MB/s |
| rust/regex | 806.9 MB/s | 801.5 MB/s | 692.4 MB/s | **806.9 MB/s** |
| rust/regex/meta | **2.2 GB/s** | **2.7 GB/s** | **2.1 GB/s** | 743.7 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**uri**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/lh3lh3-reb/uri` |
| model | [`grep`](MODELS.md#grep) |
| regex | `````([a-zA-Z][a-zA-Z0-9]*)://([^ /]+)(/[^ ]*)?````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/lh3lh3-reb-howto.txt`](benchmarks/haystacks/imported/lh3lh3-reb-howto.txt) |
| count(`.*`) | 17549 |


**email**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/lh3lh3-reb/email` |
| model | [`grep`](MODELS.md#grep) |
| regex | `````([^ @]+)@([^ @]+)````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/lh3lh3-reb-howto.txt`](benchmarks/haystacks/imported/lh3lh3-reb-howto.txt) |
| count(`.*`) | 15057 |


**date**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/lh3lh3-reb/date` |
| model | [`grep`](MODELS.md#grep) |
| regex | `````([0-9][0-9]?)/([0-9][0-9]?)/([0-9][0-9]([0-9][0-9])?)````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/lh3lh3-reb-howto.txt`](benchmarks/haystacks/imported/lh3lh3-reb-howto.txt) |
| count(`.*`) | 668 |


**uri-or-email**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/lh3lh3-reb/uri-or-email` |
| model | [`grep`](MODELS.md#grep) |
| regex | `````([a-zA-Z][a-zA-Z0-9]*)://([^ /]+)(/[^ ]*)?\|([^ @]+)@([^ @]+)````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/lh3lh3-reb-howto.txt`](benchmarks/haystacks/imported/lh3lh3-reb-howto.txt) |
| count(`.*`) | 32539 |


</details>

#### mariomka

This group of benchmarks was imported from
[mariomka's regex benchmark][mariomka].

The benchmark metholodgy section for this benchmark is extremely sparse, but
after looking at [some example programs][mariomka-example], it's clear that
it slurps up the entire haystack and just finds all matches. That is, this is
*not* a line oriented benchmark.

The benchmark *also* includes the compile times of regexes. But... it uses
a haystack that is so small that the compile times can actually have a
measurable impact on the final results. The benchmark makes an attempt to
control for this by providing an "optimized" version, which would be better
called "where we compare apples to apples."

Anyway, in this benchmark suite, we separate compile time from search
time. And here, we only measure search time, so it is not quite a precise
reproduction of mariomka's benchmark.

Another difference from mariomka's benchmark is that we try to control for
equivalent functionality between regex engines, instead of comparing apples
with oranges. For example, we don't let one regex engine run with Unicode
enabled and another run without it enabled.

[mariomka]: https://github.com/mariomka/regex-benchmark
[mariomka-example]: https://github.com/mariomka/regex-benchmark/blob/17d073ec864931546e2694783f6231e4696a9ed4/rust/src/main.rs

| Engine | email | uri | ip |
| - | - | - | - |
| dotnet/compiled | 30.4 GB/s | **8.2 GB/s** | 1777.3 MB/s |
| dotnet/nobacktrack | 26.9 GB/s | 3.5 GB/s | 705.1 MB/s |
| go/regexp | 46.6 MB/s | 46.9 MB/s | 30.4 MB/s |
| hyperscan | 37.6 GB/s | 4.1 GB/s | **14.9 GB/s** |
| icu | 30.8 MB/s | 34.3 MB/s | 574.2 MB/s |
| java/hotspot | 62.1 MB/s | 73.0 MB/s | 53.0 MB/s |
| javascript/v8 | 163.8 MB/s | 213.5 MB/s | 10.4 GB/s |
| pcre2 | 87.8 MB/s | 94.4 MB/s | 819.4 MB/s |
| pcre2/jit | 497.9 MB/s | 496.8 MB/s | 2.5 GB/s |
| perl | 159.9 MB/s | 154.9 MB/s | 581.3 MB/s |
| python/re | 46.3 MB/s | 77.3 MB/s | 40.5 MB/s |
| python/regex | 20.6 MB/s | 36.2 MB/s | 736.2 MB/s |
| re2 | 986.8 MB/s | 891.1 MB/s | 982.3 MB/s |
| regress | 88.5 MB/s | 99.0 MB/s | 105.3 MB/s |
| rust/regex | 825.6 MB/s | 765.6 MB/s | 721.5 MB/s |
| rust/regex/meta | **52.8 GB/s** | 7.9 GB/s | 3.0 GB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**email**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/mariomka/email` |
| model | [`count`](MODELS.md#count) |
| regex | `````[\w\.+-]+@[\w\.-]+\.[\w\.-]+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/mariomka.txt`](benchmarks/haystacks/imported/mariomka.txt) |
| count(`hyperscan`) | 351 |
| count(`.*`) | 92 |


**uri**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/mariomka/uri` |
| model | [`count`](MODELS.md#count) |
| regex | `````[\w]+://[^/\s?#]+[^\s?#]+(?:\?[^\s#]*)?(?:#[^\s]*)?````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/mariomka.txt`](benchmarks/haystacks/imported/mariomka.txt) |
| count(`hyperscan`) | 171720 |
| count(`.*`) | 5301 |


**ip**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/mariomka/ip` |
| model | [`count`](MODELS.md#count) |
| regex | `````(?:(?:25[0-5]\|2[0-4][0-9]\|[01]?[0-9][0-9])\.){3}(?:25[0-5]\|2[0-4][0-9]\|[01]?[0-9][0-9])````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/mariomka.txt`](benchmarks/haystacks/imported/mariomka.txt) |
| count(`.*`) | 5 |


</details>

#### regex-redux

This is a port of the [Benchmark Game's regex-redux
benchmark][benchgame-regexredux]. It is not precisely equivalent because it
bans things like parallelism and ensures all regex engines use as similar
configuration as possible. (For example, disabling Unicode mode in any regex
engine that enables it by default, since this benchmark is purely ASCII.)

See the [MODELS](MODELS.md) document in the repo root for a more in depth
description of the regex-redux model. Namely, this particular benchmark is
different from the rest in that it is more complicated than "search a regex in
some haystack repeatedly."

[benchgame-regexredux]: https://benchmarksgame-team.pages.debian.net/benchmarksgame/performance/regexredux.html

| Engine | regex-redux |
| - | - |
| dotnet | 223.01ms |
| dotnet/compiled | 62.28ms |
| dotnet/nobacktrack | 65.55ms |
| go/regexp | 407.51ms |
| hyperscan | 77.04ms |
| java/hotspot | 137.21ms |
| javascript/v8 | 25.35ms |
| pcre2 | 158.03ms |
| pcre2/jit | 21.00ms |
| perl | 151.96ms |
| python/re | 135.95ms |
| python/regex | 202.29ms |
| re2 | 30.96ms |
| regress | 63.68ms |
| rust/regex | 13.09ms |
| rust/regex/meta | **12.45ms** |

<details>
<summary>Show individual benchmark parameters.</summary>

**regex-redux**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/regex-redux/regex-redux` |
| model | [`regex-redux`](MODELS.md#regex-redux) |
| regex | NONE |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/regex-redux-100000.fasta`](benchmarks/haystacks/imported/regex-redux-100000.fasta) |
| count(`.*`) | 547899 |


</details>

#### rsc

These benchmarks primarily come from Russ Cox's [RE2 and Go regexp
libraries][golang-regexp-bench]. They were originally included as ["misc"
benchmarks in the regex crate][rust-regex-rsc-bench] before being ported here.

There are likely some small changes and a few benchmarks that I've added, but
they should be largely the same. Many of them (like 'easy1') are somewhat
"broken" in that they are anchored at the end as a way of ensuring the regex
engine scans through the entire haystack. But at some point, the regex crate
(and also RE2) got an optimization that does a reverse scan when the regex is
anchored at the end, which made the point of those benchmarks somewhat moot.
Instead of reconceptualizing them, we retain them as-is.

[golang-regexp-bench]: https://github.com/golang/go/blob/5724daa6825db0a9097254060633439e6538d845/src/regexp/all_test.go#L666
[rust-regex-rsc-bench]: https://github.com/rust-lang/regex/blob/a9b2e02352db92ce1f6e5b7ecd41b8bbffbe161a/bench/src/misc.rs

| Engine | no-exponential | literal | not-literal | match-class | match-class-in-range | match-class-unicode | anchored-literal-short-non-match | anchored-literal-long-non-match | anchored-literal-short-match | anchored-literal-long-match | one-pass-short | one-pass-short-not | one-pass-long-prefix | one-pass-long-prefix-not | long-needle1 | long-needle2 | easy0-32 | easy0-1k | easy0-32k | easy0-1mb | easy1-32 | easy1-1k | easy1-32k | easy1-1mb | medium-32 | medium-1k | medium-32k | medium-1mb | hard-32 | hard-1k | hard-32k | hard-1mb | reallyhard0-32 | reallyhard0-1k | reallyhard0-32k | reallyhard0-1mb | reallyreallyhard0-32 | reallyreallyhard0-1k | reallyreallyhard0-32k | reallyreallyhard0-1mb | reallyreallyreallyhard0-32 | reallyreallyreallyhard0-1k | reallyreallyreallyhard0-32k | reallyreallyreallyhard0-1mb |
| - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - |
| go/regexp | 33.3 MB/s | 337.8 MB/s | 87.3 MB/s | 96.3 MB/s | 105.1 MB/s | 103.7 MB/s | 604.8 MB/s | 8.4 GB/s | 213.8 MB/s | 3.0 GB/s | 62.8 MB/s | 49.0 MB/s | 206.6 MB/s | 138.5 MB/s | 5.6 MB/s | 5.5 MB/s | 280.8 MB/s | 4.0 GB/s | 30.9 GB/s | 55.6 GB/s | 235.0 MB/s | 4.3 GB/s | 33.5 GB/s | 66.2 GB/s | 129.3 MB/s | 127.0 MB/s | 78.1 MB/s | 77.9 MB/s | 78.6 MB/s | 66.1 MB/s | 44.5 MB/s | 42.5 MB/s | 77.7 MB/s | 67.0 MB/s | 45.1 MB/s | 44.0 MB/s | 93.6 MB/s | 50.4 MB/s | 36.2 MB/s | 35.9 MB/s | 92.6 MB/s | 50.1 MB/s | 36.3 MB/s | 35.9 MB/s |
| pcre2 | 21 B/s | 972.7 MB/s | 66.1 MB/s | 908.8 MB/s | 908.8 MB/s | 90.9 MB/s | 1180.7 MB/s | 17.3 GB/s | 335.1 MB/s | 4.9 GB/s | 70.5 MB/s | 71.7 MB/s | 302.4 MB/s | 269.5 MB/s | 20.9 MB/s | 20.8 MB/s | 666.4 MB/s | 11.4 GB/s | **120.7 GB/s** | **120.0 GB/s** | 552.7 MB/s | 10.7 GB/s | 117.9 GB/s | 122.7 GB/s | 562.7 MB/s | 2.2 GB/s | 2.7 GB/s | 2.7 GB/s | 194.1 MB/s | 7.5 MB/s | 7.0 MB/s | 6.8 MB/s | 189.4 MB/s | 7.5 MB/s | 7.0 MB/s | 7.0 MB/s | 567.4 MB/s | 35.6 MB/s | 32.9 MB/s | 30.1 MB/s | 472.2 MB/s | 33.8 MB/s | 32.2 MB/s | 29.6 MB/s |
| pcre2/jit | 157 B/s | 1519.9 MB/s | **1473.9 MB/s** | 1404.5 MB/s | 2.3 GB/s | **783.4 MB/s** | 1239.8 MB/s | 18.2 GB/s | **774.9 MB/s** | **11.7 GB/s** | **395.4 MB/s** | **368.5 MB/s** | **799.9 MB/s** | **774.9 MB/s** | 328.3 MB/s | 328.3 MB/s | 1580.4 MB/s | 15.0 GB/s | 43.6 GB/s | 43.3 GB/s | **1314.5 MB/s** | 14.7 GB/s | 43.6 GB/s | 43.2 GB/s | **1607.6 MB/s** | 15.3 GB/s | 42.8 GB/s | 46.2 GB/s | **987.7 MB/s** | 1882.3 MB/s | 1684.2 MB/s | 1300.5 MB/s | **970.4 MB/s** | 1834.0 MB/s | 1680.5 MB/s | 1310.2 MB/s | **862.0 MB/s** | 1811.5 MB/s | 1365.8 MB/s | 1141.0 MB/s | **935.3 MB/s** | 1869.7 MB/s | 1499.7 MB/s | 1165.9 MB/s |
| python/re | 18 B/s | 111.8 MB/s | 75.9 MB/s | 112.9 MB/s | 115.0 MB/s | - | 57.0 MB/s | 243.1 MB/s | 46.3 MB/s | 229.6 MB/s | 22.4 MB/s | 21.8 MB/s | 48.5 MB/s | 47.8 MB/s | 704.3 MB/s | **704.2 MB/s** | 119.7 MB/s | 1526.5 MB/s | 4.5 GB/s | 4.8 GB/s | 95.2 MB/s | 1406.9 MB/s | 4.4 GB/s | 4.8 GB/s | 95.2 MB/s | 363.2 MB/s | 450.1 MB/s | 446.4 MB/s | 97.7 MB/s | 31.6 MB/s | 29.8 MB/s | 29.8 MB/s | 98.1 MB/s | 31.6 MB/s | 29.8 MB/s | 29.9 MB/s | 86.2 MB/s | 6.7 MB/s | 6.4 MB/s | 6.4 MB/s | 89.2 MB/s | 6.7 MB/s | 6.4 MB/s | 6.4 MB/s |
| python/regex | 765.6 KB/s | 103.7 MB/s | 101.5 MB/s | 116.7 MB/s | 116.9 MB/s | 224.1 MB/s | 70.8 MB/s | 1053.6 MB/s | 47.8 MB/s | 718.0 MB/s | 13.7 MB/s | 13.5 MB/s | 50.7 MB/s | 51.4 MB/s | 448.9 MB/s | 443.7 MB/s | 117.7 MB/s | 1844.1 MB/s | 8.0 GB/s | 11.3 GB/s | 90.2 MB/s | 1335.1 MB/s | 4.4 GB/s | 4.8 GB/s | 111.6 MB/s | 1793.0 MB/s | 9.5 GB/s | 11.3 GB/s | 98.6 MB/s | 266.3 MB/s | 247.1 MB/s | 243.9 MB/s | 97.9 MB/s | 257.4 MB/s | 247.0 MB/s | 242.7 MB/s | 70.8 MB/s | 6.1 MB/s | 5.8 MB/s | 5.8 MB/s | 71.8 MB/s | 6.1 MB/s | 5.8 MB/s | 5.8 MB/s |
| re2 | 390.9 MB/s | 486.4 MB/s | 340.1 MB/s | 446.5 MB/s | 451.7 MB/s | 616.6 MB/s | **1458.6 MB/s** | **21.4 GB/s** | 486.2 MB/s | 7.1 GB/s | 257.3 MB/s | 176.2 MB/s | 506.0 MB/s | 344.4 MB/s | 253.6 MB/s | 251.3 MB/s | 366.3 MB/s | 6.3 GB/s | 88.0 GB/s | 116.1 GB/s | 523.0 MB/s | 10.4 GB/s | 324.8 GB/s | 10389.2 GB/s | 521.0 MB/s | 9.1 GB/s | 282.8 GB/s | 9042.5 GB/s | 422.2 MB/s | 9.0 GB/s | 282.8 GB/s | 7689.7 GB/s | 250.3 MB/s | 886.2 MB/s | 986.9 MB/s | 990.1 MB/s | 250.4 MB/s | 900.8 MB/s | 988.4 MB/s | 990.1 MB/s | 238.4 MB/s | 880.2 MB/s | 987.6 MB/s | 990.1 MB/s |
| regress | 30 B/s | 1247.1 MB/s | 262.9 MB/s | 1355.2 MB/s | 2032.8 MB/s | - | 253.0 MB/s | 368.3 MB/s | 203.2 MB/s | 361.1 MB/s | 74.0 MB/s | 72.7 MB/s | 604.8 MB/s | 604.8 MB/s | 248.1 MB/s | 248.1 MB/s | 1106.3 MB/s | 3.1 GB/s | 3.4 GB/s | 3.5 GB/s | 1034.8 MB/s | 18.0 GB/s | 137.5 GB/s | 108.6 GB/s | 1250.4 MB/s | 2.8 GB/s | 3.2 GB/s | 3.2 GB/s | 327.3 MB/s | 19.2 MB/s | 17.9 MB/s | 17.8 MB/s | 319.7 MB/s | 18.0 MB/s | 17.9 MB/s | 17.5 MB/s | 527.3 MB/s | 7.8 MB/s | 7.4 MB/s | 7.6 MB/s | 426.6 MB/s | 8.2 MB/s | 7.5 MB/s | 7.6 MB/s |
| rust/regex/meta | **467.5 MB/s** | **2.6 GB/s** | 1247.1 MB/s | **2.3 GB/s** | **4.0 GB/s** | 701.1 MB/s | 1078.1 MB/s | 15.1 GB/s | 729.3 MB/s | 10.7 GB/s | 330.9 MB/s | 368.5 MB/s | 435.0 MB/s | 435.0 MB/s | **50.9 GB/s** | 487.7 MB/s | **2.3 GB/s** | **23.9 GB/s** | 72.0 GB/s | 40.7 GB/s | 1034.8 MB/s | **20.7 GB/s** | **649.7 GB/s** | **20778.3 GB/s** | 937.8 MB/s | **16.6 GB/s** | **517.7 GB/s** | **16837.7 GB/s** | 558.7 MB/s | **16.9 GB/s** | **526.6 GB/s** | **9574.4 GB/s** | 329.2 MB/s | **11.1 GB/s** | **62.0 GB/s** | **41.4 GB/s** | 278.4 MB/s | **10.3 GB/s** | **55.4 GB/s** | **50.9 GB/s** | 287.8 MB/s | **11.2 GB/s** | **54.3 GB/s** | **50.8 GB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**no-exponential**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/no-exponential` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````(a+)*[b-z]````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `aaaaaaaaaaaaaaaaaaaaaaaaa` |
| count(`.*`) | 0 |


**literal**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/literal` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````y````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxy` |
| count(`.*`) | 1 |


**not-literal**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/not-literal` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````.y````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxy` |
| count(`.*`) | 2 |


**match-class**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/match-class` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[abcdw]````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx [.. snip ..]` |
| count(`.*`) | 1 |


**match-class-in-range**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/match-class-in-range` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[ac]````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb [.. snip ..]` |
| count(`.*`) | 1 |


**match-class-unicode**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/match-class-unicode` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\pL````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack | `☃5☃5☃5☃5☃5☃5☃5☃5☃5☃5☃5☃5☃5☃5☃5 [.. snip ..]` |
| count(`.*`) | 1 |


**anchored-literal-short-non-match**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/anchored-literal-short-non-match` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````^zbc(d\|e)````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `abcdefghijklmnopqrstuvwxyz` |
| count(`.*`) | 0 |


**anchored-literal-long-non-match**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/anchored-literal-long-non-match` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````^zbc(d\|e)````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/anchored-literal-long.txt`](benchmarks/haystacks/imported/rsc/anchored-literal-long.txt) |
| count(`.*`) | 0 |


**anchored-literal-short-match**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/anchored-literal-short-match` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````^.bc(d\|e)````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `abcdefghijklmnopqrstuvwxyz` |
| count(`.*`) | 4 |


**anchored-literal-long-match**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/anchored-literal-long-match` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````^.bc(d\|e)````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/anchored-literal-long.txt`](benchmarks/haystacks/imported/rsc/anchored-literal-long.txt) |
| count(`.*`) | 4 |


**one-pass-short**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/one-pass-short` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````^.bc(d\|e)*$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `abcddddddeeeededd` |
| count(`.*`) | 17 |


**one-pass-short-not**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/one-pass-short-not` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````.bc(d\|e)*$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `abcddddddeeeededd` |
| count(`.*`) | 17 |


**one-pass-long-prefix**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/one-pass-long-prefix` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````^abcdefghijklmnopqrstuvwxyz.*$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `abcdefghijklmnopqrstuvwxyz` |
| count(`.*`) | 26 |


**one-pass-long-prefix-not**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/one-pass-long-prefix-not` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````^.bcdefghijklmnopqrstuvwxyz.*$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `abcdefghijklmnopqrstuvwxyz` |
| count(`.*`) | 26 |


**long-needle1**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/long-needle1` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````aaaaaaaaaaaaaaaaaaaaaaaaaaaaaab````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/long-needle1.txt`](benchmarks/haystacks/imported/rsc/long-needle1.txt) |
| count(`.*`) | 31 |


**long-needle2**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/long-needle2` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````bbbbbbbbbbbbbbbbbbbbbbbbbbbbbba````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/long-needle2.txt`](benchmarks/haystacks/imported/rsc/long-needle2.txt) |
| count(`.*`) | 31 |


**easy0-32**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/easy0-32` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````ABCDEFGHIJKLMNOPQRSTUVWXYZ````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32.txt`](benchmarks/haystacks/imported/rsc/32.txt) |
| count(`.*`) | 26 |


**easy0-1k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/easy0-1k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````ABCDEFGHIJKLMNOPQRSTUVWXYZ````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1K.txt`](benchmarks/haystacks/imported/rsc/1K.txt) |
| count(`.*`) | 26 |


**easy0-32k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/easy0-32k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````ABCDEFGHIJKLMNOPQRSTUVWXYZ````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32K.txt`](benchmarks/haystacks/imported/rsc/32K.txt) |
| count(`.*`) | 26 |


**easy0-1mb**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/easy0-1mb` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````ABCDEFGHIJKLMNOPQRSTUVWXYZ````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1MB.txt`](benchmarks/haystacks/imported/rsc/1MB.txt) |
| count(`.*`) | 26 |


**easy1-32**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/easy1-32` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````A[AB]B[BC]C[CD]D[DE]E[EF]F[FG]G[GH]H[HI]I[IJ]J$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32.txt`](benchmarks/haystacks/imported/rsc/32.txt) |
| count(`.*`) | 19 |


**easy1-1k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/easy1-1k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````A[AB]B[BC]C[CD]D[DE]E[EF]F[FG]G[GH]H[HI]I[IJ]J$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1K.txt`](benchmarks/haystacks/imported/rsc/1K.txt) |
| count(`.*`) | 19 |


**easy1-32k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/easy1-32k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````A[AB]B[BC]C[CD]D[DE]E[EF]F[FG]G[GH]H[HI]I[IJ]J$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32K.txt`](benchmarks/haystacks/imported/rsc/32K.txt) |
| count(`.*`) | 19 |


**easy1-1mb**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/easy1-1mb` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````A[AB]B[BC]C[CD]D[DE]E[EF]F[FG]G[GH]H[HI]I[IJ]J$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1MB.txt`](benchmarks/haystacks/imported/rsc/1MB.txt) |
| count(`.*`) | 19 |


**medium-32**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/medium-32` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[XYZ]ABCDEFGHIJKLMNOPQRSTUVWXYZ$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32.txt`](benchmarks/haystacks/imported/rsc/32.txt) |
| count(`.*`) | 27 |


**medium-1k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/medium-1k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[XYZ]ABCDEFGHIJKLMNOPQRSTUVWXYZ$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1K.txt`](benchmarks/haystacks/imported/rsc/1K.txt) |
| count(`.*`) | 27 |


**medium-32k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/medium-32k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[XYZ]ABCDEFGHIJKLMNOPQRSTUVWXYZ$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32K.txt`](benchmarks/haystacks/imported/rsc/32K.txt) |
| count(`.*`) | 27 |


**medium-1mb**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/medium-1mb` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[XYZ]ABCDEFGHIJKLMNOPQRSTUVWXYZ$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1MB.txt`](benchmarks/haystacks/imported/rsc/1MB.txt) |
| count(`.*`) | 27 |


**hard-32**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/hard-32` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[ -~]*ABCDEFGHIJKLMNOPQRSTUVWXYZ$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32.txt`](benchmarks/haystacks/imported/rsc/32.txt) |
| count(`.*`) | 58 |


**hard-1k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/hard-1k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[ -~]*ABCDEFGHIJKLMNOPQRSTUVWXYZ$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1K.txt`](benchmarks/haystacks/imported/rsc/1K.txt) |
| count(`.*`) | 27 |


**hard-32k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/hard-32k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[ -~]*ABCDEFGHIJKLMNOPQRSTUVWXYZ$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32K.txt`](benchmarks/haystacks/imported/rsc/32K.txt) |
| count(`.*`) | 27 |


**hard-1mb**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/hard-1mb` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[ -~]*ABCDEFGHIJKLMNOPQRSTUVWXYZ$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1MB.txt`](benchmarks/haystacks/imported/rsc/1MB.txt) |
| count(`.*`) | 51 |


**reallyhard0-32**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/reallyhard0-32` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[ -~]*ABCDEFGHIJKLMNOPQRSTUVWXYZ.*````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32.txt`](benchmarks/haystacks/imported/rsc/32.txt) |
| count(`.*`) | 58 |


**reallyhard0-1k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/reallyhard0-1k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[ -~]*ABCDEFGHIJKLMNOPQRSTUVWXYZ.*````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1K.txt`](benchmarks/haystacks/imported/rsc/1K.txt) |
| count(`.*`) | 27 |


**reallyhard0-32k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/reallyhard0-32k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[ -~]*ABCDEFGHIJKLMNOPQRSTUVWXYZ.*````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32K.txt`](benchmarks/haystacks/imported/rsc/32K.txt) |
| count(`.*`) | 27 |


**reallyhard0-1mb**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/reallyhard0-1mb` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[ -~]*ABCDEFGHIJKLMNOPQRSTUVWXYZ.*````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1MB.txt`](benchmarks/haystacks/imported/rsc/1MB.txt) |
| count(`.*`) | 51 |


**reallyreallyhard0-32**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/reallyreallyhard0-32` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+\s+Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32.txt`](benchmarks/haystacks/imported/rsc/32.txt) |
| count(`.*`) | 47 |


**reallyreallyhard0-1k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/reallyreallyhard0-1k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+\s+Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1K.txt`](benchmarks/haystacks/imported/rsc/1K.txt) |
| count(`.*`) | 16 |


**reallyreallyhard0-32k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/reallyreallyhard0-32k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+\s+Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32K.txt`](benchmarks/haystacks/imported/rsc/32K.txt) |
| count(`.*`) | 16 |


**reallyreallyhard0-1mb**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/reallyreallyhard0-1mb` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+\s+Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1MB.txt`](benchmarks/haystacks/imported/rsc/1MB.txt) |
| count(`.*`) | 40 |


**reallyreallyreallyhard0-32**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/reallyreallyreallyhard0-32` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+\s+Holmes\s+\w+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32.txt`](benchmarks/haystacks/imported/rsc/32.txt) |
| count(`.*`) | 51 |


**reallyreallyreallyhard0-1k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/reallyreallyreallyhard0-1k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+\s+Holmes\s+\w+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1K.txt`](benchmarks/haystacks/imported/rsc/1K.txt) |
| count(`.*`) | 20 |


**reallyreallyreallyhard0-32k**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/reallyreallyreallyhard0-32k` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+\s+Holmes\s+\w+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/32K.txt`](benchmarks/haystacks/imported/rsc/32K.txt) |
| count(`.*`) | 20 |


**reallyreallyreallyhard0-1mb**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/rsc/reallyreallyreallyhard0-1mb` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+\s+Holmes\s+\w+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/rsc/1MB.txt`](benchmarks/haystacks/imported/rsc/1MB.txt) |
| count(`.*`) | 44 |


</details>

#### sherlock

These benchmarks were imported from the [old regex crate's benchmark
suite][regex-old-suite]. They were loosely inspired by the leipzig benchmark,
but I added to it quite a bit over time to test various optimizations I had
added to the regex crate. I've ported them over to this harness as a way of
doing a direct comparison between this benchmarking tool and the older suite.
The benchmarks themselves are also generally useful, but do have a lot of
overlap with other benchmarks.

[regex-old-suite]: https://github.com/rust-lang/regex/blob/a9b2e02352db92ce1f6e5b7ecd41b8bbffbe161a/bench/src/sherlock.rs

| Engine | name-sherlock | name-holmes | name-sherlock-holmes | name-sherlock-casei | name-holmes-casei | name-sherlock-holmes-casei | name-whitespace | name-alt1 | name-alt2 | name-alt3 | name-alt3-casei | name-alt4 | name-alt4-casei | name-alt5 | name-alt5-casei | no-match-uncommon | no-match-common | no-match-really-common | the-lower | the-upper | the-casei | everything-greedy | everything-greedy-nl | letters | letters-upper | letters-lower | words | before-holmes | before-after-holmes | holmes-cochar-watson | holmes-coword-watson | quotes | line-boundary-sherlock-holmes | word-ending-n | repeated-class-negation | ing-suffix | ing-suffix-limited-space |
| - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - |
| go/regexp | 14.4 GB/s | 5.4 GB/s | 12.0 GB/s | 43.4 MB/s | 63.9 MB/s | 43.6 MB/s | 10.7 GB/s | 6.1 GB/s | 48.0 MB/s | 14.4 MB/s | 9.0 MB/s | 47.0 MB/s | 26.7 MB/s | 32.9 MB/s | 18.7 MB/s | 58.3 GB/s | 2.6 GB/s | 2.3 GB/s | 263.9 MB/s | 4.6 GB/s | 56.5 MB/s | 38.1 MB/s | 46.2 MB/s | 7.7 MB/s | 36.9 MB/s | 8.9 MB/s | 18.6 MB/s | 34.1 MB/s | 34.2 MB/s | 47.4 MB/s | 9.4 MB/s | 50.7 MB/s | 54.6 MB/s | 31.7 MB/s | 15.5 MB/s | 40.6 MB/s | 29.2 MB/s |
| pcre2 | 23.0 GB/s | 12.6 GB/s | 24.8 GB/s | 832.7 MB/s | 737.7 MB/s | 992.0 MB/s | 24.9 GB/s | 20.1 GB/s | 2023.5 MB/s | 858.0 MB/s | 62.6 MB/s | 2018.3 MB/s | 291.0 MB/s | 1747.3 MB/s | 213.3 MB/s | 76.1 GB/s | 1055.1 MB/s | 876.5 MB/s | 530.3 MB/s | 13.4 GB/s | 417.2 MB/s | - | **4.8 GB/s** | - | 711.6 KB/s | - | 121.2 MB/s | 44.6 MB/s | 45.5 MB/s | 1614.9 MB/s | - | 484.9 MB/s | 60.4 MB/s | 55.5 MB/s | 64.5 MB/s | 34.2 MB/s | 80.6 MB/s |
| pcre2/jit | 34.9 GB/s | 24.9 GB/s | 36.4 GB/s | **14.8 GB/s** | **13.2 GB/s** | **23.6 GB/s** | **33.9 GB/s** | 26.3 GB/s | **17.0 GB/s** | 1350.3 MB/s | 348.1 MB/s | **17.0 GB/s** | 840.9 MB/s | 1250.4 MB/s | 707.5 MB/s | 46.2 GB/s | **46.2 GB/s** | 45.9 GB/s | 2.3 GB/s | 20.1 GB/s | **2.1 GB/s** | **988.3 MB/s** | 4.8 GB/s | **83.3 MB/s** | **550.8 MB/s** | **83.8 MB/s** | **233.5 MB/s** | 370.8 MB/s | 370.8 MB/s | **17.0 GB/s** | - | **2.7 GB/s** | **33.4 GB/s** | 313.5 MB/s | 195.6 MB/s | 402.4 MB/s | 322.4 MB/s |
| python/re | 4.3 GB/s | 3.5 GB/s | 4.3 GB/s | 292.5 MB/s | 282.3 MB/s | 292.5 MB/s | 4.1 GB/s | 3.5 GB/s | 417.2 MB/s | 420.3 MB/s | 26.1 MB/s | 414.1 MB/s | 88.4 MB/s | 472.8 MB/s | 65.5 MB/s | 4.8 GB/s | 1278.5 MB/s | 1279.1 MB/s | 545.6 MB/s | 3.1 GB/s | 179.5 MB/s | 132.6 MB/s | 2.9 GB/s | - | - | - | 36.4 MB/s | 31.9 MB/s | 31.9 MB/s | 465.1 MB/s | - | 266.4 MB/s | 143.6 MB/s | 79.4 MB/s | 50.3 MB/s | 65.9 MB/s | 102.0 MB/s |
| python/regex | 2.8 GB/s | 1725.1 MB/s | 3.6 GB/s | 2.5 GB/s | 1571.4 MB/s | 3.3 GB/s | 2.7 GB/s | 3.5 GB/s | 405.3 MB/s | 262.7 MB/s | 44.2 MB/s | 402.4 MB/s | 93.5 MB/s | 388.6 MB/s | 79.7 MB/s | 4.8 GB/s | 1312.7 MB/s | 1324.5 MB/s | 420.3 MB/s | 2.6 GB/s | 280.9 MB/s | 131.6 MB/s | 3.0 GB/s | 9.8 MB/s | 177.9 MB/s | 10.0 MB/s | 33.1 MB/s | 19.3 MB/s | 18.8 MB/s | 405.3 MB/s | - | 218.2 MB/s | 2.7 GB/s | 39.5 MB/s | 2.3 GB/s | 24.1 MB/s | 439.8 MB/s |
| re2 | 37.5 GB/s | 12.9 GB/s | 32.6 GB/s | 2.8 GB/s | 2.4 GB/s | 2.8 GB/s | 32.6 GB/s | 28.9 GB/s | 925.5 MB/s | 904.5 MB/s | 903.0 MB/s | 921.8 MB/s | 909.9 MB/s | 916.9 MB/s | 915.1 MB/s | **109.1 GB/s** | 2.9 GB/s | 2.9 GB/s | 778.4 MB/s | 8.9 GB/s | 692.4 MB/s | 210.1 MB/s | 493.4 MB/s | 21.1 MB/s | 370.8 MB/s | 21.7 MB/s | 68.9 MB/s | 945.2 MB/s | 967.3 MB/s | 986.0 MB/s | 888.8 MB/s | 880.2 MB/s | 984.9 MB/s | 461.3 MB/s | 310.0 MB/s | 711.8 MB/s | 759.2 MB/s |
| regress | 3.5 GB/s | 3.5 GB/s | 3.6 GB/s | 1086.9 MB/s | 2.4 GB/s | 1091.3 MB/s | 3.5 GB/s | **56.1 GB/s** | 16.6 GB/s | 1464.6 MB/s | 115.1 MB/s | 14.9 GB/s | 472.8 MB/s | 2.5 GB/s | 341.8 MB/s | 2.4 GB/s | 2.4 GB/s | 2.4 GB/s | 1763.8 MB/s | 2.3 GB/s | 1338.8 MB/s | 700.3 MB/s | - | - | - | - | 219.1 MB/s | 34.2 MB/s | 34.2 MB/s | 8.1 GB/s | - | - | - | - | 72.4 MB/s | 83.1 MB/s | 148.9 MB/s |
| rust/regex/meta | **38.8 GB/s** | **30.6 GB/s** | **37.5 GB/s** | 13.7 GB/s | 7.0 GB/s | 11.4 GB/s | 30.7 GB/s | 42.7 GB/s | 12.7 GB/s | **11.4 GB/s** | **1776.6 MB/s** | 10.2 GB/s | **5.6 GB/s** | **12.1 GB/s** | **3.6 GB/s** | 38.5 GB/s | 39.4 GB/s | **49.5 GB/s** | **3.0 GB/s** | **26.7 GB/s** | 1825.9 MB/s | 195.6 MB/s | 241.4 MB/s | 44.7 MB/s | 484.9 MB/s | 46.2 MB/s | 100.6 MB/s | **18.3 GB/s** | **19.9 GB/s** | 9.9 GB/s | **1669.5 MB/s** | 2.6 GB/s | 32.3 GB/s | **886.3 MB/s** | **23.0 GB/s** | **3.4 GB/s** | **3.3 GB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**name-sherlock**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-sherlock` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sherlock````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 776 |


**name-holmes**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-holmes` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 2766 |


**name-sherlock-holmes**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-sherlock-holmes` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sherlock Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 1365 |


**name-sherlock-casei**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-sherlock-casei` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sherlock````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 816 |


**name-holmes-casei**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-holmes-casei` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Holmes````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 2802 |


**name-sherlock-holmes-casei**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-sherlock-holmes-casei` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sherlock Holmes````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 1440 |


**name-whitespace**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-whitespace` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sherlock\s+Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 1461 |


**name-alt1**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-alt1` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sherlock\|Street````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 1142 |


**name-alt2**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-alt2` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sherlock\|Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 3542 |


**name-alt3**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-alt3` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sherlock\|Holmes\|Watson\|Irene\|Adler\|John\|Baker````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 4507 |


**name-alt3-casei**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-alt3-casei` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sherlock\|Holmes\|Watson\|Irene\|Adler\|John\|Baker````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 4593 |


**name-alt4**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-alt4` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sher[a-z]+\|Hol[a-z]+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 3686 |


**name-alt4-casei**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-alt4-casei` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sher[a-z]+\|Hol[a-z]+````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 4254 |


**name-alt5**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-alt5` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sherlock\|Holmes\|Watson````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 4028 |


**name-alt5-casei**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/name-alt5-casei` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Sherlock\|Holmes\|Watson````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 4104 |


**no-match-uncommon**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/no-match-uncommon` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````zqj````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 0 |


**no-match-common**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/no-match-common` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````aqj````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 0 |


**no-match-really-common**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/no-match-really-common` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````aei````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 0 |


**the-lower**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/the-lower` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````the````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 21654 |


**the-upper**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/the-upper` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````The````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 2223 |


**the-casei**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/the-casei` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````the````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 23961 |


**everything-greedy**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/everything-greedy` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````.*````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`regress`) | 568829 |
| count(`.*`) | 581881 |


**everything-greedy-nl**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/everything-greedy-nl` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````(?s).*````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 594933 |


**letters**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/letters` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\pL````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 447175 |


**letters-upper**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/letters-upper` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\p{Lu}````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 14180 |


**letters-lower**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/letters-lower` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\p{Ll}````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 432995 |


**words**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/words` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 447639 |


**before-holmes**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/before-holmes` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+\s+Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 4073 |


**before-after-holmes**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/before-after-holmes` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+\s+Holmes\s+\w+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 2593 |


**holmes-cochar-watson**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/holmes-cochar-watson` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Holmes.{0,25}Watson\|Watson.{0,25}Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 150 |


**holmes-coword-watson**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/holmes-coword-watson` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````Holmes(?:\s*.+\s*){0,10}Watson\|Watson(?:\s*.+\s*){0,10}Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 14309 |


**quotes**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/quotes` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````["'][^"']{0,30}[?!.]["']````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 14437 |


**line-boundary-sherlock-holmes**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/line-boundary-sherlock-holmes` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````(?m)^Sherlock Holmes\|Sherlock Holmes$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 510 |


**word-ending-n**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/word-ending-n` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b\w+n\b````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 35297 |


**repeated-class-negation**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/repeated-class-negation` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[a-q][^u-z]{13}x````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 2130 |


**ing-suffix**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/ing-suffix` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[a-zA-Z]+ing````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 20547 |


**ing-suffix-limited-space**

| Parameter | Value |
| --------- | ----- |
| full name | `imported/sherlock/ing-suffix-limited-space` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\s[a-zA-Z]{0,12}ing\s````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 19658 |


</details>

### opt

#### accelerate

These benchmarks test an optimization only present in full DFAs where some
states get "accelerated." Namely, this happens when a state has only a few
transitions that leave the state and where all other transitions point back to
the same state. When this happens, fast vectorized routines like `memchr` can
be used to look for just the outgoing transitions because otherwise the search
will just stay in the same state.

This optimization only applies when full DFAs are used because it requires
knowing ahead of the time the full set of transitions for a state. Since things
like the lazy DFA compute their transitions lazily, this technique can't
(easily) be applied there. Since building full DFAs is extremely expensive, it
is only done for very small regexes. Therefore, this optimization is somewhat
limited in scope.

| Engine | whole-line | non-dna |
| - | - | - |
| pcre2/jit | 979.5 MB/s | **3.9 GB/s** |
| re2 | 482.7 MB/s | 323.2 MB/s |
| rust/regex/meta | **1262.1 MB/s** | 1844.4 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**whole-line**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/accelerate/whole-line` |
| model | [`grep`](MODELS.md#grep) |
| regex | `````(?m)^.*$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`.*`) | 239963 |

This just tries to match an entire line with line anchors. The DFA sees that
`.` can't match `\n` but otherwise will match everything. So the DFA will pass
`\n` to `memchr` to quickly find the end of the line.

**non-dna**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/accelerate/non-dna` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````>[^\n]*\n\|\n````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`imported/regex-redux-100000.fasta`](benchmarks/haystacks/imported/regex-redux-100000.fasta) |
| count(`.*`) | 16745 |

This regex was taken from the `regex-redux` benchmark. It is used at the
beginning of that benchmark to remove all line terminators and lines beginning
with `>`. The result is a flattened string of nucleotides.

The DFA is able to accelerate the `[^\n]*\n` construction by feeding `\n` and
`>` to `memchr2`. It does the same for the reverse DFA. Why `>`? Well, this is
an unanchored regex, so if the regex stumbles on a `>`, then that could start a
new match. So that transitions to some other state in the DFA.

</details>

#### backtrack

These benchmarks demonstrate the "bounded backtracking" optimization. This
optimization actually causes the regex crate, which is principally based
on finite automata, to use backtracking! It turns out the backtracking can
actually be quite fast in a lot of cases. Of course, its main problem is that
its worst case time complexity is exponential, and it can be difficult to
predict when exactly it will hit that worst case.

The regex crate mitigates the worst case by placing a bound on how much
backtracking can occur. Namely, it ensures that every byte in the haystack
is visited at most once by each NFA state. It does this by keeping track of
which combinations have been visited via a bitset. Thus, bounding backtracking
in this way requires `O(m*n)` space, where `m ~ len(regex)` and `n ~
len(haystack)`. Since either (or both) regex and haystack can get quite large,
the backtracking optimization is only used when `len(regex) * len(pattern)` is
below some heuristic limit.

This limit is in part why we use the `grep-captures` model. Namely, it ensures
the actual haystack that gets searched is quite small by capping it to the
length of a line, while still reflecting a real world use case.

We are also careful to:

1. Use a somewhat small regex, especially with Unicode enabled, or else it's
likely to go over the limits and thus disable the backtracking optimization.
2. Ensure the regex is not one-pass, or else we'll be measuring that
optimization instead of backtracking.
3. Benchmark the backtracking engine directly, to ensure it can be succesfully
used without hitting any limits. Otherwise, the meta regex engine will
transparently use another engine (probably the PikeVM).
4) In the Unicode Russian benchmark below, we use a Unicode-aware word boundary
which inhibits the use of a DFA and means the backtracking optimization is even
more critical.

Overall the backtracker usually edges out the PikeVM by about 2x or so. It's
not earth shattering, but it does help in a lot of cases.

| Engine | words-english | words-russian |
| - | - | - |
| go/regexp | 31.7 MB/s | - |
| pcre2/jit | **212.7 MB/s** | - |
| python/re | 27.4 MB/s | - |
| python/regex | - | 30.1 MB/s |
| re2 | 117.7 MB/s | - |
| regress | 82.4 MB/s | - |
| rust/regex | 159.0 MB/s | **45.7 MB/s** |
| rust/regex/meta | 201.0 MB/s | 45.6 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**words-english**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/backtrack/words-english` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````\b(?:(\w{6})\|(\w{5}))\b````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 40536 |

This just looks for words of length 6 or 5. Everything here is just ASCII
aware.

**words-russian**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/backtrack/words-russian` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````\b(?:([\w&&\p{Cyrillic}]{6})\|([\w&&\p{Cyrillic}]{5}))\b````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-huge.txt`](benchmarks/haystacks/opensubtitles/ru-huge.txt) |
| count(`.*`) | 27996 |

Like `words-english`, but makes everything Unicode aware. We also limit our
word characters to those that are also Cyrillic given that our haystack is
mostly written in a Cyrillic script.

Sadly very few regex engines support things like `[\w&&\p{Cyrillic}]`,
yet, that syntax is incredibly useful for situations like this. Namely,
`[\w&&\p{Cyrillic}]` is far smaller than `\w`, by about an order of magnitude.
And we just don't need the full `\w` here. Just the subset that is Cyrillic.

</details>

#### fixed-length

These benchmarks test "fixed length" optimizations. That is, for regexes with
no unbounded repetitions, one can compute the minimum and maximum lengths
that they can match. From those properties, one can sometimes reject a search
immediately based purely on the length of the haystack. The main complication
here is making sure things like a Unicode aware `.` contributes 4 to the
maximum since it could match up to 4 UTF-8 code units.

For minimums, things are relatively straight-forward. If your regex is
guaranteed to match at least 10 bytes and your haystack is less than that,
well, you can bail immediately.

Maximums are a little more tricky. Namely, if your regex is guaranteed to
match no more than 10 bytes and your haystack is longer than that, then you
can't necessarily bail because the regex could match anywhere. Thus, the
maximum optimization only kicks in when the regex is fully anchored at both
the start and end. In that case, the length of the haystack corresponds to
precisely how much the regex must match in order for a match to be reported
at all. And in that case, the maximum optimization kicks in.

| Engine | too-small-ascii | too-small-unicode | too-big-ascii | too-big-unicode | go33484-1 | go33484-2 | go33484-3 |
| - | - | - | - | - | - | - | - |
| go/regexp | 317.9 MB/s | - | 237.3 MB/s | - | 163.4 GB/s | 61.7 GB/s | 1543.2 MB/s |
| re2 | 133.1 MB/s | - | 679.3 MB/s | - | 216.6 GB/s | 227.2 GB/s | 3.1 GB/s |
| rust/regex/meta | **336.6 MB/s** | **2019.5 MB/s** | **2.7 GB/s** | **2.3 GB/s** | **517.4 GB/s** | **517.4 GB/s** | **51.7 GB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**too-small-ascii**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/fixed-length/too-small-ascii` |
| model | [`count`](MODELS.md#count) |
| regex | `````\w{10,}````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `abcdef` |
| count(`.*`) | 0 |

This benchmark tests that the regex engine fails very quickly when it knows
that a match is impossible because the haystack is too small. In this case,
the regex needs to match at least 10 word ASCII bytes, but the haystack is
shorter than that. You can see the difference between `rust/regex/meta` and
`rust/regex/hybrid` for example.

**too-small-unicode**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/fixed-length/too-small-unicode` |
| model | [`count`](MODELS.md#count) |
| regex | `````[\p{math}&&\u{10000}-\u{10FFFF}]{10,}````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack | `𝛃𝛃𝛃𝛃𝛃𝛃𝛃𝛃𝛃` |
| count(`.*`) | 0 |

Same idea as `too-small-ascii`, but for Unicode.

We specifically use a codepoint that uses 4 bytes per encoding to ensure that
the maximum length computation for a Unicode-aware class is correct. And in
particular, we construct the class such that it only matches codepoints that
are encoded with 4 bytes. So the 9 instances of 𝛃 make up 36 bytes, which is 4
fewer than the minimum of 40 computed by the regex.

Sadly, the class intersection syntax isn't well supported, so we only measure
the regex crate. We could probably use a simpler regex with some effort, but
¯\\\_(ツ)_/¯.

**too-big-ascii**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/fixed-length/too-big-ascii` |
| model | [`count`](MODELS.md#count) |
| regex | `````^\w{30}$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz` |
| count(`.*`) | 0 |

This checks that we bail early when we know the haystack is too big to match.

**too-big-unicode**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/fixed-length/too-big-unicode` |
| model | [`count`](MODELS.md#count) |
| regex | `````^\w{10}$````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack | `𝛃𝛃𝛃𝛃𝛃𝛃𝛃𝛃𝛃𝛃𝛃` |
| count(`.*`) | 0 |

Same idea as `too-big-ascii`, but for Unicode.

**go33484-1**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/fixed-length/go33484-1` |
| model | [`count`](MODELS.md#count) |
| regex | `````^a{2,5}$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa [.. snip ..]` |
| count(`.*`) | 0 |

This comes from: https://github.com/golang/go/issues/33484

**go33484-2**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/fixed-length/go33484-2` |
| model | [`count`](MODELS.md#count) |
| regex | `````^((aaa)\|(aa))$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa [.. snip ..]` |
| count(`.*`) | 0 |

This comes from: https://github.com/golang/go/issues/33484

**go33484-3**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/fixed-length/go33484-3` |
| model | [`count`](MODELS.md#count) |
| regex | `````^.{249}$````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack | `aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa [.. snip ..]` |
| count(`.*`) | 0 |

This comes from: https://github.com/golang/go/issues/33484

It was adapted slightly with a repetition to make the difference a bit more
noticeable.

</details>

#### literal-alt

These benchmarks demonstrate regex-automata's "literal alternation"
optimization. Namely, the optimization proceeds by tranforming a regex like
`zapper|z|zap` into `z(?:apper||ap)`. In effect, it pushes what is one very
large alternation down as far as it can. When matching the regex, especially
with an engine like the PikeVM, that initial large alternation can utterly
trash perf because the regex engine is shuffling states around for the one big
alternation for every byte of the haystack. But by pushing the branches down,
most bytes of the haystack are dealing with a much smaller branch and thus less
state shuffling.

This has an **enormous** impact on perf. And in the two benchmarks below, we
can observe the difference. In `one-pattern`, we join the dictionary into a
single pattern with `|`. But in the `pattern-per-word` benchmark, we treat
each word in the dictionary as its own pattern. The optimization described
above doesn't apply in the latter case. (It hasn't been attempted and I'm not
sure it's possible, since you need to preserve distinct match states for each
pattern.)

| Engine | one-pattern | pattern-per-word |
| - | - | - |
| go/regexp | 472.3 KB/s | - |
| re2 | 3.5 MB/s | - |
| rust/regex/meta | **694.1 MB/s** | **383.3 MB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**one-pattern**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/literal-alt/one-pattern` |
| model | [`count`](MODELS.md#count) |
| regex-path | [`dictionary/english/length-15.txt`](benchmarks/regexes/dictionary/english/length-15.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/en-tiny.txt`](benchmarks/haystacks/opensubtitles/en-tiny.txt) |
| count(`.*`) | 1 |


**pattern-per-word**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/literal-alt/pattern-per-word` |
| model | [`count`](MODELS.md#count) |
| regex-path | [`dictionary/english/length-15.txt`](benchmarks/regexes/dictionary/english/length-15.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/en-tiny.txt`](benchmarks/haystacks/opensubtitles/en-tiny.txt) |
| count(`.*`) | 1 |


</details>

#### onepass

These benchmarks test the "one-pass" DFA optimization. A one-pass NFA (before
being turned into a one-pass DFA) has the special property that at any point
during matching, there is at most one NFA state that can be transitioned to
from the current state. (Typically there may be many.) When this property
holds, the state shuffling done by the PikeVM to guarantee linear time can
be omitted entirely. We then capitalize on this absence of state shuffling
by turning the NFA into a DFA. It results in more memory usage but a much
faster search. And in particular, because of the one-pass property, there is at
most one DFA state for every NFA state. So there is no worst cast exponential
time/space complexity to worry about here that would otherwise apply to general
NFA-to-DFA conversion.

Note that RE2 calls this a "one-pass NFA" internally where as the regex crate
calls it a "one-pass DFA." Both are correct in some sense. One-pass NFA more
precisely refers to the formulation of the property, but I chose to call it a
DFA because its execution model more closely resembles that of a DFA. Namely,
every state transition is executed in a constant number of instructions, where
as this is generally not true for an NFA. (I'm being a bit hand-wavy here, bit
parallel NFAs would like to have a word with that generalization.)

The one-pass DFA occupies an important niche in the regex crate. Namely, it
has two critical powers that lazy and full DFAs do not have: it can match
Unicode word boundaries on all haystacks and it can resolve capture groups.
Normally, if we need to match Unicode word boundaries on non-ASCII text or
resolve capture groups, then we would otherwise be forced to resort to the
PikeVM or the bounded backtracker. Both of them are generally slower than the
one-pass DFA. Indeed, the benchmarks here include the PikeVM and the bounded
backtracker to demonstrate the difference.

Note that we benchmark the one-pass DFA explicitly to ensure that it
always works. If the regex in a benchmark changes or something about the
implementation changes to make the regex no longer one-pass, then the benchmark
will fail. In contrast, the regex crate API will not fail. It will just
silently not use the onepass DFA. We want the failure to surface and smack us
in the face.

| Engine | fn-predicate | first-three-words-english | first-three-words-russian | word-boundary-english | word-boundary-russian |
| - | - | - | - | - | - |
| go/regexp | 350.7 MB/s | 87.7 MB/s | - | - | - |
| pcre2/jit | **1280.4 MB/s** | **633.2 MB/s** | **804.1 MB/s** | **1158.7 MB/s** | **1332.9 MB/s** |
| python/re | 76.7 MB/s | 43.1 MB/s | - | - | - |
| python/regex | - | - | 78.9 MB/s | 68.4 MB/s | 135.4 MB/s |
| re2 | 732.8 MB/s | 453.4 MB/s | - | - | - |
| regress | 172.6 MB/s | 142.3 MB/s | - | - | - |
| rust/regex/meta | 1046.4 MB/s | 513.1 MB/s | 673.4 MB/s | 840.7 MB/s | 1081.3 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**fn-predicate**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/onepass/fn-predicate` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````^\s*fn\s+(is_([^\(]+))\(([^)]+)\) -> bool \{$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`.*`) | 916 |

This benchmark just tries to extract some properties of a subset of functions
from a pile of Rust source code. We use capture groups to get the function name
and parameters.

**first-three-words-english**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/onepass/first-three-words-english` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````^ *(\w+) +(\w+) +(\w+)````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 35128 |

This benchmark looks for the first three "words" in each line, and captures
each word. This is a bit of a higher match count than `fn-predicate`, so its
throughput is overall lower.

**first-three-words-russian**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/onepass/first-three-words-russian` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````^ *(\w+) +(\w+) +(\w+)````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-huge.txt`](benchmarks/haystacks/opensubtitles/ru-huge.txt) |
| count(`.*`) | 19224 |

This is the same as `first-three-words-english`, but enables Unicode and runs
it on a Russian corpus. Engines like the PikeVM tend to slow down here because
the `\w` class is much bigger and requires more state shuffling.

**word-boundary-english**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/onepass/word-boundary-english` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````^(\S{8})(\S)\b````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 579 |

This benchmark uses an ASCII word boundary to find all 9 letter words that
occur at the beginning of a line, while capturing the first 8 letters and last
letter separately. Mostly this is meant to serve as an interesting comparison
point to to `word-boundary-russian`, which is Unicode aware.

**word-boundary-russian**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/onepass/word-boundary-russian` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````^(\S{8})(\S)\b````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-huge.txt`](benchmarks/haystacks/opensubtitles/ru-huge.txt) |
| count(`.*`) | 873 |

This is the same as the English counterpart, but enables Unicode mode and thus
uses Unicode-aware word boundaries. This demonstrates that the one-pass DFA
works with Unicode word boundaries on non-ASCII text. (It is the only DFA in
the regex crate that can do so.)

</details>

#### prefilter

These benchmarks demonstrate the prefix prefilter optimization. This is
the most "basic" of literal optimizations in that it looks for prefixes
that occur in every match of a regex, and then looks for occurrences of
those prefixes in the haystack. Occurrences are considered candidate matches
and must be confirmed by the regex engine. Upon failure, the prefilter is
usually restarted to look for more candidates. In general, this results is
significant (sometimes by orders of magnitude) speed improvements in many
cases. It can sometimes lead to slower searches if the false positive rate
of candidate matches is high. (For those, see the `folly` benchmarks in the
parent directory.)

Despite the seeming simplicity of this optimization, regex engines vary
in their sophistication. Some just look for a single byte prefix and use
memchr in that case, and then otherwise don't do much. The regex crate tries
quite hard to extract prefixes and has a couple different SIMD algorithms it
can employ, including for handling cases where there are multiple distinct
prefixes.

The trick is generally knowing when to stop. For example, you could generate
52 different prefixes all ending with `foo` for `[A-Za-z]foo\w+`, but that's
unlikely to be effective because as you increase the number of prefixes you
look for, you 1) generally do so at a slower speed and 2) risk the chance of
increasing your false positive rate.

Still... You don't necessarily want to find the fewest prefixes either. For
example, given something like `foo[a-c]`, you might actually want to look for
`{fooa, foob, fooc}`, as that set is still fairly small and might lead to a
lower false positive rate than just `foo`.

And still yet, you also want to generally prefer longer prefixes if you can.
For example, given `Sherlock|Holmes`, you might be tempted to just look for
occurrences of `S` and `H`. But if they are frequent in your haystack, that
might lead to a substantially higher false positive rate than just looking
for `{Sherlock, Holmes}`.

In other words, this is basically a dark art and is really just a whole pile
of heuristics that seem to work well empirically.

| Engine | literal-english | literal-casei-english | literal-russian | literal-casei-russian | rust-functions |
| - | - | - | - | - | - |
| go/regexp | 29.1 GB/s | 44.2 MB/s | 3.1 GB/s | 32.8 MB/s | 2.6 GB/s |
| pcre2/jit | 35.2 GB/s | **23.0 GB/s** | **41.5 GB/s** | **25.7 GB/s** | **22.8 GB/s** |
| python/re | 4.3 GB/s | 295.4 MB/s | 8.4 GB/s | 460.6 MB/s | - |
| python/regex | 4.7 GB/s | 4.0 GB/s | 6.1 GB/s | 5.3 GB/s | 2.4 GB/s |
| re2 | **48.1 GB/s** | 3.0 GB/s | 806.6 MB/s | 988.4 MB/s | 5.2 GB/s |
| regress | 3.6 GB/s | 1261.3 MB/s | 3.6 GB/s | 323.2 MB/s | 2.3 GB/s |
| rust/regex/meta | 41.5 GB/s | 17.1 GB/s | 40.2 GB/s | 12.3 GB/s | 14.9 GB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**literal-english**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/prefilter/literal-english` |
| model | [`count`](MODELS.md#count) |
| regex | `````Sherlock Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 1 |


**literal-casei-english**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/prefilter/literal-casei-english` |
| model | [`count`](MODELS.md#count) |
| regex | `````Sherlock Holmes````` |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 1 |


**literal-russian**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/prefilter/literal-russian` |
| model | [`count`](MODELS.md#count) |
| regex | `````Шерлок Холмс````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-huge.txt`](benchmarks/haystacks/opensubtitles/ru-huge.txt) |
| count(`.*`) | 1 |

This measures a non-ASCII literal on a non-ASCII haystack. Regex engines that
naively just feed the first byte to memchr get wrecked on this benchmark by a
high false positive rate, because they wind up using a UTF-8 leading byte that
occurs in almost every codepoint in the haystack.

**literal-casei-russian**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/prefilter/literal-casei-russian` |
| model | [`count`](MODELS.md#count) |
| regex | `````Шерлок Холмс````` |
| case-insensitive | `true` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-huge.txt`](benchmarks/haystacks/opensubtitles/ru-huge.txt) |
| count(`.*`) | 1 |


**rust-functions**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/prefilter/rust-functions` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex | `````fn is_(\w+)\|fn as_(\w+)````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`.*`) | 948 |

This is a mostly silly little benchmark that looks for a couple different
functions. This mostly checks whether regex engines do a literal scan for both
`fn is_` and `fn as_`, or just `fn `. (One doesn't seem obviously better than
the other. The former will have a lower false positive rate, but the latter is
likely to be much quicker at identifying candidates, depending on algorithm
choice.)

</details>

#### reverse-anchored

This benchmark demonstrates that if a regex is anchored at the end, then
regex-automata's meta engine will do a reverse search starting at the end to
look for a match. In contrast, lower level regex engines don't know about this
optimization and just naively scan starting at the beginning.

| Engine | word-end |
| - | - |
| go/regexp | 78.4 MB/s |
| pcre2/jit | 1652.5 MB/s |
| re2 | 12417.6 GB/s |
| rust/regex/meta | **34493.5 GB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**word-end**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/reverse-anchored/word-end` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `************************************************************ [.. snip ..]` |
| count(`.*`) | 1 |


</details>

#### reverse-inner

These benchmarks test the "reverse inner" literal optimization. This is very
much like the "reverse suffix" literal optimization, except it works on
literals that are neither prefixes or suffixes. It works by looking for a
literal that *must* appear in all matches and then splitting the regex into
pieces: a prefix regex, followed by the literal and then a suffix regex.

Candidate matches are found by searching for the literal, then doing a reverse
search from the start of the literal using the prefix regex and then finally a
full forward scan to find the end of the match.

This is quite a precarious optimization since quadratic behavior can occur
just like in the reverse suffix optimization. However, the quadratic behavior
can be more subtle because of the final forward scan to confirm the match.
Namely, when a match is not found, you need to know how far that scan looked in
the haystack to ensure you don't scan that region of the haystack again in a
subsequent reverse scan.

We test both forms of quadratic behavior below. In both cases, the regex engine
detects it and bails out of the optimization.

| Engine | holmes | email | factored-prefix | no-quadratic-backward | no-quadratic-forward |
| - | - | - | - | - | - |
| go/regexp | 33.8 MB/s | 46.7 MB/s | 16.6 MB/s | 137.8 MB/s | 44.7 MB/s |
| pcre2/jit | 375.7 MB/s | 640.2 MB/s | 170.4 MB/s | **4.8 GB/s** | **1888.5 MB/s** |
| re2 | 966.8 MB/s | 983.6 MB/s | 925.4 MB/s | 987.0 MB/s | 987.0 MB/s |
| rust/regex/meta | **19.8 GB/s** | **82.2 GB/s** | **9.8 GB/s** | 823.0 MB/s | 404.3 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**holmes**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/reverse-inner/holmes` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+\s+Holmes\s+\w+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 2593 |

A regex that looks for any words preceding and following `Holmes`. The reverse
inner literal scan is able to very quickly look for matches of `Holmes`, then
do a reverse scan for `\w+\s+` to find the start of the match, and finally a
foward scan for `\s+\w+` to find the end of the match.

**email**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/reverse-inner/email` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+@\w+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`.*`) | 47 |

A very simple regex for matching things that might be email addresses. The
"reverse inner" optimization shines here because `@` is pretty rare and there
is no overlap between `\w` and `@`, so quadratic behavior is impossible.
In this particularly ideal case, the regex crate absolutely screams. On my
system, even though the hybrid engine is at a respectable ~800 MB/s throughput,
this optimization bumps it up to ~80 GB/s, an improvement of two orders of
magnitude.

**factored-prefix**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/reverse-inner/factored-prefix` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\pL+herloc\pL+\|\pL+olme\pL+````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 3542 |

This benchmark also uses the reverse inner optimization, and does despite
there not *appearing* to be a top-level concatenation. Namely, at the time of
writing, the reverse inner literal detection is not supremely dumb, but also
not particularly smart either. It only looks for a top-level concatenation, and
in that concatenation, looks for required literals. But if the regex just has
an alternation at the top, it bails out.

The trick here is that the HIR smart constructors in the `regex-syntax` crate
will notice that each branch of the alternation has a common prefix and factor
it out. So the regex gets rewritten as `\pL+(?:herloc\pL+|olme\p+)`. (Common
suffix factoring doesn't exist at the time of writing.) This transforms
the regex from one with a top-level alternation to one with a top-level
concatenation, and thus amenable to the not-so-smart reverse inner literal
extraction.

**no-quadratic-backward**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/reverse-inner/no-quadratic-backward` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[A-Z].*bcdefghijklmnopq[a-z]+````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `bcdefghijklmnopqbcdefghijklmnopqbcdefghijklmnopqbcdefghijklm [.. snip ..]` |
| count(`.*`) | 0 |

This benchmark checks that the reverse inner optimization does not result
in quadratic behavior within a single search. This specifically looks for
quadratic behavior that occurs from the reverse search after finding a literal
candidate backs up too far (past the previous literal candidate).

The mitigation technique is to detect the quadratic behavior and bail out of
the reverse inner optimization and use a normal forward search instead.

See also the `opt/reverse-suffix/no-quadratic` benchmark for a bit more
explanation. The optimization is a little different but the mechanism for
quadratic behavior is very similar.

**no-quadratic-forward**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/reverse-inner/no-quadratic-forward` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````.efghijklmnopq[a-z]+[A-Z]````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `bcdefghijklmnopqbcdefghijklmnopqbcdefghijklmnopqbcdefghijklm [.. snip ..]` |
| count(`.*`) | 0 |

This benchmark also checks that quadratic behavior does not occur. Unlike
the backward case above, this checks that a *second* way quadratic behavior
*could* occur. Namely, in this case, the problem isn't the reverse scan,
but the forward scan that occurs following the reverse scan. (Remember, the
opptimization works by finding literal candidates, then a reverse scan to find
the start position and finally a forward scan to find the end position. This
benchmark is about that final forward scan.)

Namely, it is possible that the forward scan looks ahead to the end of the
haystack but then doesn't find a match. And indeed, that's exactly how we craft
our benchmark here. Without a mitigation in place, this would find many literal
candidates that also match the reverse scan, but then don't match the forward
scan only after scanning to the end of the haystack. This then repeats itself
and you wind up with quadratic behavior.

The way we prevent this is to track how far the forward scan gets to when it
fails. Then, after we find our next literal candidate, we instruct the reverse
scan to avoid scanning past the previous position at which the forward scan
fails. If it does reach that position, then we know we've potentially triggered
quadratic behavior.

</details>

#### reverse-suffix

These benchmarks test the "reverse suffix" optimization. This works by looking
for a suffix literal that must occur at the end of every match, searching for
that literal and then doing a reverse DFA scan to confirm any candidates that
were found from the literal scan.

The main downside of this approach is that it opens you up to quadratic
behavior. The `no-quadratic` benchmark below demonstrates that we mitigate
against that.

| Engine | holmes | no-quadratic |
| - | - | - |
| go/regexp | 34.1 MB/s | 138.1 MB/s |
| pcre2/jit | 373.3 MB/s | **4.8 GB/s** |
| re2 | 946.0 MB/s | 987.0 MB/s |
| rust/regex/meta | **18.2 GB/s** | 821.2 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**holmes**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/reverse-suffix/holmes` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\w+\s+Holmes````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`sherlock.txt`](benchmarks/haystacks/sherlock.txt) |
| count(`.*`) | 4073 |

A regex that looks for any words preceding `Holmes`. The reverse suffix
literal scan is able to very quickly look for matches of `Holmes`, then do a
reverse scan for `\w+\s+` to find the start of the match.

**no-quadratic**

| Parameter | Value |
| --------- | ----- |
| full name | `opt/reverse-suffix/no-quadratic` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````[A-Z].*bcdefghijklmnopq````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `bcdefghijklmnopqbcdefghijklmnopqbcdefghijklmnopqbcdefghijklm [.. snip ..]` |
| count(`.*`) | 0 |

This benchmark checks that the reverse suffix optimization does not result in
quadratic behavior within a single search. Namely, this benchmark is designed
such that the suffix literal matches many times, and at each point, the reverse
scan to confirm that match will continue all the way back to the beginning of
the haystack. Without some kind of mitigation for this, it would result in
quadratic behavior in the size of the haystack.

There is no real way to get a point of reference for this. The only way to see
the difference is to remove the mitigation within the meta regex engine and
re-run the benchmark. Indeed, it shows a perf reduction by about two orders of
magnitude.

Currently, the mitigation is that the reverse search will stop itself before it
reaches any point in the haystack that has already been scanned. In that case,
the optimization will fail and a normal forward search will be used.

Another possible mitigation is to analyze the regex to check that there is no
way for the literal search to overlap with the reverse scan to confirm the
match. Indeed, this regex would fail that test since `.*` will happily match
the literal `bcdefghi...`. The problem with that approach in general is that
even if there is overlap, that doesn't mean a search will result in quadratic
behavior. There isn't too much downside to just trying and bailing out when we
detect it, other than code complexity. But the upside is that the optimization
will apply even when there is overlap.

</details>

### reported

#### i787-keywords

These benchmarks came from: https://github.com/rust-lang/regex/issues/787

This regex is a common sort of occurrence where someone puts a bunch of
literals in an alternation and wraps them in word boundary assertions. The main
issue for the regex crate is the Unicode case (which is the default). Namely,
Unicode word boundaries can't be handled by the DFAs on non-ASCII haystacks,
so perf can get trashed because the regex crate has to fall back to a slower
engine (like the PikeVM).

In the old regex crate, this was made doubly worse because the PikeVM isn't
great at dealing with large alternations. But in the rewrite, literal
alternations are rewritten in a more optimized way that pushes the branching
down more. (See the `opt/literal-alt` benchmarks for more on that.)

Another problem in the old regex crate is that it had trouble with prefilters
in regexes surrounded by look-around assertions, primarily because its internal
engine APIs were not principled about separating "search the haystack" and
"search the haystack at this range while taking the surrounding context into
account in order to correctly compute look-around assertions."

So in the regex crate rewrite, these things were all fixed. And now both
cases will use Teddy because of the extracted prefilter... But... Ug. Sadly,
Teddy doesn't do so hot on this particular benchmark. Profiling suggests it
gets stuck in its verification step, which is quite unfortunate, because an
Aho-Corasick DFA (or even lazy DFA) do much better here.

Unless we can fix Teddy, the ideal scenario here is that we just use the lazy
DFA for the ASCII case and an Aho-Corasick DFA prefilter for the Unicode case.
Why the difference? Because without the prefilter for the Unicode case, the
lazy DFA will quit once it sees a non-ASCII byte in the haystack, and there are
a number of them here. The prefilter avoids that quitting problem and instead
just uses lazy DFA for confirming the match. (Which could still quit, but it's
far less likely to because the alternation is just a bunch of ASCII words.)
Unfortunately, it seems tricky to figure out a good heuristic that selects our
ideal scenario.

The Teddy engine may be difficult to fix in this particular case because the
match count is pretty high.

| Engine | compile | ascii | unicode | opt-ascii | opt-unicode |
| - | - | - | - | - | - |
| go/regexp | 141.88us | 925.0 KB/s | - | 14.1 MB/s | - |
| hyperscan | 9.41ms | 414.4 MB/s | - | 370.5 MB/s | - |
| pcre2/jit | 63.47us | 19.5 MB/s | 10.6 MB/s | 283.9 MB/s | 213.2 MB/s |
| python/re | 657.06us | 3.2 MB/s | - | 72.9 MB/s | - |
| python/regex | 2.28ms | 22.5 MB/s | 22.6 MB/s | 40.6 MB/s | 40.2 MB/s |
| re2 | 74.89us | **458.3 MB/s** | - | **460.6 MB/s** | - |
| regress | **45.12us** | 4.1 MB/s | - | 52.4 MB/s | - |
| rust/regex/meta | 339.58us | 177.3 MB/s | **176.8 MB/s** | 356.2 MB/s | **359.2 MB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**compile**

| Parameter | Value |
| --------- | ----- |
| full name | `reported/i787-keywords/compile` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`reported/i787-keywords.txt`](benchmarks/regexes/reported/i787-keywords.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `f64` |
| count(`.*`) | 1 |


**ascii**

| Parameter | Value |
| --------- | ----- |
| full name | `reported/i787-keywords/ascii` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex-path | [`reported/i787-keywords.txt`](benchmarks/regexes/reported/i787-keywords.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`bstr-ext-slice-65993b58.txt`](benchmarks/haystacks/bstr-ext-slice-65993b58.txt) |
| count(`.*`) | 5674 |


**unicode**

| Parameter | Value |
| --------- | ----- |
| full name | `reported/i787-keywords/unicode` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex-path | [`reported/i787-keywords.txt`](benchmarks/regexes/reported/i787-keywords.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`bstr-ext-slice-65993b58.txt`](benchmarks/haystacks/bstr-ext-slice-65993b58.txt) |
| count(`.*`) | 5674 |


**opt-ascii**

| Parameter | Value |
| --------- | ----- |
| full name | `reported/i787-keywords/opt-ascii` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b(Self\|a(?:bstract\|s)\|b(?:ecome\|o(?:ol\|x)\|reak)\|c(?:har\|on(?:st\|tinue)\|rate)\|do\|e(?:lse\|num\|xtern)\|f(?:32\|64\|alse\|inal\|n\|or)\|i(?:1(?:28\|6)\|32\|64\|mpl\|size\|[8fn])\|l(?:et\|oop)\|m(?:a(?:cro\|tch)\|o(?:d\|ve)\|ut)\|override\|p(?:riv\|ub)\|re(?:f\|turn)\|s(?:elf\|t(?:atic\|r(?:(?:uct)?))\|uper)\|t(?:r(?:ait\|ue\|y)\|ype(?:(?:of)?))\|u(?:1(?:28\|6)\|32\|64\|8\|ns(?:afe\|ized)\|s(?:(?:(?:iz)?)e))\|virtual\|wh(?:(?:er\|il)e)\|yield)\b````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`bstr-ext-slice-65993b58.txt`](benchmarks/haystacks/bstr-ext-slice-65993b58.txt) |
| count(`.*`) | 5674 |

This is the "optimized" variant of the regex as posted in the GitHub issue
linked above. I believe this isn't quite a correct transformation, since the
program that produced this variant gets rid of preference order. Nevertheless,
it's interesting to benchmark this and compare it to others. (Note that the
regex crate does its own internal rewriting of literal alternations like this,
but in a way that preserves preference order.)

**opt-unicode**

| Parameter | Value |
| --------- | ----- |
| full name | `reported/i787-keywords/opt-unicode` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b(Self\|a(?:bstract\|s)\|b(?:ecome\|o(?:ol\|x)\|reak)\|c(?:har\|on(?:st\|tinue)\|rate)\|do\|e(?:lse\|num\|xtern)\|f(?:32\|64\|alse\|inal\|n\|or)\|i(?:1(?:28\|6)\|32\|64\|mpl\|size\|[8fn])\|l(?:et\|oop)\|m(?:a(?:cro\|tch)\|o(?:d\|ve)\|ut)\|override\|p(?:riv\|ub)\|re(?:f\|turn)\|s(?:elf\|t(?:atic\|r(?:(?:uct)?))\|uper)\|t(?:r(?:ait\|ue\|y)\|ype(?:(?:of)?))\|u(?:1(?:28\|6)\|32\|64\|8\|ns(?:afe\|ized)\|s(?:(?:(?:iz)?)e))\|virtual\|wh(?:(?:er\|il)e)\|yield)\b````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`bstr-ext-slice-65993b58.txt`](benchmarks/haystacks/bstr-ext-slice-65993b58.txt) |
| count(`.*`) | 5674 |

Same as `opt-ascii`, but with Unicode mode enabled.

</details>

### slow

These benchmarks test regexes that are known to be slow for at least some regex
engines. Usually they are somewhat pathological, but not always.

| Engine | quadratic |
| - | - |
| go/regexp | 670 B/s |
| re2 | 263 B/s |
| rust/regex/meta | **848.3 KB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**quadratic**

| Parameter | Value |
| --------- | ----- |
| full name | `slow/quadratic` |
| model | [`count`](MODELS.md#count) |
| regex | `````(?:A+){1000}\|````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA [.. snip ..]` |
| count(`.*`) | 1000 |

This one shows a particularly gnarly case where each search scans to the end
of the haystack only to discover that there is no match for the first branch.
It then reports the last match it did find, which was all the way back at the
beginning of the haystack, corresponding to the empty regex on the right hand
side of the alternation. Since we find all matches, the search then restarts
immediately after the first match, only to scan all the way to the end of the
haystack again.

Each individual search is linear in the length of the haystack, but the overall
iteration for every match is actually quadratic. Unfortunate, but unavoidable.

Adapted from: https://github.com/golang/go/issues/11181

</details>

### unicode

#### codepoints

These benchmarks are meant to demonstrate just how difficult Unicode can be,
and how easy it is for Unicode to make your regexes jump off of a perf cliff.
These benchmarks demonstrate why you should disable Unicode in the regex crate
unless you need it. (One wonders whether it should have been disabled by
default, but that ship has sailed and I don't think it's ever coming back.)

| Engine | any-one | any-all | letters-one | letters-alt | letters-lower-or-upper | contiguous-greek |
| - | - | - | - | - | - | - |
| go/regexp | 25.8 MB/s | 164.4 MB/s | 34.5 MB/s | - | - | 213.5 MB/s |
| hyperscan | **682.9 MB/s** | - | - | - | - | - |
| pcre2 | - | - | - | - | - | 53.5 MB/s |
| pcre2/jit | 310.7 MB/s | 2.3 GB/s | 1002.3 MB/s | - | 570.2 MB/s | 1032.0 MB/s |
| python/regex | 52.1 MB/s | **4.1 GB/s** | **1068.9 MB/s** | **182.9 MB/s** | 571.0 MB/s | **3.2 GB/s** |
| re2 | 63.5 MB/s | 491.1 MB/s | 3.1 MB/s | - | - | 981.1 MB/s |
| regress | - | - | - | - | - | 446.1 MB/s |
| rust/regex/meta | 356.3 MB/s | 399.2 MB/s | 13.7 MB/s | 13.7 MB/s | **821.1 MB/s** | 826.0 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**any-one**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/codepoints/any-one` |
| model | [`count`](MODELS.md#count) |
| regex | `````(?s:.)````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`unicode/allcodepoints.txt`](benchmarks/haystacks/unicode/allcodepoints.txt) |
| count(`.*`) | 1112064 |

A simple benchmark that matches each individual codepoint.

**any-all**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/codepoints/any-all` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````(?s:.)+````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`unicode/allcodepoints.txt`](benchmarks/haystacks/unicode/allcodepoints.txt) |
| count(`.*`) | 4382592 |

Same as 'any-one', but matches all the codepoints in one go.

**letters-one**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/codepoints/letters-one` |
| model | [`count`](MODELS.md#count) |
| regex | `````\p{L}{100}````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`unicode/allcodepoints.txt`](benchmarks/haystacks/unicode/allcodepoints.txt) |
| count(`go/regexp`) | 1201 |
| count(`pcre2/jit`) | 1203 |
| count(`.*`) | 1244 |

This benchmarks finding 100 contiguous letters in a sequence of all codepoints.
This basically takes a huge character class and then repeats it 100 times over.
The regex for this---at least at the automata level---is absolutely huge.
This wreaks all kind of havoc. It prevents a lazy DFA from being used because
it requires a cache that is beyond the default capacity. It's way too big
for a one-pass DFA and also the backtracker. So the PikeVM is stuck with the
unenviable task of dealing with it.

The bounded repeat really slows the PikeVM way down. Because at each position
in the haystack, the PikeVM has to keep track of how many letters have matched.
And huge Unicode classes are very branchy, which just overall leads to a lot of
state shuffling.

`python/regex` does *really* well on this benchmark. It likely has a specific
optimization in place for bounded repeats.

**letters-alt**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/codepoints/letters-alt` |
| model | [`count`](MODELS.md#count) |
| regex | `````(?x)(?:
  \p{Lowercase}
  \|\p{Uppercase}
  \|\p{Titlecase_Letter}
  \|\p{Modifier_Letter}
  \|\p{Other_Letter}
){100}````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`unicode/allcodepoints.txt`](benchmarks/haystacks/unicode/allcodepoints.txt) |
| count(`.*`) | 1244 |

This benchmark looks for exactly the same thing as `letters-one`, except it
uses the constituent Unicode properties that make up `\p{Letter}`.

The regex crate will actually see the alternation of classes at the syntax
level and join them into a single class.

`python/regex` is quite interesting here because it does about an order of
magnitude worse on this benchmark than in `letters-one`. (Although still
quite a bit better than the regex crate.) It seems likely that it has some
kind of optimization for bounded repeats that kicks in, but the alternation
slows it down because it is, after, a backtracker.

**letters-lower-or-upper**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/codepoints/letters-lower-or-upper` |
| model | [`count`](MODELS.md#count) |
| regex | `````(?:\p{Lowercase}\|\p{Uppercase}){100}````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`unicode/allcodepoints.txt`](benchmarks/haystacks/unicode/allcodepoints.txt) |
| count(`.*`) | 13 |

This is yet another variant on the "letters" idea, but we only select a subset
here: uppercase and lowercase. This does substantially reduce the match
count, but that doesn't matter so much for perf. (Change `\pL{100}` above to
`\pL{100}x` and perf generally remains the same for the regex crate.)

But! This does result in the regex becoming substantially smaller. Small
enough for the lazy DFA's default cache size to handle it. This makes it nice
and fast. You can get a similar speed on `letters-alt` if you increase the lazy
DFA's cache capacity.

**contiguous-greek**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/codepoints/contiguous-greek` |
| model | [`count`](MODELS.md#count) |
| regex | `````\p{Greek}+````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`unicode/allcodepoints.txt`](benchmarks/haystacks/unicode/allcodepoints.txt) |
| count(`pcre2`) | 38 |
| count(`pcre2/jit`) | 38 |
| count(`.*`) | 36 |


</details>

#### compile

These benchmarks demonstrate the impact that Unicode has on the time it takes
to compile regexes. Generally speaking, Unicode impacts the compile times of
automata engines the most, since (most, but not all) automata engines compile
regexes down to machines whose transitions are defined over bytes. This takes
quite a bit of work to do when faced with large Unicode character classes.

| Engine | one-letter | fifty-letters | fifty-letters-ascii | match-every-line | match-every-line-ascii | negated-class-matches-codepoint |
| - | - | - | - | - | - | - |
| go/regexp | 16.41us | 14.80us | 4.98us | **785.00ns** | **788.00ns** | 419.00ns |
| hyperscan | 24.10ms | - | 465.86us | - | 128.96us | 312.54us |
| pcre2 | - | - | - | - | - | **176.00ns** |
| pcre2/jit | **2.44us** | **2.52us** | **2.14us** | 3.15us | 2.32us | 2.20us |
| python/re | - | - | - | - | - | 6.45us |
| python/regex | 20.91us | 37.25us | 63.65us | - | 41.76us | 23.79us |
| re2 | 429.78us | 11.66ms | 7.40us | 2.48us | 1.77us | 1.87us |
| regress | - | - | - | - | - | 768.00ns |
| rust/regex/meta | 130.91us | 6.24ms | 11.11us | 52.76us | 13.59us | 36.59us |

<details>
<summary>Show individual benchmark parameters.</summary>

**one-letter**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/compile/one-letter` |
| model | [`compile`](MODELS.md#compile) |
| regex | `````\pL````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack | `ͱͳͷΐάέήίΰαβγδεζηθικλμνξοπρςστυ [.. snip ..]` |
| count(`.*`) | 53 |

Measures how long it takes to compile one particularly large Unicode class.

**fifty-letters**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/compile/fifty-letters` |
| model | [`compile`](MODELS.md#compile) |
| regex | `````\pL{50}````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack | `ͱͳͷΐάέήίΰαβγδεζηθικλμνξοπρςστυ [.. snip ..]` |
| count(`.*`) | 1 |

Like `one-letter`, but repeats that class 50 times. Backtrackers in
particular do well here and generally aren't any slower than the `one-letter`
case, where as byte-oriented automata engines do quite a bit worse. This is
because they compile classes like `\pL` down into machines with transitions
defined over bytes and *not* codepoints. So there is a lot of work that is
done here.

I don't think any of the automata engines "cache" the work done to compile
`\pL` once and then reuse it for subsequent `\pL`s.

Note that `go/regexp` does quite well here despite using automata internally.
This is likely because it defines its transitions over codepoints, and so never
compiles things like `\pL` down to bytes at all. (`go/regexp` also lacks a DFA,
which lessens the need for byte oriented transitions.)

**fifty-letters-ascii**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/compile/fifty-letters-ascii` |
| model | [`compile`](MODELS.md#compile) |
| regex | `````[a-zA-Z]{50}````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz` |
| count(`hyperscan`) | 3 |
| count(`.*`) | 1 |

This just provides a baseline comparison point with `\pL{50}` to demonstate
that Unicode is indeed the culprit here.

**match-every-line**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/compile/match-every-line` |
| model | [`compile`](MODELS.md#compile) |
| regex | `````(?m)^.+$````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`lines-with-invalid-utf8.txt`](benchmarks/haystacks/lines-with-invalid-utf8.txt) |
| count(`go/regexp`) | 4 |
| count(`.*`) | 3 |

This compiles a simple regex that matches non-empty lines that are valid UTF-8.
The haystack we use contains 4 lines, but one of them contains invalid UTF-8.

**match-every-line-ascii**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/compile/match-every-line-ascii` |
| model | [`compile`](MODELS.md#compile) |
| regex | `````(?m)^.+$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`lines-with-invalid-utf8.txt`](benchmarks/haystacks/lines-with-invalid-utf8.txt) |
| count(`.*`) | 4 |

This is like `match-every-line`, but disables Unicode mode and permits `.`
to match any byte (except for `\n`). So this includes the line that contains
invalid UTF-8 in the count.

**negated-class-matches-codepoint**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/compile/negated-class-matches-codepoint` |
| model | [`compile`](MODELS.md#compile) |
| regex | `````[^a]````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack | `☃` |
| count(`.*`) | 1 |

This tests that `[^a]` in Unicode mode matches an entire codepoint and not
the individual bytes.

</details>

#### overlapping-words

These benchmarks test a few different variants of "overlapping words." That is,
we look for sequences of word characters of some length, and if that fails, we
fall back to a smaller sequence and so on. We in particular use this as a way
of contrasting how much Unicode hurts something like this. Namely the bounded
repeats greatly inflate the size of the regex and make matching quite a bit
harder, especially for automata oriented engines.

The difference between the `ascii` and Unicode aware benchmarks can be quite
brutal for automata engines. It can go from a bearable 50 MB/s for ASCII to
an eye watering 1 MB/s for Unicode. The main problem is that `\pL` is just
absolutely huge, and there is a lot of overhead associated with plowing through
the NFA states for it when resolving capture groups.

| Engine | ascii | english | russian |
| - | - | - | - |
| go/regexp | 6.5 MB/s | 3.5 MB/s | 8.1 MB/s |
| pcre2/jit | **97.7 MB/s** | **48.8 MB/s** | **73.9 MB/s** |
| python/regex | 4.2 MB/s | 5.1 MB/s | 11.4 MB/s |
| re2 | 50.5 MB/s | 941.7 KB/s | 781.4 KB/s |
| regress | 26.0 MB/s | - | - |
| rust/regex | 47.6 MB/s | 45.4 MB/s | 18.6 MB/s |
| rust/regex/meta | 69.6 MB/s | 6.4 MB/s | 5.9 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**ascii**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/overlapping-words/ascii` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````([A-Za-z]{14})\|([A-Za-z]{13})\|([A-Za-z]{12})\|([A-Za-z]{11})\|([A-Za-z]{10})\|([A-Za-z]{9})\|([A-Za-z]{8})\|([A-Za-z]{7})\|([A-Za-z]{6})\|([A-Za-z]{5})````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-medium.txt`](benchmarks/haystacks/opensubtitles/en-medium.txt) |
| count(`.*`) | 6156 |


**english**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/overlapping-words/english` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````(\pL{14})\|(\pL{13})\|(\pL{12})\|(\pL{11})\|(\pL{10})\|(\pL{9})\|(\pL{8})\|(\pL{7})\|(\pL{6})\|(\pL{5})````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/en-medium.txt`](benchmarks/haystacks/opensubtitles/en-medium.txt) |
| count(`.*`) | 6156 |


**russian**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/overlapping-words/russian` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````(\pL{14})\|(\pL{13})\|(\pL{12})\|(\pL{11})\|(\pL{10})\|(\pL{9})\|(\pL{8})\|(\pL{7})\|(\pL{6})\|(\pL{5})````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-medium.txt`](benchmarks/haystacks/opensubtitles/ru-medium.txt) |
| count(`.*`) | 5710 |


</details>

#### word

These benchmarks try to demonstrate some differences between ASCII-only words
and word boundaries, and Unicode-aware words and word boundaries.

The `boundary-any-*` benchmarks are somewhat dominated by latency. That is, the
faster your regex engine can get out of the way of the core matching logic,
the better. The `boundary-long-*` benchmarks are more dominated by throughput
because it matches far less frequently. As one example, `regress` has pretty
similar performance on both, but the regex crate does a lot better on the
throughput oriented benchmark.

But also, the regex crate in particular can fall off of a perf cliff when using
Unicode word boundaries. The specific reason is that the DFAs in the regex
crate can't deal with Unicode word boundaries on non-ASCII text, so the regex
crate is forced to use a slower matching engine. (In this case, that's most
likely the PikeVM. Owch.)

| Engine | boundary-any-english | boundary-any-russian | boundary-long-english | boundary-long-russian | around-holmes-english | around-holmes-russian |
| - | - | - | - | - | - | - |
| go/regexp | 15.8 MB/s | - | 45.3 MB/s | - | 30.5 MB/s | - |
| hyperscan | 158.5 MB/s | - | 439.8 MB/s | - | - | - |
| pcre2 | 98.3 MB/s | - | 71.3 MB/s | - | 51.4 MB/s | - |
| pcre2/jit | **197.6 MB/s** | **228.5 MB/s** | 247.9 MB/s | **195.7 MB/s** | 325.0 MB/s | **319.7 MB/s** |
| python/re | 32.5 MB/s | - | 102.8 MB/s | - | 64.9 MB/s | - |
| python/regex | 27.4 MB/s | 44.8 MB/s | 53.3 MB/s | 100.7 MB/s | 35.5 MB/s | 68.7 MB/s |
| re2 | 63.3 MB/s | - | **953.3 MB/s** | - | 988.6 MB/s | - |
| regress | 168.6 MB/s | - | 146.6 MB/s | - | 72.8 MB/s | - |
| rust/regex/meta | 107.7 MB/s | 21.3 MB/s | 812.4 MB/s | 35.2 MB/s | **49.4 GB/s** | 30.0 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**boundary-any-english**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/word/boundary-any-english` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b\w+\b````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 450561 |


**boundary-any-russian**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/word/boundary-any-russian` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b\w+\b````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-huge.txt`](benchmarks/haystacks/opensubtitles/ru-huge.txt) |
| count(`.*`) | 529194 |


**boundary-long-english**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/word/boundary-long-english` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b\w{12,}\b````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 3466 |


**boundary-long-russian**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/word/boundary-long-russian` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b\w{12,}\b````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-huge.txt`](benchmarks/haystacks/opensubtitles/ru-huge.txt) |
| count(`.*`) | 21332 |


**around-holmes-english**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/word/around-holmes-english` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b\w+\s+Holmes\s+\w+\b````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 27 |

Looks for occurrences of `Holmes` and the words surrounding them while
ensuring they fall on word boundaries. Here, we only use ASCII constructs.

**around-holmes-russian**

| Parameter | Value |
| --------- | ----- |
| full name | `unicode/word/around-holmes-russian` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex | `````\b\w+\s+Холмс\s+\w+\b````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/ru-huge.txt`](benchmarks/haystacks/opensubtitles/ru-huge.txt) |
| count(`.*`) | 44 |

Like `around-holmes-english`, but make it Unicode aware and run it on a
non-ASCII haystack. This one is kind of brutal for the regex crate. It picks
up on the `Холмс` literal and triggers the "reverse inner" optimization. But
once it finds a literal match and does a reverse scan from that point to find
the start position, it trips over a non-ASCII byte and bails out because of
the Unicode word boundary. Then it falls back to... the PikeVM.

This makes a good case for reverse PikeVM/backtrack/OnePass searchers... If
we had even one of those, we could use it for the reverse search because all
three support Unicode word boundaries.

</details>

### wild

#### bibleref

Sadly I can't remember where I found the regex that is the subject of these
benchmarks, but it basically finds bible references in English prose text.

These benchmarks aren't strictly apples to apples because the regex uses
Unicode features and things like `\s` and `\d`, but the latter are not Unicode
aware in some regex engines despite enabling Unicode mode.

| Engine | compile | long | short | line |
| - | - | - | - | - |
| go/regexp | 38.43us | 10.7 MB/s | 27.2 MB/s | 13.5 MB/s |
| pcre2/jit | **22.30us** | 77.9 MB/s | **222.9 MB/s** | 73.5 MB/s |
| python/regex | 384.44us | 4.5 MB/s | 9.1 MB/s | 4.3 MB/s |
| re2 | 402.38us | 765.5 MB/s | 9.2 MB/s | 414.9 MB/s |
| rust/regex/meta | 310.77us | **791.7 MB/s** | 55.9 MB/s | **625.2 MB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**compile**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/bibleref/compile` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`wild/bibleref.txt`](benchmarks/regexes/wild/bibleref.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack | `Gen 1:1, 2
3 King 1:3-4
II Ki. 3:12-14, 25
` |
| count(`.*`) | 3 |

Tests how long it takes to build the bible ref regex, which is kinda big.

**long**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/bibleref/long` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex-path | [`wild/bibleref.txt`](benchmarks/regexes/wild/bibleref.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 874 |

Runs the bible ref regex on a very long haystack.

**short**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/bibleref/short` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex-path | [`wild/bibleref.txt`](benchmarks/regexes/wild/bibleref.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack | `Gen 1:1, 2
3 King 1:3-4
II Ki. 3:12-14, 25
` |
| count(`.*`) | 30 |

Does the same as `long`, but on a very short haystack. PCRE2 does better here
than in the long case, since the entirety of this benchmark is basically 1)
latency and 2) extracting capture groups. In contrast, the `long` benchmark
tends to stay in the DFA in the regex crate until a match is found, and only
then are captures resolved.

For the regex crate, this also reflects an interesting balance point on some
internal heuristics. For example, in this case, the DFA will be run to look
for a match and then the bounded backtracker will be run to resolve capture
groups. In this specific case, since there is a match, it would be faster
to just run the backtracker. The problem is... We don't know ahead of time
whether a match occurs, and it is much faster to run the DFA to reject a
haystack than the backtracker. We *could* perhaps avoid using the DFA when
the haystack is small enough, but the DFA is still likely faster for cases
where a match isn't found. Overall... it's hard to know the right strategy
here. In the end, running the DFA when it isn't needed leads to a small cost,
but *not* using the DFA when it would benefit things tends to lead to a much
bigger cost. Why? Because the DFA is often an order of magnitude faster. So
generally speaking, its cost only shows up in latency oriented benchmarks
like this.

**line**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/bibleref/line` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex-path | [`wild/bibleref.txt`](benchmarks/regexes/wild/bibleref.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`opensubtitles/en-huge.txt`](benchmarks/haystacks/opensubtitles/en-huge.txt) |
| count(`.*`) | 874 |

This provides an interesting counter-balance to the `short` benchmark. Namely,
here, our haystacks are all short but the regex crate does quite a bit better
than in the `short` benchmark. Why? Because most lines don't match and so we
don't have to do the more expensive work of resolving capture groups in most
cases.

</details>

#### caddy

Some benchmarks for parsing Caddy log files.

| Engine | caddy |
| - | - |
| go/regexp | 124.7 MB/s |
| pcre2 | 485.9 MB/s |
| pcre2/jit | **1523.0 MB/s** |
| python/re | 133.9 MB/s |
| python/regex | 101.0 MB/s |
| re2 | 144.1 MB/s |
| regress | 518.0 MB/s |
| rust/regex | 121.9 MB/s |
| rust/regex/meta | 409.8 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**caddy**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/caddy/caddy` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex | `````^([0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}Z)\t(\w+)\t(\w+)\t([^\t]+)(?:\t(.+))?$````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack | `2022-07-27T00:18:48Z	info	tls	cleaning storage unit	{"descri [.. snip ..]` |
| count(`.*`) | 6 |


</details>

#### dot-star-capture

This benchmark was inspired by the regex being benchmarked in this
[exchange][rust-regex-dot-star-exchange].

I probably wouldn't have included these as the regex is a bit strange, but
`pcre2` and `python/re` seem to be able to report results for these in constant
time, which is quite interesting! If you look at the regex, it kind of makes
sense: the `.*` always matches everything up to the end, and since `()` and
`()` are both empty they also always match. So you can actually compute the
capturing group match locations without actually running the search.

I do wonder if such a thing is worth it, but it does look like some regex
engines do indeed do it. I'd like to get a sense of how often these types of
regexes are actually used in practice. If they are often, then it might be
worth doing. But I suspect not because their results are not dependent on the
haystack (other than its length) at all!

(I tried finding where this optimization was implemented inside of `pcre2`
but I had no luck. That code is just impenetrable. Note that the optimization
appears to exist in both the interpreter and JIT code.)

[rust-regex-dot-star-exchange]: https://github.com/rust-lang/regex/discussions/903

| Engine | rust-src-tools |
| - | - |
| go/regexp | 19.8 MB/s |
| pcre2 | 68773.8 GB/s |
| pcre2/jit | **191038.3 GB/s** |
| python/re | 11092.5 GB/s |
| re2 | 17.5 MB/s |
| rust/regex/meta | 491.8 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**rust-src-tools**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/dot-star-capture/rust-src-tools` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex | `````(?s)^((.*)()()($))````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`.*`) | 6 |


</details>

#### grapheme

At time of writing, the Rust [`bstr` crate uses a regex for finding
graphemes][bstr-regex-grapheme] and builds a full DFA from it. Someone might
also conceivably use this with the regex crate, so we benchmark its compilation
and search time.

[bstr-regex-grapheme]: https://github.com/BurntSushi/bstr/blob/65993b58be1547bfc0de9ad8c1c8f3d3fcb0a32f/scripts/regex/grapheme.sh

| Engine | compile | source-code | codepoints |
| - | - | - | - |
| python/regex | 426.69us | 4.1 MB/s | 13.5 MB/s |
| rust/regex/meta | **337.44us** | **95.3 MB/s** | **349.5 MB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**compile**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/grapheme/compile` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`wild/grapheme.txt`](benchmarks/regexes/wild/grapheme.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack | `à̖🇺🇸` |
| count(`.*`) | 2 |


**source-code**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/grapheme/source-code` |
| model | [`count`](MODELS.md#count) |
| regex-path | [`wild/grapheme.txt`](benchmarks/regexes/wild/grapheme.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`python/regex`) | 6832947 |
| count(`.*`) | 7382210 |


**codepoints**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/grapheme/codepoints` |
| model | [`count`](MODELS.md#count) |
| regex-path | [`wild/grapheme.txt`](benchmarks/regexes/wild/grapheme.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`unicode/allcodepoints.txt`](benchmarks/haystacks/unicode/allcodepoints.txt) |
| count(`.*`) | 1109158 |

This looks for graphemes in a file that contains a sequence of all codepoints.
This is somewhat pathological, but it pushes the regex engines inside the regex
crate to their limit, especially the lazy DFA, because it forces the transition
table to get substantially filled out.

</details>

#### parol-veryl

This is another particularly gnarly regex that is used for lexing tokens. It
comes from a [particular use of Parol project][parol-issue-56]. It has a bunch
of rules, and because of limitations in the regex API, it has been squashed
into one regex with a bunch of capture groups to determine which token matches.
But if we use a multi-pattern regex instead, things get quite a bit better
because it lets us skip the capture groups altogether.

[parol-issue-56]: https://github.com/jsinger67/parol/issues/56

| Engine | ascii | unicode | multi-patternid-ascii | multi-captures-ascii |
| - | - | - | - | - |
| rust/regex/meta | **9.4 MB/s** | **8.1 MB/s** | **63.8 MB/s** | **28.8 MB/s** |

<details>
<summary>Show individual benchmark parameters.</summary>

**ascii**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/parol-veryl/ascii` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex-path | [`wild/parol-veryl.txt`](benchmarks/regexes/wild/parol-veryl.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/parol-veryl.vl`](benchmarks/haystacks/wild/parol-veryl.vl) |
| count(`.*`) | 124800 |


**unicode**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/parol-veryl/unicode` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex-path | [`wild/parol-veryl.txt`](benchmarks/regexes/wild/parol-veryl.txt) |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`wild/parol-veryl.vl`](benchmarks/haystacks/wild/parol-veryl.vl) |
| count(`.*`) | 124800 |


**multi-patternid-ascii**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/parol-veryl/multi-patternid-ascii` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex-path | [`wild/parol-veryl.txt`](benchmarks/regexes/wild/parol-veryl.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/parol-veryl.vl`](benchmarks/haystacks/wild/parol-veryl.vl) |
| count(`.*`) | 150600 |


**multi-captures-ascii**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/parol-veryl/multi-captures-ascii` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex-path | [`wild/parol-veryl.txt`](benchmarks/regexes/wild/parol-veryl.txt) |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/parol-veryl.vl`](benchmarks/haystacks/wild/parol-veryl.vl) |
| count(`.*`) | 124800 |


</details>

#### ruff

These regexes were taken from the [Ruff project][ruff-project]. Ruff is
a Python linter in Rust that claims to be quite fast. I took a quick
skim at one point and was interested to see that it [makes heavy use of
regexes][ruff-issue-2901].

While it's not clear how sensitive the performance of Ruff is to the speed of
the individual regexes, it seemed like a nice place to pluck out some real
world regexes to benchmark. I didn't take all of them, but I picked through a
few that looked "interesting." For example, I skipped most anchored regexes
and most regexes that had some obvious literal prefixes.

Interestingly, many regex engines don't run well on the haystack because it
contains invalid UTF-8 *and* Ruff keeps Unicode mode enabled on its regexes.
Many regex engines require valid UTF-8 when Unicode mode is enabled. (I'm
looking at you, Python and Hyperscan.) And some regex engines (regress)
require valid UTF-8 regardless of whether Unicode mode is enabled or not.

[ruff-project]: https://github.com/charliermarsh/ruff
[ruff-issue-2901]: https://github.com/charliermarsh/ruff/issues/2901

| Engine | whitespace-around-keywords | noqa | unnecessary-coding-comment | string-quote-prefix | space-around-operator | shebang |
| - | - | - | - | - | - | - |
| pcre2/jit | 106.0 MB/s | 311.9 MB/s | **1621.8 MB/s** | 1812.3 MB/s | 180.4 MB/s | **1583.7 MB/s** |
| re2 | - | 538.2 MB/s | 772.1 MB/s | 629.5 MB/s | 319.6 MB/s | 835.8 MB/s |
| rust/regex | 191.0 MB/s | 731.3 MB/s | 1296.3 MB/s | 892.6 MB/s | 324.2 MB/s | 264.4 MB/s |
| rust/regex/meta | **314.5 MB/s** | **1661.8 MB/s** | 1261.0 MB/s | **1976.3 MB/s** | **424.0 MB/s** | 979.4 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**whitespace-around-keywords**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/ruff/whitespace-around-keywords` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````(\s*)\b(?:False\|None\|True\|and\|as\|assert\|async\|await\|break\|class\|continue\|def\|del\|elif\|else\|except\|finally\|for\|from\|global\|if\|import\|in\|is\|lambda\|nonlocal\|not\|or\|pass\|raise\|return\|try\|while\|with\|yield)\b(\s*)````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`wild/cpython-226484e4.py`](benchmarks/haystacks/wild/cpython-226484e4.py) |
| count(`.*`) | 1312482 |


**noqa**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/ruff/noqa` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````(?P<spaces>\s*)(?P<noqa>(?i:# noqa)(?::\s?(?P<codes>([A-Z]+[0-9]+(?:[,\s]+)?)+))?)````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`wild/cpython-226484e4.py`](benchmarks/haystacks/wild/cpython-226484e4.py) |
| count(`.*`) | 84 |


**unnecessary-coding-comment**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/ruff/unnecessary-coding-comment` |
| model | [`grep`](MODELS.md#grep) |
| regex | `````^[ \t\f]*#.*?coding[:=][ \t]*utf-?8````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`wild/cpython-226484e4.py`](benchmarks/haystacks/wild/cpython-226484e4.py) |
| count(`.*`) | 16 |


**string-quote-prefix**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/ruff/string-quote-prefix` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````^(?i)[urb]*['"](?P<raw>.*)['"]$````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`wild/cpython-226484e4.py`](benchmarks/haystacks/wild/cpython-226484e4.py) |
| count(`.*`) | 2972 |


**space-around-operator**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/ruff/space-around-operator` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````[^,\s](\s*)(?:[-+*/\|!<=>%&^]+\|:=)(\s*)````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`wild/cpython-226484e4.py`](benchmarks/haystacks/wild/cpython-226484e4.py) |
| count(`.*`) | 1224378 |


**shebang**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/ruff/shebang` |
| model | [`grep-captures`](MODELS.md#grep-captures) |
| regex | `````^(?P<spaces>\s*)#!(?P<directive>.*)````` |
| case-insensitive | `false` |
| unicode | `true` |
| haystack-path | [`wild/cpython-226484e4.py`](benchmarks/haystacks/wild/cpython-226484e4.py) |
| count(`.*`) | 282 |


</details>

#### rustsec-cargo-audit

This group of benchmarks comes from the `cargo audit` tool out of the [rustsec]
project. It was [reported by Shnatsel][shnatsel-report] to the Rust regex
crate.

This regex attempts to extract dependencies and their versions from a
compiled Rust program. The actual issue reported here was that Shnatsel, for
performance, split the regex into a Unix and Windows version, and runs them
separately. The reason for this was that combining them into one regex was
slower. Indeed, when they're combined (and we measure two differents ways
of doing that below), the heuristics in the old regex crate wind up doing a
multiple substring search instead of a search for `cargo`, which is a common
prefix. This slows things down quite a bit.

[rustsec]: https://github.com/rustsec/rustsec
[shnatsel-report]: https://github.com/rust-lang/regex/discussions/960#discussioncomment-5099839
[rustsec-dep-regex]: https://github.com/rustsec/rustsec/blob/26a04a409da94c30b67f45878461998df68d4108/quitters/src/lib.rs#L28-L29

| Engine | original-unix | original-windows | both-slashes | both-alternate |
| - | - | - | - | - |
| go/regexp | 10.1 GB/s | 9.4 GB/s | 9.7 GB/s | 9.9 GB/s |
| pcre2 | 14.4 GB/s | 14.8 GB/s | 14.1 GB/s | 13.4 GB/s |
| pcre2/jit | **36.6 GB/s** | **38.7 GB/s** | **41.4 GB/s** | **37.5 GB/s** |
| python/re | 4.2 GB/s | 4.2 GB/s | 4.2 GB/s | 4.2 GB/s |
| python/regex | 7.1 GB/s | 7.0 GB/s | 2.1 GB/s | 2001.2 MB/s |
| re2 | 21.3 GB/s | 20.3 GB/s | 20.7 GB/s | 20.5 GB/s |
| rust/regex | 9.7 GB/s | 9.0 GB/s | 7.8 GB/s | 6.9 GB/s |
| rust/regex/meta | 25.2 GB/s | 24.0 GB/s | 27.0 GB/s | 27.2 GB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**original-unix**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/rustsec-cargo-audit/original-unix` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex | `````cargo/registry/src/[^/]+/([0-9A-Za-z_-]+)-([0-9]+\.[0-9]+\.[0-9]+[0-9A-Za-z+.-]*)/````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/rg-13.0.0-x86_64-unknown-linux-musl.bin`](benchmarks/haystacks/wild/rg-13.0.0-x86_64-unknown-linux-musl.bin) |
| count(`.*`) | 471 |


**original-windows**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/rustsec-cargo-audit/original-windows` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex | `````cargo\\registry\\src\\[^\\]+\\([0-9A-Za-z_-]+)-([0-9]+\.[0-9]+\.[0-9]+[0-9A-Za-z+.-]*)\\````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/rg-13.0.0-x86_64-pc-windows-msvc.bin`](benchmarks/haystacks/wild/rg-13.0.0-x86_64-pc-windows-msvc.bin) |
| count(`.*`) | 462 |


**both-slashes**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/rustsec-cargo-audit/both-slashes` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex | `````cargo[\\/]registry[\\/]src[\\/][^\\/]+[\\/]([0-9A-Za-z_-]+)-([0-9]+\.[0-9]+\.[0-9]+[0-9A-Za-z+.-]*)[\\/]````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/rg-13.0.0-x86_64-unknown-linux-musl.bin`](benchmarks/haystacks/wild/rg-13.0.0-x86_64-unknown-linux-musl.bin) |
| count(`.*`) | 471 |


**both-alternate**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/rustsec-cargo-audit/both-alternate` |
| model | [`count-captures`](MODELS.md#count-captures) |
| regex | `````cargo/registry/src/[^/]+/([0-9A-Za-z_-]+)-([0-9]+\.[0-9]+\.[0-9]+[0-9A-Za-z+.-]*)/\|cargo\\registry\\src\\[^\\]+\\([0-9A-Za-z_-]+)-([0-9]+\.[0-9]+\.[0-9]+[0-9A-Za-z+.-]*)\\````` |
| case-insensitive | `false` |
| unicode | `false` |
| haystack-path | [`wild/rg-13.0.0-x86_64-unknown-linux-musl.bin`](benchmarks/haystacks/wild/rg-13.0.0-x86_64-unknown-linux-musl.bin) |
| count(`.*`) | 471 |


</details>

#### url

This benchmark's regex came from a [reddit discussion].

The actual regex was originally retrieved from a [comparison between the Rust
regex crate and Oniguruma][rust-regex-vs-onig].

It's an absolutely ginormous regex for recognizing what appears to be URLs.

[reddit-rust-url]: https://old.reddit.com/r/rust/comments/xr0ztr/lemmeknow_the_fastest_way_to_identify_anything/iqdsa3c/
[rust-regex-vs-onig]: https://github.com/swanandx/regex_vs_onig/blob/d4e21afeb2053f5148095c97054f53299b02e29b/src/lib.rs

| Engine | compile | search |
| - | - | - |
| go/regexp | 5.51ms | 3.1 MB/s |
| hyperscan | 339.55ms | - |
| pcre2 | **200.05us** | 3.1 MB/s |
| pcre2/jit | 944.92us | 20.4 MB/s |
| python/re | 7.21ms | 3.5 MB/s |
| python/regex | 22.33ms | 1613.3 KB/s |
| re2 | 2.20ms | **124.6 MB/s** |
| rust/regex/meta | 3.57ms | 97.7 MB/s |

<details>
<summary>Show individual benchmark parameters.</summary>

**compile**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/url/compile` |
| model | [`compile`](MODELS.md#compile) |
| regex-path | [`wild/url.txt`](benchmarks/regexes/wild/url.txt) |
| case-insensitive | `true` |
| unicode | `false` |
| haystack | `https://google.com` |
| count(`hyperscan`) | 2 |
| count(`.*`) | 1 |


**search**

| Parameter | Value |
| --------- | ----- |
| full name | `wild/url/search` |
| model | [`count-spans`](MODELS.md#count-spans) |
| regex-path | [`wild/url.txt`](benchmarks/regexes/wild/url.txt) |
| case-insensitive | `true` |
| unicode | `false` |
| haystack-path | [`rust-src-tools-3b0d4813.txt`](benchmarks/haystacks/rust-src-tools-3b0d4813.txt) |
| count(`.*`) | 234965 |

Benchmarking the search is quite interesting for the regex crate. It starts
out fine using the lazy DFA. The cache slowly gets filled up and cleared, but
otherwise moves along at a decent clip. But as the benchmark repeats itself,
since the same cache is reused, the clear count goes up until it hits an
heuristic limit. At that point, the lazy DFA fails and the search falls back to
the slower PikeVM and performance tanks.

RE2 does okay here, presumably because it keeps using the lazy DFA. RE2 does
have more sophisticated logic for determining whether it should keep using the
cache. Namely, in addition to the total number of times the cache is cleared,
it also considers the number of bytes processed per new state added. If that's
good enough, then absolute cache clear count on its own isn't necessarily a
problem.

The old regex crate also used to have a check like this, and sadly, I lost it
in the migration to the `regex-automata` crate. I really should bring it back,
but have avoided doing so because it probably requires some inconvenient API
work in the hybrid NFA/DFA API. But I suppose that's just reality.

UPDATE: The lazy DFA now supports this heuristic and it approximately matches
RE2's perf on my system now.

</details>

